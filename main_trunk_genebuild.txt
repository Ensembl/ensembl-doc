Running an assembly based genebuild
-------------------------

(Contact: vac@sanger.ac.uk, eae@sanger.ac.uk, lec@sanger.ac.uk)

A. Overview
-----------

As far as possible the genebuild is automated, though not (yet)
incorporated into the pipeline.


The pre-genebuild prepares genes from known proteins, from regions
with similarity to proteins specific to the species being built, and
also to proteins in swall. Data from the pre-genebuild are
incorporated into the final gene build during which additional genes
will also be predicted from evidence not yet used.

We can run either on an assembled genome ie one that has a golden
path, or on RawContigs. This document describes how to run the build
on assembled data though the basic concepts are common to both approaches.

These instructions assume you already have a database with an uploaded
golden path, entries in contig, clone and dna tables, and have
completed any raw computes you plan to do.

The instructions are written from the point of view of human
sequence. The general points should be applicable to any assembled
genome.

The modules that need to be run are contained in Bio::EnsEMBL::Pipeline::RunnableDB and are:

1. FPC_TargettedGeneWise

   Uses genewise to build a gene by comparing an input protein (id)
   with a region of the golden path.
   Alternatively, one could use FPC_TargettedGeneE2G which uses proteins and their corresponding cDNAs 
   together with a golden path (fpc contig) location to which that protein matches. 

2. FPC_BlastMiniGenewise

   Depends on the genscan peptides having already been blasted against
   sptr during the pipeline run. Feature names in the database we use for the build are typically:
   
   +-----------+
   | genscan   |	
   | wublastx  |
   | wutblastn |
   | wublastp  |
   +-----------+

   For a given golden path region, finds all the blast hits from
   genescan peptides to entries in sptr and filters out those that
   overlap with genes predicted in stage 1. Any remaining sequences
   are reblasted against the golden path region and the resultant
   features used to build a MiniSeq over which we can run genewise and
   build genes.

3. Combine_Genewises_and_E2Gs

   Depends on the species specific cDNAs having been run against the
   whole genome sequence and genes built using a combination of
   exonerate and est2genome (will eventually be exonerate only). This
   is described in cdna_analysis.txt

   Also depends on stages 1 and 2 having already been run

   Combines the two predictions, using UTRs from the est_genome
   prediction with internal gene structure from the genewise
   prediction.

4. Gene_Builder

   Can create additional genes from genscan predictions that have strong supporting evidence. 
   Performs various validity checks on transcripts.
   Clusters transcripts into genes.
   Writes genes to database.

However, various steps need to be completed before you can run these modules.


B. General preparation
----------------------

1. I use 2 areas, one scratch area for the genebuild LSF error and
output files, and an area where I keep all the files generated during
processing so I can refer back.

So, for example, for the April golden path:

mkdir /work2/vac/GeneBuild/Apr_gp
mkdir /scratch4/ensembl/vac/GeneBuild

2. make sure you are using bioperl 0.7 

3. cvs checkout ensembl and ensembl-pipeline code (Oct 2002 I am using branch_9 for both the core ensembl code and ensembl-pipeline (lec))

4. make sure ensembl/modules and ensembl-pipeline/modules and bioperl 0.7 are in your $PERL5LIB

5. make sure ensembl-pipeline/scripts/GeneBuild is in your PATH, or be
prepared to qualify the names of the various scripts.

6. cvs checkout rd-utils (and run make pmatch), or make sure you have access to a pmatch executable.

7. The scripts for running the pre-genebuild are in ensembl-pipeline/scripts/GeneBuild

8. There are configuration files specific to the various stages of the
genebuild
ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config/GeneBuild, which
need to be filled in with details relevant to your directory
setup. The fields to be filled in should be self explanatory. We will
describe the specifics for each stage as we go along.

9. Fill in relevant fields of pipe_conf.pl, notably bindir - which is essential!

10. Various databases are needed. We'll describe these here.

Diagram of jobs and databases:

 _______  
|       |   pmatches
| GB_DB |   features 
|_______|
      |
      |
targetted genewise
      |
      |
similarity genewise
      |	   
      |              __________	
      |             |          | 
      |------------>| GB_GW_DB |
      |             |__________|         ____________
      |                  |              |            |
      |-----------------\|/-------------| GB_cDNA_DB |
      |     combine genewises and cdnas |____________|
      |                  |
      |             _____|______	
      |            |            | 
      |            | GB_COMB_DB |
      |            |____________| 
      |                  |
       -----------------\|  
	            genebuilder
	            _____|______	
                   |            | 
                   | GB_FINALDB |
                   |____________| 


All these databases need the following tables filled in:
clone, contig, assembly, analysis, meta, chromosome

More info about the various databases:
--------------------------------------

Here we descibe the various databases and the configuration options relevant to each.

* reference db ( or sometimes called dna db )
Contain the dnas, the features from the pre-compute pipeline and the pmatch_features
(included in the extra tables pmatch_feature and protein).
All the other databases will use it as dna_db to get sequence when necessary.
Databases::GB_DBHOST
Databases::GB_DBNAME
Databases::GB_DBUSER
Databases::GB_DBPASS

* genewise db
In this database we will store the genes/transcripts/exon/translation/supporting_evidence
from the runs of similarity_genewise (FPC_BlastMiniGenewise) and targetted_genewise (FPC_TargettedGeneWise)
Databases::GB_GW_DBHOST
Databases::GB_GW_DBNAME
Databases::GB_GW_DBUSER
Databases::GB_GW_DBPASS   

* cdna db
this database contains the genes/transcripts/... from the analysis of cdnas with est2genome
Databases::GB_cDNA_DBHOST
Databases::GB_cDNA_DBNAME
Databases::GB_cDNA_DBUSER
Databases::GB_cDNA_DBPASS

* combined genes db
This db will have the genes/... built by combining the cdnas from
GB_cDNA with the genewise genes.  Before running the combining code
(Combine_Genewises_and_E2Gs), you should copy all genewise genes (plus
translations,etc...)  into this database from GB_GW_DB. In this way we
assure that combined_genes will get different internal ids, which will
be less confusing during the GeneBuilder analysis.
Databases::GB_COMB_DBHOST             
Databases::GB_COMB_DBNAME
Databases::GB_COMB_DBUSER
Databases::GB_COMB_DBPASS

* final genes db
This database will have the genes built by GeneBuilder from the genewise and combined genes
in GB_COMB_DB and the genescan prediction in GB_DB. 
Databases::GB_FINALDBHOST
Databases::GB_FINALDBNAME
Databases::GB_FINALDBUSER
Databases::GB_FINALDBPASS

11. Other general configurable options

GB_LENGTH_RUNNABLES 
This is an array of hashes; each hash contains the
logic name and the runnable name for a particular analysis.

GB_INPUTID_REGEX 
This should be a regex for parsing whatever format your input
ids will be in.  For most people name.start-end is what is used, so
the regex will be (\S+)\.(\d+)-(\d+)

GB_REPEAT_MASKING 

This is an anonymous array which should contain the logic name of each
repeat analysis you want masked for. At present we only have
RepeatMask but soon there will tandem repeats and low complexity
repeats in the repeat table too. As standard we advise only making what
RepeatMask flags

12. filling in the analysis table

You need to populate the analysis table for each database (except the
db with features which will be populated by the raw computes).

First, copy the analysis table from the raw compute
database you are using as these analysis ids will be required in order
to fetch the repeat masking and the blast hits properly.

Then load the analysis entries needed for the genebuild.

mysql -uuser -ppassword -hhost databasename < /path/to/ensembl-pipeline/sql/GeneBuild_analysis.sql

The db with features will contain other entries in the analysis table derived from the pre-compute pipeline

13. Make sure each of these databases has an entry in the meta table to
tell it the default assembly name - should look something like this:

| meta_id | meta_key         | meta_value                 |
+---------+------------------+----------------------------+
|       2 | assembly.default | NCBI_28                    |

If not, load one up like this:

mysql -uuser -ppassword -hhost -e "insert into meta values (\N, 'assembly.default', 'NCBI_28');" dbname

Obviously, replace NCBI_28 with the name of the appropriate assembly -
use the string from the 'type' column of the assembly table.

C. TargettedGenewise build
----------------------------------

1. download assembled sequence eg for the human build:

    old way:
    from Jim Kent

    This takes up a good bit of space.

    http://genome-test.cse.ucsc.edu/gs.7/oo.29/contigFa.zip

    mv contigFa.zip /data/humangenome
    cd /data/humangenome
    unzip contigFa.zip

    new way:
    or for NCBI (and mouse and ...)
    get out chromosome names & coords into a file called names_and_coords:
    select chr_name,min(chr_start),max(chr_end) from static_golden_path group by chr_name into outfile
    '/.../names_and_coords';

    more names_and_coords:
    chr4_NT_022804  1       153717
    chr4_NT_022910  1       196456
    chr4_NT_022923  1       94217
    etc

    you can produce this file with the sql

    select chromosome_id, "1", max(chr_end) from assembly group by chromosome_id into outfile '/path/to/names_and_coords'

    dump out one sequence per chromosome.

    perl -e 'while(<>){next unless /^(\S+)\s+(\d+)\s+(\d+)/; $command = "/path/to/ensembl-pipeline/scripts/GeneBuild/dump_slice_seq.pl -chrname $1 -start $2 -end $3 -outfile $1.fa"; system("$command"); print "$command\n"}' < names_and_coords

    the regular expression you want to use here will probably be whatever regex you have put in the config file General::GB_INPUTID_REGEX

Create one directory per chromosome underneath the directory $GB_FPCDIR, and into each
of those move the fasta file(s) with the appropropriate genomic sequence in fasta format. eg for chr1, make a directory called 1, and place the file 1.fa into it.

You can also put several files under the same directory

Options to be set in Config::GeneBuild files
Scripts::GB_FPCDIR  `     Directory which will contain one directory per chromosome.
General::GB_INPUTID_REGEX Regexp for parsing input ids eg name.start-end

2. download proteome data

TargettedGeneE2G needs as input protein ids and corresponding cDNA ids
 together with a golden path (fpc contig) location to which that
 protein matches. The first stage is downloading human protein
 sequences from SwissProt/TrEMBL and Refseq. 

TargettedGeneWise needs as input protein ids 
 together with a golden path (fpc contig) 
 location to which that
 protein matches. The first stage is downloading human protein
 sequences from SwissProt/TrEMBL and Refseq. 

example:
mkdir .../GeneBuild/NCBI_30/human_proteome
cd .../GeneBuild/NCBI_30/human_proteome

Note: SPproteomes ( non redundant Sptr peptide set ) and alternative peptide products:
SPproteomes (fasta format) will contain just one peptide per gene-entry. 
There are exceptions: Two variants for a gene might be in different entries if 
the peptides are very different, but this is rare.
To get an 'expanded' SPproteome  you can do two things:
1) ask  Paul Kersey <pkersey@ebi.ac.uk> to generate one for you, or
2) generate one yourself: 
the program is a perl program and can be found at
/ebi/ftp/pub/software/swissprot/varsplic/varsplicExapnd.pl.

the program has a horrible choice of options, but the following syntax

perl varsplic.pl -input "filename" -fasta "output_file_name" -which full -uniqids

will probably get you what you want.

the program requires the use of SWISSKNIFE, a perl library for parsing
SWISS-PROT.  It is also on the ftp site /ebi/ftp/pub/software/swissprot/swissknife, 
but from the EBI alphas (e.g. ice) you can use it simply by adding /ebi/sp/pro3/shared/lib/perl
to your @INC.

3) Paul Kersey also produces an expanded version of all human or mouse SPTr entries each week.
You can ask him to point you to the files . But this is not an expanded version of the non-redundant SPTr sets.


ftp: username: anonymous, passwd is your email address
#downloading swissprot files:
 ftp ftp.ebi.ac.uk
 ftp>cd pub/databases/SPproteomes

# for human
 ftp>get fasta_files/proteomes/9606.FASTAC
 ftp>get swissprot_files/proteomes/9606.SPC

# for mouse
 ftp>get fasta_files/proteomes/10090.FASTAC   
 ftp>get swissprot_files/proteomes/10090.SPC
 ftp> bye
 mv 9606.FASTAC sptr.fa

# downloading refseq files:
 ftp ftp.ncbi.nih.gov
 # for human
 ftp>cd refseq/H_sapiens/mRNA_Prot/
 ftp>get hs.faa.gz 

# for mouse
 ftp>cd refseq/M_musculus/mRNA_Prot/
 ftp>get mouse.faa.gz 
 ftp>bye
 gunzip hs.faa.gz
 mv hs.fsa refseq.fa

3. prepare_proteome data

ensembl-pipeline/scripts/GeneBuild/prepare_proteome.pl cleans up the
sptr and refseq files, prepares a single proteome file and runs a test
pmatch to make sure there are no sequences that will cause pmatch to
loop.

You can do this manually. Simply check for long runs of XXX's (undetermined aminoacid)
and delete those entries. You can also parse the headers and put them all in a single-field format.

all you need to do, assuming you have filled in the config files properly, is type:

prepare_proteome.pl

Note: remove manually sequences containing many X's (stands for undetermined residue) 
from your input fasta files. 

Also if you are using files which aren't RefSeq and Swall you may want to alter the regex being used to produce the ids in the proteome file

Options to be set in Config::GeneBuild::Scripts:

Scripts::GB_REFSEQ  Path to fasta file of Refseq protein sequences
Scripts::GB_SPTR    Path to fasta file of swissprot/trembl protein sequences
Scripts::GB_PFASTA  Path to file where cleaned up proteome data will be written
Scripts::GB_PMATCH  Path to pmatcn executable

4. index the seqfile using, for example, makeseqindex. You can use whatever 
sequence indexing/fetching setup you like as long as the ensembl modules know how to use
it. Look at ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/SeqFetcher/*
for inspiration.

Whichever indexer you use needs to be specified in
Sequences::GB_PROTEIN_SEQFETCHER so the correct seqfetcher can be
made.

It's easier to index files with a single id header, you can do that with for example:
shell> cat refseq.fa | perl -lane '{if ($F[1] =~ /\((\S+)\)/){ print ">$1";} else{ print $_;} }' > refseq.fa_single_id

With makeseqindex, it is important to make sure that the .jidx file
doesn't have hard-coded path information in it. SO:

     cd to the directory where your GeneConf::GB_PFASTA file is - this is the proteom file you have just produced
     /path/to/makeseqindex proteome_file > proteome_file.jidx

Copy proteome_file and proteome_file.jidx to the place you have
specified in GeneConf::GB_PROTEIN_INDEX and rdist across the
farm - this is important when you come to run the Targetted build
jobs.

**** need to document indicate?


Options to be set in Config::GeneBuild::Sequences:

Sequences::GB_PROTEIN_INDEX	 Path to proteome file that is rdisted across farm. (Not .jidx file!)
Sequences::GB_PROTEIN_SEQFETCHER Type of seqfetcher you want to use to retrieve sequences from 
				 your indexed file 
				 eg Bio::EnsEMBL::Pipeline::SeqFetcher::OBDAIndexSeqFetcher

5. run the pmatches

We use pmatch to compare all the human protein sequences to the golden
path sequence to try to find the best match for each protein in the
genome. We look at the extent to which the protein is covered ie what
percentage of the protein matches the genome in a given region, and we
take the hit with the best coverage, plus any hits within 2% of that
coverage. Caveat - we throw away anything that has less than 25%
coverage as the best hit.

There are 2 scripts that deal with pmatch running and result
processing: 

    ensembl-pipeline/scripts/GeneBuild/pm_filter.pl : finds best in chromosome matches
    ensembl-pipeline/scripts/GeneBuild/pm_bestmatch.pl finds the best in genome match from all of these.

    mkdir /work2/vac/TGE/Apr_gp/pmatches
    cd /work2/vac/TGE/Apr_gp/pmatches
    pm_filter.pl -chr 1 -protfile protein_filename
    pm_filter.pl -chr 2 -protfile protein_filename
    .
    .
    pm_filter.pl -chr UL -protfile protein_filename

    (each job will create an output file chr_name.pm.out in GeneConf::GB_PM_OUTPUT)

    cat *.pm.out > all.pm.out
    pm_bestmatch.pl < all.pm.out

    (this will create a file pm_best.out in GeneConf::GB_PM_OUTPUT)

NB If you have dumped one fasta file per chromosome then the output
from pm_bestmatch is fine as it is. If you had one sequence per FPC
contig (eg UCSC human data) then you need to use the -chromo_coords
option with pm_bestmatch.pl to get the pmatch hits in chromosomal
coordinates - this is essential later in the build!

ie

pm_bestmatch.pl -chromo_coords < all.pm.out

For some assemblies you may need to split the protein file up into
pieces and run pm_filter.pl on each chromosome with each piece - keep
an eye on how big the script gets and split up the file as necessary.

Options to be set in Confif::GeneBuild::Scripts.pm:

Scripts::GB_PFASTA     Path to file containing cleaned up proteome data produced by prepare_proteome.pl
Scripts::GB_PMATCH     Path to pmatcn executable
Scripts::GB_PM_OUTPUT  Path to directory to write pmatch output files
Scripts::GB_FPCDIR     Path to top level directory containing genomic sequence in subdirectories
Scripts::GB_TMPDIR     Path to scratch directory for output files


6. Create tables in database to hold the pmatch results
mysql -uuser -ppasswd -hhost dbname < ensembl-pipeline/sql/pmatch.sql

7. Load up pmatch results into the database using pmatch_feature_loader. This script will directly
read from pm_best.out

Run FPC_TargettedGeneWise on (5MB) regions

Options to be set in Config::GeneBuild modules:

Databases::GB_GW_DBNAME	  Name of interim genewise gene build database 
Databases::GB_GW_DBUSER   DB username (need read/write access)
Databases::GB_GW_DBHOST   Host for interim build database
Databases::GB_GW_DBPASS   Password for accessing build database 

Scripts::GB_PM_OUTPUT     Path to directory containing pmatch results

8. make bsub lines for submitting jobs

The runnable you need is called FPC_TargettedGeneE2G so enter this in Config::GeneBuild::Scripts (in length_runnables) and run

make_bsubs.pl

The jobs to be run are in FPC_TargettedGeneWise.jobs.dat
in the directory you specified in Scripts::GB_TMPDIR

Also, look in your tmpdir - all the
output directories needed for collecting the bsub out & err files are
made if they didn't already exist

Options to be set in Config::GeneBuild files:

Databases::GB_DBNAME		Name of interim gene build database containing dna, features, assembly etc
Databases::GB_DBHOST		Host for interim build database
Databases::GB_DBUSER		DB username (need read/write access)
Databases::GB_DBPASS		Password for accessing build database

Scripts::GB_RUNNER		Path to run_GeneBuild_RunnableDB
Scripts:GB_QUEUE		LSF queue on which jobs should be run, plus any options

    Jobs will run by default on any machine type (alpha on acaris and Linux on Blades). 
    If you want to restrict your jobs to alpha machines (for example, if you're running genewise,
    which is much faster on the  alphas) you will need to specify -Ralpha in
    your job submissions.  Likewise -Rlinux will restrict your jobs to
    the blades, which you might wist to do for algorithms which run better on them (e.g. exonerate).

Scripts::GB_TMPDIR		Path to scratch directory
Scripts::GB_LENGTH_RUNNABLES	Names of length based runnables to be used
Scripts::GB_PM_OUTPUT		Path to directory containing pmatch results
Scripts::GB_SIZE		size of chunk to use in build - typically 5000000 (5MB)

9. Run FPC_TargettedGeneWise jobs 

This is as easy as typing submit.pl < FPC_TargettedGeneWise.jobs BUT WAIT!

I ALWAYS run test jobs first before unleashing everything on the farm:

a. check the pre-exec command (the bit in double quotes) by running on
the command line
b. send one job via LSF to check the output goes to the right place
c. usually I then slowly load up the farm to check for database
contention issues - generally I run 10 jobs via LSF, then 100 jobs. If
there ARE database issues, consider "dripping" the jobs into LSF ie
submit a job, sleep (10,20,30, ...) seconds, submit the next etc.
d. if all is well, submit the remaining jobs.

You can submit jobs using
ensembl-pipeline/scripts/GeneBuild/submit.pl, adjusting the number of
seconds in the sleep statement as required.

Options to be set in Config::GeneBuild config files:
Scripts::GB_SIZE			    Chunk size you're running with

General::GB_INPUTID_REGEX		    Regexp for parsing your input ids

Sequences::GB_PROTEIN_INDEX		    Path to indexed rdisted proteome file produced earlier
Sequences::GB_PROTEIN_SEQFETCHER	    Module name

Databases::GB_GW_DBNAME		            Details of database for storing genewise genes
Databases::GB_GW_DBHOST
Databases::GB_GW_DBUSER
Databases::GB_GW_DBPASS

Targetted::GB_TARGETTED_SINGLE_EXON_COVERAGE 
Targetted::GB_TARGETTED_MULTI_EXON_COVERAGE genewise predictions are compared back to the protein
					    they were built from and a coverage percentage
					    calculated. We weed out gene predictions based on the
					    coverage score; single exon predictions are typically
					    require to have a much higher percentage match than
					    multiexon predictions
Targetted::GB_TARGETTED_MAX_INTRON Maximum  allowed intron size
Targetted::GB_TARGETTED_MIN_SPLIT_COVERAGE  Transcripts are split on long introns 
					    unless the predicted protein matches the parent 
					    protein with coverage in excess of this threshold - a 
					    straight length based split loses us real genes

Targetted::GB_TARGETTED_GW_GENETYPE	    For writing genes

10. Check for errors

Sadly, there's no easy way to do this yet. I grep through the err
files from bsub commands for "EXCE" and look to see what the errors
are. Once I identify key words seen in exceptions I'll grep through
for those, and use comm to find if there are more things I am missing
eg

find ./ -name "*.err" -exec grep -q EXCE {} \; -print > EXEC.txt
find ./ -name "*.err" -exec grep -q "exon lies on a gap" {} \; -print > gaps.txt

(comm only works on sorted files, but effectively these already are sorted)

comm -23 EXCE.txt gaps.txt

this will tell you which files have EXCE in them but not "gap contigs" so you can look at those.

Error checking often takes as much time as setting up to run the jobs in the first place ...

Common errors in the Targetted jobs: look for things like:
'EXCE' - all exceptions will be reported like this
'Problem fetching sequence' -  this means you need to look at your sequence index and rerun those jobs that failed.
'abort' (in out files) - Jobs that have aborted can be rerun.
'memory' - f you see jobs that have run out of memory, the thing to do is to split the vcontig into smaller chunks and rerun them.
'lost' (use case insensitive search) -  If you have jobs that lost their database connection, first check the mysql parameters to make sure you don't have a ludicrously short connection  timeout (these jobs can take HOURS to run), and resubmit them.
Also look for 'exon lies on a gap' ( in gaps it won't be able to remap the gene ) - there's not much to be done about these - we need to make a fix in the code.

grep for exit codes in .out files
exit code 130 -> job was bkilled
          139 -> weird memory error
          2   -> sequence fetching prob? can indicate a dodgy node
          1   -> out of memory
          9   -> out of pfetch servers

Look also for exceptions which do not fall into these categories - they might be new bugs that need fixing!

good things to search for here are the words Can't and Shouldn't, they must be capitalised as this may be errors throw by perl which weren't caught

In some cases, some runs on certain proteins fail for other reasons e.g. blast cannot open a file  
You can find these by grepping for uncommon exit codes in the err files eg  EXIT CODE 8, 18... 
Then you can rerun these individual protein jobs, e.g.:
.../ensembl-pipeline/scripts/run_GeneBuild_RunnableDB -runnable Bio::EnsEMBL::Pipeline::RunnableDB::TargettedGeneWise  -input_id 3:156380770,156456450:P01132: -write

It's ok if genes get written twice at this stage, as this will be dealt with later on by GeneBuilder.

D. Similarity Genewise build
----------------------------

Don't start this stage until all the Targetted jobs are finished and checked.

1. Edit GeneConf.pm and set length_runnables to be FPC_BlastMiniGenewise

2. run make_bsubs.pl - output will go into FPC_BlastMiniGenewise.jobs
in the directory specified in Scripts::GB_OUTPUT_DIR

3. run some test jobs as before - this stage is the most likely to
cause database contentions so you really might need to stagger job
submissions.

4. If all is well, submit the rest of the jobs as before

Config options:

Similarity::GB_SIMILARITY_DATABASES	hash containing information about each source of homologies you want to use

an example 
GB_SIMILARITY_DATABASES => [
					 # fill in one complete hash for each database from which blast 
					 # features are to be retrieved
					 {				  
					  'type'       => 'Swall',
					  'threshold'  => '200',
					  'index'      => '/data/blastdb/Ensembl/swall_021116',
					  'seqfetcher' => 'Bio::EnsEMBL::Pipeline::SeqFetcher::OBDAIndexSeqFetcher'
					 }
			     ],

type is the logic name of the analysis type in the analysis table
threshold is the score cut off used when fetching the blast hits
index is the lodation of the index file
seqfetcher is the type of seqfetcher object to be used to access the index file

GB_SIMILARITY_COVERAGE	         genewise predictions are compared to the protein they were 
				 built from and the predicted gene rejected if this coverage 
				 is lower than GB_SIMILARITY_COVERAGE
GB_SIMILARITY_MAX_INTRON         Transcripts are split on introns that exceed GB_SIMILARITY_MAX_INTRON
GB_SIMILARITY_MIN_SPLIT_COVERAGE Transcripts are split on long introns  unless the predicted 
				 protein matches the parent protein with coverage in excess of 
				 this threshold - a straight length based split loses us real genes
GB_SIMILARITY_MAX_LOW_COMPLEXITY Predicted proteins are rejected if they contain more than this 
				 level of low complexity sequence

If you are unsure about what to use for these values stick with what's in the file already

GB_SIMILARITY_GENETYPE           This is the type which will be given to the gene and should be 
				 the same as the logic name of the analysis object in the table 
				 for this analysis

5. Check for errors. One to look for at this stage is "Out of memory"
- if you see this in any of the stderr files, you'll need to identify
the regions that had problems, and rerun on smaller subsets of those
regions


eg if chr1.1-5000000 ran out of memory, set up new jobs to run that are only 1MB long:
chr1.1-1000000
chr1.1000001-2000000
chr1.2000001-3000000

etc.
there's no script to do this for you but it's a pretty easy one to write ...

E. cDNA build
-------------

This is described in ensembl-doc/cdna_analysis.txt

F. Adding UTRs to genewise predictions
--------------------------------------

The module that does this for you is called Combine_Genewises_and_E2Gs

Don't start this stage until all the Similarity jobs are finished and checked, and you have run the cDNA analysis.

1. Edit GeneConf.pm and set length_runnables to be Combine_Genewises_and_E2Gs

2. run make_bsubs.pl - output will go into Combine_Genewises_and_E2Gs.jobs
in the directory specified in GB_OUTPUT_DIR

3. run some test jobs as before.

4. If all is well, submit the rest of the jobs as before

5. Check for errors. 

Options to be set in GeneConf.pm:

GB_GW_DBHOST  this is information about where to get the genewise genes
GB_GW_DBUSER  from
GB_GW_DBNAME
GB_GW_DBPASS

GB_cDNA_DBHOST this is information about where to get the cdna est2genome
GB_cDNA_DBUSER genes from 
GB_cDNA_DBNAME
GB_cDNA_DBPASS

GB_COMB_DBHOST this is where to write the combined genes too
GB_COMB_DBUSER
GB_COMB_DBNAME
GB_COMB_DBPASS

GB_COMBINED_GENETYPE
GB_cDNA_GENETYPE
GB_COMBINED_MAX_INTRON

G. Final genebuild
-----------------
The module that does this for you is called Gene_Builder

You will be writing the final ensembl genes to a separate database -
the reason for this is that we've had sever database locking issues
when trying to read genes from and write genes to the same database
across 400 farm nodes. The name and host of this database are
specified in GeneConf.pm as finaldbname and finaldbhost. Make sure
this database exists, has a schema (!) and has the same entries in
clone, contig, static_golden_path and analysisprocess tables as the
one you have been using. eg:

mysql -uuser -ppassword -hinterim_dbhost interim_db
mysql> select * from contig into outfile '/path/to/contig.file';
mysql> select * from contig into outfile '/path/to/clone.file';
mysql> select * from static_golden_path into outfile '/path/to/sgp.file';
mysql> select * from analysisprocess into outfile '/path/to/ap.file';
mysql> quit

mysql -uuser -ppassword -hfinal_dbhost finaldb
mysql> load data infile '/path/to/contig.file' into table contig;
mysql> load data infile '/path/to/clone.file' into table clone;
mysql> load data infile '/path/to/sgp.file' into table static_golden_path;
mysql> load data infile '/path/to/ap.file' into table analysisprocess;
mysql> quit

Don't start this stage until all the Similarity jobs are finished and checked.

1. Edit GeneConf.pm and set length_runnables to be Gene_Builder

2. run make_bsubs.pl - output will go into Gene_Builder.jobs
in the directory from which make_bsubs.pl was run

3. run some test jobs as before.

4. If all is well, submit the rest of the jobs as before

5. Check for errors. You may have memory errors as with the Similarity
jobs. If so, resplit and rerun as you did before.

NOTE:
It is very important that none of the Gene_Builder jobs is run more than
once otherwise you'll end up with duplicate genes in the final
database.

Options to be set in GeneConf.pm:

GB_ABINITIO_TYPE
GB_ABINITIO_SUPPORTED_TYPE types given to abintio exons
GB_FINALDBNAME	 Name of databse to which final stage genes will be written
GB_FINALDBHOST	 Host where final db lives
GB_FINALDBUSER   user
GB_FINALDBPASS   password        
GB_MIN_GENSCAN_EXONS the minimum number of genes a abintio gene needs before it is considered
GB_GENSCAN_MAX_INTRON  the max intron size for an abintio gene


H. Checking results before web handover
---------------------------------------

This is described in ensembl-doc/post_genebuild_checks.txt


I. Preparations of the Data before handing Over:
------------------------------------------------

#################
Dump the peptides
#################

dump the translations from the built genes into multiple fasta format.
This will be used for protein-mapping 
( see .../ensembl-doc/protein_mapping.txt )

You can use the script .../ensembl-pipeline/scripts/GeneBuild/dump_peptides.pl*

Make sure to produce a log-file where you keep record of the translations with
stop codos, maybe as well of monkey-exons, and the transcript ids being dumped
etc. Communicate the ones with stop codos to the people checking the data
so that they can remove them from the database.


########################
Final GeneBuild Database
########################

Also, you'll need to make sure that the final database you hand over
has dna, feature, repeat_feature etc tables populated. The way I do
this is to re-use the interim database:

1. Connect to interim database and save out all entries from gene-related tables into tab delimited
files in case you need to go back later

mysql> select * from gene into outfile 'path/to/interim_gene.file';
mysql> select * from exon into outfile 'path/to/interim_exon.file';
mysql> select * from exon_transcript into outfile 'path/to/interim_exon_transcript.file';
mysql> select * from transcript into outfile 'path/to/interim_transcript.file';
mysql> select * from translation into outfile 'path/to/interim_translation.file';
mysql> select * from supporting_feature into outfile 'path/to/interim_supporting_feature.file';

2. Connect to interim database and delete all entries from gene-related tables

mysql> delete from gene;
mysql> delete from exon;
mysql> delete from exon_transcript;
mysql> delete from transcript;
mysql> delete from translation;
mysql> delete from supporting_feature;

3. Connect to final gene build db and save out gene related tables:

mysql> select * from gene into outfile 'path/to/final_gene.file';
mysql> select * from exon into outfile 'path/to/final_exon.file';
mysql> select * from exon_transcript into outfile 'path/to/final_exon_transcript.file';
mysql> select * from transcript into outfile 'path/to/final_transcript.file';
mysql> select * from translation into outfile 'path/to/final_translation.file';
mysql> select * from supporting_feature into outfile 'path/to/final_supporting_feature.file';

4. Connect to interim database and load up results from final gene build

mysql > load data infile 'path/to/final_gene.file' into table gene;
mysql > load data infile 'path/to/final_exon.file' into table exon;
mysql > load data infile 'path/to/final_exon_transcript.file' into table exon_transcript;
mysql > load data infile 'path/to/final_transcript.file' into table transcript;
mysql > load data infile 'path/to/final_translation.file' into table translation;
mysql > load data infile 'path/to/final_supporting_feature.file' into table supporting_feature;

note: we might have to do something about this supporting feature table, as it will contain evidence
with analysis ids which are not in the analysisprocess table, e.g.: combined_e2g

5. Transfer the analysisprocess entry relevant to genes of type ensembl into the shiny new db.

Once all your post-build checks are run, you're done!


###########
Otherwise, you can just use the same database of the final genebuild and put in there the dna table from the 
interim database. You won't need to move the exons/genes/etc around. You'll have to prune the features and 
convert the supporting evidence anyway.
###########


##################
Prune the Features
##################

dump the feature table from the interim database into a file

mysql> select * from feature into outfile 'path/to/interim_feature_before_pruning.file';

parse the feature file and reject all those BLAST features (and only BLAST features) with score below 150.

delete the feature table from the interim database ( make sure you have a back-up and
that you are deleting the right table from the right database )
and load the resulting features back into the feature table

mysql > load data infile 'path/to/pruned_features' into table feature;


#########################################
Convert Supporting Evidence into Features
#########################################

The Web page only displays features and no supporting features, therefore we need
to convert all the supporting evidence from the supporting_feature table
into the format for the feature table.

This requires to make sure that we have a proper analysisprocess table,
and that every feature load (or to be loaded) has an analysis correctly assigned and
present in the analysisprocess table.

We need to convert the entries from the supporting_feature table originally in the interim database,
which should have been saved as above in 'interim_supporting_feature.file'.

We need to do the following conversion in the name, according to the origin of the evidence

if name = 'TGE_gw'		 ---> name = 'mouse_refseq'  ( if hid =~ /NP/ )
				 ---> name = 'mouse_swall'  ( otherwise ) 

if name = 'similarity_genewise'  ---> name = 'other_swall'

if name = 'combined_gw_e2g'      ---> we reject these ones. This information is available as a combination pf
                                      the above one plus the info from the cDNA analysis



We also need to convert the supporting_feature table in the database where we have done the cDNA analysis
into features, again splitting by the origin of the cDNA, thus

if hid =~ /NP/                   ---> name = 'refseq_cdna'
otherwise                        ---> name = 'embl_vertrna'


Each of these features will have to have a proper analysis associated in the analysisprocess table.
Make sure that the analysis ID put in the conversion from supporting_feature to feature is the right one.
In this sense it helps to construct first the analysisprocess table. It should have the following entries added:

15  2002-03-07 14:28:46	mouse_swall  mouse_swall  1	\N	TGE_gw	 1	\N	\N	TargettedGeneWise	\N	TGE_gw	gene
16	2002-03-07 14:28:46	mouse_refseq	mouse_refseq	1	\N	TGE_gw			1	\N	\N	TargettedGeneWise	\N	TGE_gw	gene      
17	2002-03-11 17:37:23	other_swall	other_swall	1	\N	similarity_genewise	1	\N	\N	FPC_BlastMiniGenewise	\N	similarity_genewise	gene
18      2002-03-07 14:28:46	refseq_cdna	refseq_cdna     1	\N	exonerate_e2g	      	1	\N	\N	FilterESTs_and_E2G	\N	exonerate_e2g	       	gene      
19      2002-03-07 14:28:46	embl_vertrna	embl_vertrna	1	\N	exonerate_e2g		1	\N	\N	FilterESTs_and_E2G	\N	exonerate_e2g	       	gene      




############################################################################################
Note: when you have all features ready, if you are working on a file, recall that
about 48 million features can take up to four hours to upload bacj into the feature table.
############################################################################################



#######################
Remove redundant tables
#######################

Remove tables pmatch_feature and protein, used in the pmatch analysis.
Make a back-up of this data somewhere before hand.




The following document describe the resources needed and how to set up
and run the ensembl pipeline for raw computes. 

The raw computes are a set of analyses which tend to get run across all 
the contigs in a database or across pieces of the assembly to generate 
some base feature on a genome before anything else is run


What you need 
=============

a mysql instance

bioperl-072 
ensembl core 
ensembl pipeline

these are all availible from cvs on the web


You also need access to the binarys which you want to use to analysis
run your sequence, where you can get these files is describe later
in this document

we have Runnables which will run

RepeatMasker 
cpg a cpg island prediction program written by Gos Micklems 
trf a tandem repeat finder 
tRNAscan-se 
epcr 
Blast 
genscan
genefinder 
fgenesh 
dust for low complexity masking

some of these are freely availible on the web, other need licenses

An ensembl core database with the pipeline tables added
(ensembl-pipeline/sql/table.sql)

you also need access to some sort of batch submission system in order
to submit you analysis to your compute. At ensembl we use LSF and there
is also code with in the pipeline for using Suns Gridengine

The rest of this document will also assume you have a database which
contains sequence and if desired an assembly 
(see loading_sequence_into_ensembl.txt)  and will describe how 
to configure and run the raw computes. The generic parts of the pipeline
which are used are documented in using_the_ensembl_pipeline.txt



Before you start the pipeline
=============================

Before you can start the pipeline you need to setup a few things first.

You need to make sure you database contains the pipeline tables.

You need to decide which analyses you are going to run. 

You need to make sure you database contains sequence and possibly an 
assembly if you wish to run on pieces of chromosomes. 
Lastly you need to fill in both the analysis table, the rule tables 
and the configuration.

(The pipeline schema and tools to fill the analysis and rule tables
are described in using_the_ensembl_pipeline.txt. How to load sequence
into a core ensembl database is described in 
loading_sequence_into_ensembl.txt)   


Pipeline database tables
========================

These table definitions can be found in ensembl-pipeline/table.sql and 
the tools used to fill them in are described in 
using_the_ensembl_pipeline.txt. 

The analysis and input_id_type_analysis tables need entries to describe 
all the analyses you wish to run. The rule tables need entries to 
describe the dependancies which exist within the pipeline (a diagram 
further down the document should describe a typical set of dependancies)
The input_id_analysis table should contain entries for all input_ids
you analysis need to run against. For the raw computes this is likely
to be contig names and 1M slice names.



Configuration
=============

To run the raw computes there are only three configuration files which 
you need to fill in. These files all live in 
Bio::EnsEMBL::Pipeline::Config and are called General.pm, BatchQueue.pm
and Blast.pm. In the cvs checkout these will all be appended with a
.example which will need removing

This files all contain generic pipeline options which are explained
in using_the_ensembl_pipeline.txt

General.pm also contains some raw_compute specific settings

SLICE_INPUT_ID_REGEX, this is the regex used to parse the input ids
		      which represent slices so the correct piece
		      can be fetched from the database. 
		      A standard slice input_id looks like this
		      1.1-1000000, the regex used to parse this would 
		      look like this (\S+)\.(\d+)-(\d+) and the 3 values
		      it would produce would represent chromosome name,
		      start and end

PIPELINE_REPEAT_MASKING This is an array which indicated if repeatmasked
			sequence should be fetched and what type of 
			repeats should be masked. If you want all repeats
			in you repeat_feature table to be masked the 
			array needs to look like this  ['']. If you want
			specific repeats to be masked you need the array
			to look like this ['RepeatMask', 'Dust'] and 
			contain the logic_names of the repeats you want 
			masked. If you want no repeats to be masked you 
			need the array to look like this [].




Running the Pipeline
====================

The Pipeline is run using a script called RuleManager3.pl, which is 
located in enembl-pipeline/modules/Bio/EnsEMBL/Pipeline/

This script is explained in using_the_ensembl_pipeline.txt. The script
also has a -help options which is used should print out its documentation

Here are some example command lines of how to run the script to
get the Raw computes run

perl RuleManager3.pl -dbhost myhost -dbuser admin -dbpass **** 
     -dbname my_pipeline_db -shuffle


this will just run the pipeline. Any jobs which can be run will be run
and the RuleManager will continue to loop through the rules ad infinitum
untill it is killed


It is generally a good idea to test the Pipeline before setting the 
RuleManager lose across the whole Rule table. This can be done in a 
number of ways.

The once flag means the RuleManager will only loop through its id set
once then stop. This will allow you to recognise is any immediate
setup is incorrect.

The idlist_file flag makes the RuleManager only run on a list of ids
provided in a file which limits what the RuleManager can do

The -analysis flag makes the RuleManager run on a specified analysis
again limiting its actions

These are all the options which can be passed to the RuleManager script


DB Connection Details


   -dbhost     The host where the pipeline database is.
   -dbport     The port.
   -dbuser     The user to connect as.
   -dbpass     The password to use.
   -dbname     The database name.

Other Useful Options

   -idlist_file a path to a file containing a list of input ids to use
    this file need to be in the format input_id input_type, for example
    1.1-100000 SLICE
   -skip_idlist_file, file same format as above of ids to skip, note
    if two different id types use the same input_id the id must be
    present in the list twice, under both types in order to always be
    skipped
   -once only run through the RuleManager loop once
   -shuffle before running though the loop shuffle the order of the 
    input ids
   -analysis only run with these analyses objects, can be logic_names or
    analysis ids, this option can appear in the commandline multiple 
    times
   -skip_analysis  don't run with these analyses objects, like -analysis 
    can be logic_names or dbIDs and can appear in the commandline 
    multiple times '
   -input_id_types, which input id types to run the RuleManager with,
    this option can also appear in the commandline multiple times
   -skip_id_types, which types of input ids not to run, again this option
    can appear on the commandline many times

Other options

   -local run the pipeline locally and not using the batch submission 
    system
   -runner path to a runner script (if you want to overide the setting
				    in BatchQueue.pm)
   -output_dir path to a output dir (if you want to overide the 
				     setting in BatchQueue.pm)
   -v verbose mode
   -dbsanity peform some db sanity checks, can be switched of with 
             -nodbsanity
   -start_from, this is which analysis to use to gather the input ids
    which will be checked by the RuleManagers central loop
   -accumulators, this flag switches the accumulators on, the 
    accumulators are on by default but can be swtiched on with the 
    -noaccumulators flag, if this flag is on the accumulator sanity check
    will also run
   -accumulator_die this is a boolean flag which if it appears in the
    command line when the accumulator sanity check is run the script will
    die if it fails rather than just printing a warning
   -max_job_time, can overide the $MAX_JOB_TIME value from General.pm
    with this flag
   -killed_file can overide the path to the $KILLED_INPUT_IDS file from
    General.pm
   -kill_jobs, this is a switch to tell the RuleManager to check how long
    jobs have been running for and kill them if they have been running for 
    to long (defined either on the commandline or in config). this is on
    by default but can be swtiched off with -no_kill_jobs
   -queue_manager can overide the $QUEUE_MANAGER from General.pm
   -rename_on_retry, this means any output already produced will be deleted
    before a job is retried, this defaults to off currently, it was put in 
    as LSF appends output and error files and sometimes you don't want 
    this'
   -rules_sanity, this checks the types are consistent between rule goals
    and rule_conditions , it is on by default but can be switched off with
    -norules_sanity
   -rerun_sleep, the amout of time to sleep for after a loop when no jobs
    were submitted, as standard it is 300s
   -base_sleep, the minumun time to sleep if there are too many pending 
    jobs, defaults to 180s
   -max_sleep, the maximum amount of time to sleep for if there are too
    many pending jobs defaults to 5400s
   -sleep_per_job, the amount of time to sleep per pending job over the 
    maximum defaults to 60s
   -max_pending_jobs defaults ot what is set in MAX_PENDING_JOBs in 
    BatchQueue.pm

The progress of the pipeline can be watched by using the monitor script.
This script lives in ensembl-pipeline/scripts. It is also documented
in using_the_ensembl_pipeline.txt and has a -help options. A common 
useage in the pipeline here at sanger is this


 
./monitor -dbhost myhost -dbuser admin -dbpass **** -dbname mypipelinedb
-current -current_summary -finished

and this might produce output like this

Name            Status   Count
----            ------  -----
Briggsae_ESTs   RUNNING   55 
Briggsae_ESTs   READING   45 
Briggsae_mRNAs  RUNNING   55 
Briggsae_mRNAs  READING   45 
Elegans_ESTs    RUNNING   55
Elegans_ESTs    READING   45 
Elegans_mRNAs   RUNNING   55 



Pipeline status summary [ecs2b pipeline_genebuild_050303]

Status   Count 
------   ----- 
RUNNING   522 
READING   405


Finished job summary [ecs2b pipeline_genebuild_050303]

Count Name            Id 
----- ----            -- 
5605  SubmitContig    1 
5605  RepeatMask      2 
5605  Genefinder      3 
5498  WormPep         5 
5482  Swall           10 
5505  Elegans_ESTs    11 

this script gets all its information from the job tables.


Now follows a description of some of the analyses which we run as part
of our pipeline or can be run as part of the pipeline. 


Repeat Features 
===============

RepeatMasker this uses libaries to identify repeat sequences in you dna
which allows you to mask them out when running other analyses, for a
academic/non comercail version of this produce contact
asmit@nootka.mbt.washington.edu with the subject RepeatMasker request
for a commercial license contact swxfr@u.washington.edu

TRF a tandem repeat finder the binaries are availible from here
    http://c3.biomath.mssm.edu/trf.download.html

Dust a low complexity repeat finder the binaries and source are
availble from here
ftp://ftp.ncbi.nih.gov/pub/tatusov/dust/

these are all used as part of the standard ensembl pipeline

Ab Intitio gene prediction 
==========================

Genscan GENSCAN is freely available for academic use. Executables are
currently available for the following Unix platforms: Sun/Solaris,
SGI/Irix, DEC/Tru64 and Intel/Linux. Platforms not listed are not
currently available.

to get genscan for academic use go here
http://genes.mit.edu/license.html 
Save the license agreement web page as a text file called 'license.txt' 
(e.g., if running Netscape use File > Save As... > Text).  
Edit the file license.txt to include your name
and complete mailing address (not email) using a simple text editor
such as SimpleText, emacs, vi, jot, etc. (if you must use Word or
WordPerfect, please save the resulting file as ASCII text).  
Send the edited license.txt file by email to Chris Burge (cburge@mit.edu)
In your email please specify which platform you intend to run Genscan on
(Sun, SGI, DEC or Intel/Linux) - I cannot process requests without this
information.  Wait for a couple of days to receive the Genscan
distribution by email. You should receive a response within one week
except during holidays - if you do not, please resend the license.txt
file.

Fgenesh is now product of softberry contact details
softberry@softberry.com

Genefinder is a species specific gene prediction tool. It uses maximum
likelihood estimation to predict gene structure. It can be configured
for a variety of species. Genefinder is an unpublished work of Colin
Wilson, LaDeana Hilyer, and Phil Green. The source code is freely
available for research and educational purposes.
ftp://ftp.genome.washington.edu/src (Please contact Colin Wilson if you
are unable to access the ftp site colin@u.washington.edu).

Genscan is the only program run as standard in the ensembl-pipeline

Simple Features 
===============

tRNAscan-SE predicts the location of tRNA genes the source and binaries
are availible here under the GPL
http://www.genetics.wustl.edu/eddy/software/

CpG predicts the location of cpg islands the source is availble here
ftp://ftp.sanger.ac.uk/pub/gos/analysis/src/

Eponine predicts transcription start sites the jar file is availble
here  http://www.sanger.ac.uk/Software/analysis/eponine/ under the LGPL

Marker and ePCR ftp://ftp.ncbi.nih.gov/pub/schuler/e-PCR this places
pcr markers in the sequence

these are all run as part of the standard ensembl pipeline

Blast 
=====

In the ensembl pipeline we normally use WU-blast which is availible
under license from Washington State
http://blast.wustl.edu/blast/README.html#Licensing 
NCBI Blast can also be used and it freely availble from NCBI 
ftp://ftp.ncbi.nih.gov/blast/

BlastGenscanPep BlastGenscanDNA

These both take sequences of ab initio predicted genes then use those
to blast against the databases, this is done for two reasons, one for
speed as to blast the whole of a human sized genome(~3GB)  against a
blast database such as swall or dbest would take to long. The other is
to allow use to find protein and dna evidence which supports these
genes so potentially they can be used further down the line to
contribute to our genebuild

Normally we run BlastGenscanPep and BlastGenscanDna but for small
genomes such as anopheles and briggsae we blast against raw sequence



Here is an example of the dependancies which will exist in a standard
raw compute pipeline


   SubmitContig	          SubmitSlice		          SubmitChromosome 
      CONTIG                 SLICE		               CHROMOSOME
     /  |  \                  /  \                    /     \    
    /   |   \                /    \                  /       \
   /    |    \              /      \                /         \
 TRF    |     tRNAScan    Eponine  Marker          CpG        Dust      
CONTIG  |      CONTIG      SLICE   SLICE        CHROMOSOME CHROMOSOME
        |
        |
     RepeatMask
       CONTIG
        |
        |
        |
      Genscan
       CONTIG
      /  |   \
    /    |    \ 
Swall  VertRNA Unigene
CONTIG CONTIG   CONTIG
 
The Genscan depends on RepeatMask as we generally run ab initio gene
predictors on repeat masked sequence. Swall, VertRNA and Unigene are all
blasts and they depend on Genscan as they are run using 
BlastGenscanPep/DNA. All the other analyses produce either simple or 
repeat features and don't depend on any other analysis


Checklist
=========

When running the raw computes it is worth checking these things out
before you hit go

1. Do you have all the executables, blast databases, matrix/data files the
analysis you want to run will need? are they push out across you compute 
farm

2. Have you filled out all the config files, General.pm, Blast.pm, 
BatchQueue.pm plus any analysis specific files like Fathom.pm

3. Does BatchQueue.pm contain entries for all the analyses you wish to run

4. Have you filled in the analysis table

5. Have you filled in the rule tables

6. Are the appropriate dummy entries in the input_id_analysis table

If the answers to all these questions are yes you are probably read
to set the RuleManager going


Here is an example of the conf file you may need to setup the analysis for
the genebuild using the analysis_setup.pl script 
(see using_the_ensembl_pipeline.txt)

In these analysis objects there are settings which are specific to the
data you are using so you may need to change some entries before using 
the data. You should note that all this objects do need a 
input_id_type as specified by the type variable and if at anypoint the
type of a goal analysis isn't the same as the type of the conditional
analysis you need an accumulator between the two and another condition
of the same type to provide the appropriate input_ids 


[SubmitContig]
type=CONTIG

[TRF]
program=trf
program_version=1
program_file=trf
module=TRF
gff_source=trf
gff_feature=tandem_repeat
type=CONTIG

[tRNAscan]
db=trna
program=tRNAscan-SE
program_version=1.23
module=tRNAscan_SE
gff_source=tRNAscan
gff_feature=tRNA
type=CONTIG

[RepeatMask]
db=repbase
db_version=13-07-2002
db_file=repbase
program=RepeatMasker
program_version=1
parameters=-m
module=RepeatMasker
gff_source=RepeatMasker
gff_feature=repeat
type=CONTIG


[Genscan]
db=HumanIso.smat
db_version=NULL
db_file=HumanIso.smat
program=genscan
program_version=1
parameters=NULL
module=Genscan
gff_source=genscan
gff_feature=similarity
type=CONTIG

[Unigene]
db=uniuni
db_file=uniuni
program=wutblastn
program_file=wutblastn
parameters=-cpus => 1, -hitdist => 40
module=BlastGenscanDNA
gff_source=wutblastn
gff_feature=similarity
type=CONTIG

[Swall]
db=swall
db_file=swall
program=wublastp
program_file=wublastp
parameters=-cpus => 1, -hitdist => 40
module=BlastGenscanPep
gff_source=wublastp
gff_feature=similarity
type=CONTIG

[Vertrna]
db=embl_vertrna
db_file=embl_vertrna
program=wutblastn
program_file=wutblastn
parameters=-cpus => 1, -hitdist => 40
module=BlastGenscanDNA
gff_source=wutblastn
gff_feature=similarity
type=CONTIG

[Submit1MSlice]
type=1MSLICE

[Eponine]
db=Eponine
program=eponine-scan
program_version=1
program_file=/usr/opt/java141/bin/java
parameters=-epojar => /usr/local/ensembl/lib/eponine-scan.jar, -threshold => 0.999
module=Slice_EponineTSS
gff_source=Eponine
gff_feature=TSS
type=1MSLICE

[Marker]
db_version=14-02-2003
program=e-PCR
program_version=1
parameters=-M => 150, -NMIN => 0, -NMAX => 2
module=Slice_EPCR
gff_source=e-PCR
gff_feature=sts
type=1MSLICE

[SubmitChr]
type=CHROMOSOME



[Dust]
program=dust
program_version=1
program_file=tcdust
module=Slice_Dust
gff_source=dust
gff_feature=dust
type=CHROMOSOME

[CpG]
db=cpg
program=cpg
module=Slice_CPG
gff_source=cpg
gff_feature=cpg_island
type=CHROMOSOME

The following document describe the resources needed and how to set up and run the ensembl pipeline for raw computes


What you need
=============

a mysql instance

bioperl-072
ensembl core
ensembl pipeline

these are all availible from cvs on the web


You also need access to the binarys which you want to use to analysis run your sequence

we have Runnables which will run

RepeatMasker
cpg a cpg island prediction program writtne by Gos Micklems
trf a tandem repeat finder
tRNAscan-se 
epcr
Blast
genscan 
genefinder
fgenesh
dust for low complexity masking

some of these are freely availible on the web, other need licenses


An ensembl core database with the pipeline tables added (ensembl-pipeline/sql/table.sql)

you also need access to some sort of batch submission system in order to submit you analysis to your compute. At ensembl we use LSF and there is also code with in the pipeline for using Suns Gridengine 

The rest of this document will assume you have a database which contains sequence and if desired an assembly and will describe how to configure and run the pipeline


Configuration
=============

There are a series of configuration files which need filling out for the pipeline to run

These files are all found in

ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config


the files present in the intial checkout will be appended with a .example but you need to rename the file

e.g General.pm.example should become General.pm

There are 3 files which need filling out

General.pm contains general settings for the pipeline
-----------------------------------------------------

BIN_DIR, DATA_DIR, LIB_DIR should be the directories where most of the binaries/data files/lib files live for the analyses you wish to run

These variables are then used to locate programs or data files when they are to be run

PIPELINE_OUTPUT_DIR this is the directory where the stdout and stderr from your analyses will be written this can be set from the 
command line of RuleManager but this will be explained later on

PIPELINE_WORK_DIR this is the directory where analyses will be run, this is set up in RunnableI and as default will be /tmp/


PIPELINE_RUNNER_SCRIPT location of the runner script to run the pipeline normally ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/runner.pl is used

SLICE_INPUT_ID_REGEX if you are running any of you analyses on pieces of dna which are bigger than contigs the object used to generate these pieces 
will be called a Slice. The inpput_id used will be the slices name which follows the format chr_name.start-end In this variable you must specify 
how to parse thatinput id into chr_name, start and end. Most ids look like this 1.1-10000 and would use the input_id_regex which is given in the example

PIPELINE_REPEAT_MASKING this is the annoymous array which is passed to the get_repeatmasked sequence call it should either contain the [''] which 
would mean all repeats would be masked out or ['RepeatMask'] which would be the specific logic_names of the type of repeat_features which you 
want masked out or [] if you want no masking to take place



BatchQueue.pm contains settings for the job object and the batch submission system
----------------------------------------------------------------------------------


QUEUE_MANAGER, this is the name of the module which should be used submit jobs to you batch submission system, These modules all live 
under Bio::EnsEMBL::Pipeline::BatchSubmission, there are currently two modules LSF which will submit jobs to an LSF system and GridEngine 
which uses Suns Gridengine If you wanted to use a different submission system the you would need to implement a module which inherits 
from Bio::EnsEMBL::Pipeline::BatchSubmission and implements two methods, construct_command_line and open_command_line which are used by 
the Job to submit the analysis


MAX_PENDING_JOBS this is the maximum number of pending jobs allowed before the RuleManager script will stop submitting more jobs to the farm

AUTO_JOB_UPDATE this being true (ie = 1) means when a jobs status is changed it get automatically updated if its set to 0 this won't happen 
generally its better to have it set to 1

Next there are a series of default values and an array of hashes which contain specific values for each analysis type(keyed in Job.pm on logicname), 
The values which have default settings are 

BATCH_SIZE this is how many jobs the batchsubmission system will run in one command on one know, for long jobs this is best left at 1 but for 
short jobs for more efficient use of compute its good to set this to around 10

RETRIES this is the number of times that the script will resubmit a job when its hass failed

OUTPUT_DIR this is where the stderr and stdout get written

BATCH_QUEUE this is the queue in your batchsubmission system that the job will get submitted to, If your system doesn't use queues this 
can probably be left blank

the array of hashs also have a few other arguements

logic_name is the logic name of a particular analysis

resource is any resource requirements which need to be passed to your batch submission for example at sanger we have a mixed os architecture farm 
so here you can specify either alpha or linux if you analysis runs better on one or the other

sub_args are any other commandline arguments your job submission system might need

resource and sub_args args are used to pass information into the batchsubmission object when it is created. In theory you could use them to 
pass in any information you wanted provided your construct_command_line method knew how to utilise that information, their function above 
relates to how we use them in the LSF module

runner is if you want to use a different runner script for a particular analysis type than the one specified in General.pm 


Blast.pm for blast specific settings
------------------------------------


This contains an array of hashes which contain specific settings for each blast database you want to run against

this are 

name this should be the entry in the db_file column of the analysis table

type this is whether the blast is agains dna or protein to ensure the correct type of AlignFeature is created

header this is the regex to be used to parse the correct id out of the fasta header for a particular hit

flavour this can be ncbi or wu as these require slightly different command line construction

ungapped if this is defined you will get the ungapped pieces or each hit as feature pairs if the data is to be stored in a standard ensembl database this should be set to 0 as the blast features are stored as align features with cigar strings which describe how the coordinates can be split to produce the individual features which make up the alignment

refilter is this is set then the an internal filtering method will be used rather than the FeatureFilter runnable 


Database setup
--------------

for the pipeline to work you also need a few things setup in the database

Analysis table
==============

the analysis table need to contain entries for all the different analysis you want to run and dummy entries for the pipeline to work

the dummy entries just need a logic name, these are used when butting intial entires in the input_id_analysis table and as the condition for rules which don't rely on other analyses

the lines which specify analyses you want to carry out mostly need more information

logic name this is a name for your analysis, we try and make them informative

db this is a name for the database your database, 
db_version the version of your database
db_file a path to a database file or matrix file needed by the program for blast the path needs to be from what ever you set to be the environment variable BLASTDB for example if you BLASTDB is /data/blastdb and you want to use the database in /data/blastdb/Ensembl/swall Ensembl/swall would be what to put in the db_file column

program this is the name for the program
program_version version of program
program_file this is the file for the program if the program is located in the directory specified in BIN_DIR in General.pm you only need give the program name otherwise it is best to specify the full path

parameters this is any commandline parameters your program might need, the format should be a comma delimited list, if the option is just a commandline flag e.g -gi or -low that is what should appear but if it is a flag and a value the entry should look like this '-lib => /path/to/library_file' for example the parameters string '-low, -lib => /ecs2/work1/lec/briggsae.lib' would be parsed to produce a command line like this /usr/local/ensembl/bin/RepeatMasker  -low -lib /ecs2/work1/lec/briggsae.lib AC091216.24470.32005.2980374.seq


module this should either be the name of the module which is located in Bio::EnsEMBL::Pipeline::RunnableDB or if the module isn't located there it should be the full name of the module
module_version version of module

gff_source the source or program which produces these results
gff_feature the type of feature e.g repeat, homology or gene


Rule tables
===========

there are two rule tables

rule_goal and rule_condition

rule_goal contains a numeric rule_id and a goal which is the numeric analysis id of a particular analysis object which you would want completed when this rule was carried out 

rule_condition this contains a numeric rule_id and a condition which is the logic_name of a analysis which must be complete before an analysis/rule can be carried out

examples

rule_goal

 rule_id | goal |  logicname of goal
+---------+------+
|       2 | 2    |	RepeatMask
|       3 | 3    |	Genefinder
|       4 | 5    |	Wormpep (blastx against wormpep)
|       5 | 10   |	Swall (blastx against swall)
|       6 | 11   |	Elegans_EST (blastn against elegans ests)
|       7 | 12		Elegans_mRNAs (blastn against elegans mrnas)

 rule_id | condition    |
+---------+--------------+
|       2 | SubmitContig |
|       3 | RepeatMask   |
|       4 | RepeatMask   |
|       5 | RepeatMask   |
|       6 | RepeatMask   |
|       7 | RepeatMask


SubmitContig a dummy analysis used inorder to get the pipeline started

input_id_analysis table
=======================

this is where entries are written for analysis which have already finished

this table has four columns

input_id #this is normally some sort of sequence name
analysis_id # this is the analysis_id of the analysi which has been completed sucessfully
created # the time at which the entry was created
result # in theory values could be placed here to indicate whether results were found etc but it isn't currently used for anything

Intially entries must be written here for the dummy analysis object for a specific input_id format, each different input_id format 
currently needs a different dummy analysis type

Some of the sequence loading scripts also put entries in the input_id_analysis for each contig which is loaded for more information 
about loading sequence into an ensembl database see the document called loading_sequence_into_ensembl.txt (not yet written lec 11/03/03)

alternatively you can use this script

ensembl-pipeline/scripts/populate_input_id_analysis.pl

this script takes several arguments

the first four must always be present

-dbname
-dbhost
-dbuser
-dbpass 

as these provide information about which database to insert the input_ids into


the other options are

-contig which must be following by the logic name of the dummy analysis which is to represent contig input_ids
-slice which must be following by the logic name of the dummy analysis which is to represent slice input_ids

one of these two options must be specified otherwise the script won't do anything

-size after which you specify what size of slice you want to generate input_ids for 

if you specify slice but no size again the script won't work as the script won't know what size slices to generate

here is an example of the commandline you would use

for contigs

./populate_input_id_analysis.pl -dbname pipeline_genebuild_050303 -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ***** -contig SubmitContig

this uses the contig names as input_ids and produces entries like this

| input_id           | analysis_id | created             | result |
+--------------------+-------------+---------------------+--------+
| c011601284.1.6205  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601266.1.4316  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601254.1.17394 |           1 | 2003-03-10 14:45:23 |      0 |
| c011601247.1.3097  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601187.1.2005  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601185.1.2030  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601180.1.19582 |           1 | 2003-03-10 14:45:23 |      0 |
| c011601175.1.22689 |           1 | 2003-03-10 14:45:23 |      0 |
| c011601167.1.1953  |           1 | 2003-03-10 14:45:23 |      0 |
| c011601162.1.20829 |           1 | 2003-03-10 14:45:23 |      0


where 1 is the analysis_id of the SubmitContig analysis object

you can also populate the input_id_analysis table for contigs using a line of sql

insert into input_id_analysis(input_id, analysis_id, created) select name, 1, now() from contig;

where the number is the analysis_id of the dummy object 

for slices though you have to do it using some sort of script as the input_ids don't currently exist anywhere

the command line would look like this


./populate_input_id_analysis.pl -dbname pipeline_genebuild_050303 -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ***** -slice SubmitSlice -size 1000000

this would produce entries like this

| input_id                     | analysis_id | created             | result |
+------------------------------+-------------+---------------------+--------+
| cb25.fpc0002.1-303432        |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0003.1-1000000       |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0003.1000001-1191916 |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0006.1-240985        |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0010.1-734258        |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0011.1-1000000       |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0011.1000001-1084169 |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0022.1-1000000       |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0022.1000001-1502397 |         126 | 2003-03-10 16:01:06 |      0 |
| cb25.fpc0023.1-1000000       |         126 | 2003-03-10 16:01:06 |      0 |

the input_ids follow the format chr_name.start-end and should fit the regular expression specified in General.pm

this are examples from a briggsae database, for briggsae there where no chromosome maps so super contigs were used instead 
most of the superctgs were less than 1MB long so the coords of the whole chromosome was simply used this also means in the case 
of briggsae the chr_names are a bit long for human and most organisms they are generally just numbers so you get 1.1-100000

if you wanted to run across whole chromosomes you need to make sure the size specifed is longer than the longest chromosome then the coords used will
always default to the start and end of the chromosome




Running the analyses
===================== 

Here are descriptions of the various analyses we actually run as part of the standard ensembl raw compute pipeline 
plus others which there are runnables for

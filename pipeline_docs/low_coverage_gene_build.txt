==============================
RUNNING THE WGA2Genes PIPELINE
==============================

The function of this system is as follows: given a low-coverge/gappy
genome to annotate (the target), an annotated, reference genome (the
query) and a filtered whole-genome alignment (WGA) between the two,
project annotation from the query onto the target through the WGA, 
at the same time inferring a super-assembly of the small scaffolds
of the target into "gene scaffolds" that contain whole genes.  

========
OVERVIEW
========

The procedure has six phases:

(1) Calculation of candidate gene scaffolds and gene structures

    This step is performed by the ensembl-analyis RunnableDB WGA2Genes, 
    the input_id is a slice id for the reference genome. This step works 
    best if the slice ids are "intelligently" constructed, and there is a 
    script to do this. 

(2) Consolidation of gene scaffolds and extension of components. 
 
    This step is performed by the ensembl-analysis script 
    wga2genes/merge_and_extend_gene_scaffolds.pl. It joins gene scaffold
    components together into larger assemblies where appropriate, and
    also extends their components such that for every scaffold that 
    contributes to a gene scaffold, every base pair is used somewhere
    (either in one large piece or a number of smaller pieces). 

(3) Loading of new assembly and gene structures

    There are two products of the above initial steps: an AGP describing
    the assembly of the gene scaffolds from their consituent scaffolds, and
    GFF-like file describing the gene structures on these gene scaffolds. These
    can be loaded into your core target database with the ensembl-analysis
    scripts wga2genes/load_gene_scaffolds.pl and wga2genes/load_transcripts.pl

(4) Re-run scaffold-level raw computes on new top-level pieces

    Because many of the original scaffolds will be fragmented in the
    gene scaffolds, it is necessary to re-run scaffold-level analyses
    on the new top-level co-ordinate system (on-the-fly projection will
    not always be possible, particularly for features that straddle a
    break-point). This step is not needed if the results of all raw 
    computes were stored at the sequence level (usually contigs).

(5) Run the later stages of the Ensembl gene-build

    It may be appropriate to run the Ensembl GeneBuilder module,
    in order to cluster transcripts into genes (and thence all subsequent
    stages of a normal gene-build, for example protein annotation). 
    In addition, it is often sensible to run some Genewises/Exonerates 
    at this point, in the style of the classical gene-build, targetting 
    regions not covered by projected genes.

(6) Extra things

    There are one or two things to do before hand-over that are specific
    to low-coverage builds. 

Note that only steps 1-3 and 6 are described in this document. Help with steps 4 
and 5 can be found in the documents "the_raw_computes.txt" and
"the_genebuild_process.txt."


============
REQUIREMENTS
============

- A query core database

  This database should contain the assembly and annotatation of the 
  reference genome. This will be referred to as $QUERY_CORE_DB.

- A target core database

  This database should contain the sequence and assembly of the low-
  coverage genome to be annotated. This will be referred to as
  $TARGET_CORE_DB.

- A compara database

  This database should contain the processed whole-genome alignment 
  (WGA) between the query and target genomes. This alignment *must* have
  the following property: each bp in the target genome should be represented 
  at most once. See other documentation for how to produce a useful
  alignment with this property. This database will be referred to as
  $COMPARA_DB, and the MethodLinkType of the processed alignment to be
  used as $WGA_METHOD_LINK_TYPE.

- A pipeline database

  This facilitates the running of the first, WGA2Genes stage; the assumption 
  is that you will have already set up a pipeline database for the preparation 
  and processing of the WGA). This database will be referred to as 
  $PIPELINE_DB.

- A kill-list

  This is a file containing a list of genes/transcripts in the query database
  that should be ignored during the process. This "kill-list" should contain, 
  at the very least, all transcripts in the query database that have a coding 
  length that is not modulo-3 in length (the code throws an exception when
  trying to project these). It can also contain other entries for gene structures 
  that are known to be wrong in the query database. The script  
  ensembl-analysis/scripts/wga2genes/identify_bad_source_transcripts.pl can be run 
  against the query database to make an initial kill-list. The kill-list file will
  be referred to as $KILL_LIST.

- Code

  The following CVS check-outs are required: 

  ensembl (plus bioperl)
  ensembl-analysis
  ensembl-compara
  ensembl-pipeline


===================================================================
STEP 1: Calculation of Candidate gene scaffolds and gene structures
===================================================================

This step is performed by the ensembl-analysis RunnableDB WGA2Genes,
behaviour being controlled by a config file with the same name. 

The input ids are identifiers of slices from the reference (query) 
genome that should contain one or more genes. Because each of these
slices can be considered independently, the process can be parallelised
on the farm using the pipeline. 

Config setup
------------

It is necessary to plug in the details of the $QUERY_CORE_DB, $TARGET_CORE_DB 
and $COMPARA_DB and $KILL_LIST into the appropriate places in the config 
file, and to set INPUT_METHOD_LINK_TYPE to $WGA_METHOD_LINK_TYPE. 

The values of other attributes should only be changed if you know what 
you're doing (the .example file gives an overview of the function of each 
attribute).


Pipeline set-up
---------------

(*) Analysis, rule and BatchQueue set-up

Use ensembl-pipeline/scripts/analysis_setup.pl with the following:

<cut>
[WGA2Genes]
module=WGA2Genes
input_id_type=HUMANGENESLICE

[SubmitHumanGeneSlice]
input_id_type=HUMANGENESLICE
</cut>

And ensembl-pipeline/scripts/rule_setup.pl with the following:

<cut>
[WGA2Genes]
condition=SubmitHumanGeneSlice
</cut>

And ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config/BatchQueue.pm
should have the following (minimally; you might want to add some
resource requirements to check loads on the servers containing
$QUERY_CORE_DB, $TARGET_CORE_DB, $COMPARA_DB and $PIPELINE_DB to make
it play nicely with the farm. The results are written to $OUTPUT_DIR,
so set this to something that exists and has lots of space):

<cut>
    {
      logic_name => 'WGA2Genes',
      runnabledb_path => 'Bio/EnsEMBL/Analysis/RunnableDB',
      batch_size => 500,
      resource   => '',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      output_dir => '$OUTPUT_DIR',
      cleanup => 'yes',
    },
</cut>


(*) Input ID generation

The generation of input-ids for the WGA2Genes RunnableDB is an
important step because, due to the way the procedure works, different
segmentations of the query genome can give rise to different
results. The ensembl-pipeline script wga2genes/make_wga2genes_iids.pl
uses alignments from the compara database and genes from the query
database to determine "sensible" regions in the query to process with
a single job.

This script should be run for each top-level slice in the query
genome. However, only top-level sequences that both (a) are aligned
to the target genomes, and (b) contain non-killed genes/transcripts, need 
input ids (for the human genome, this means that haplotypes, which are not 
treated by compara, can be ignored). If the script is not supplied with a 
seq_region_name, it will contruct input ids for all top-level, reference
(i.e. no HAPs) non-duplicate (i.e. PARs present only only once)
sequences. This can take a bit of time, so it pays to parallelise the
task by using the fact that each top-level sequence region can
be treated independently. So you might proceed in the following manner:


% foreach i (1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 X Y)
foreach? perl ensembl-pipeline/scripts/wga2genes/make_wga2genes_iids.pl -dbname $PIPELINE_DB -dbhost *** -dbuser *** -dbpass *** -querydbname $QUERY_CORE_DB -querydbhost *** -querydbport *** -targetdbname $TARGET_CORE_DB -targetdbhost *** -comparadbname $COMPARA_DB -comparadbhost *** -aligntype $WGA_METHOD_LINK_TYPE -logic SubmitHumanGeneSlice -kill $KILL_LIST  -write -seq_region_name $i
foreach? end

(note that here the Mitochondrion is exluded; it is common to do this 
because the annotated mitochrondrion is usally loaded separately for 
the target genome). 

=====================================================
STEP 2: Consolidation and extension of gene scaffolds
=====================================================

Because this process needs to consider the data for all gene scaffolds at 
once, it is performed by a single run of a script. 

- extraction of lines from outfiles from Step 1

% perl -ne '/^\#\#-AGP\s+(.+)/ and print "$1\n";' outdir/*.out > gene_scaffolds.raw.agp
% perl -ne '/^\#\#-GENES\s+(.+)/ and print "$1\n";' outdir/*.out > gene_scaffolds.raw.genes

- Run merge/consolidation script

% perl merge_and_extend_gene_scaffolds.pl -dbname $TARGET_CORE_DB -dbuser *** -dbhost *** -agp gene_scaffolds.raw.agp -genes gene_scaffolds.raw.genes -outagp gene_scaffolds.merged.agp -outgenes gene_scaffolds.merged.genes -outlog merge.log

===============================================
STEP 3: Loading of assembly and gene structures
===============================================

perl load_gene_scaffolds.pl -dbname $TARGET_CORE_DB -dbuser *** -dbpass *** -dbhost *** -asm_coord_sys_name genescaffold -cmp_coord_sys_name scaffold -seqlevel -directlevel gene_scaffolds.merged.agp 

perl load_transcripts.pl -dbname $TARGET_CORE_DB -dbuser *** -dbpass *** -dbhost *** -logic WGA2Genes gene_scaffolds.merged.genes


===================
STEP 6: Extra stuff
===================

Pseudogene analysis
-------------------

Because the system routinely produces transcript structures that include
exons placed in gaps, the standard Pseudogene-calling code is inappropriate
for these 2x builds. A specilised derivative RunnableDB, Pseudogene2x, should
be run instead. The config and set-up are the same as the standard system. 
However, it might be advisable to increase PS_FRAMESHIFT_INTRON_LENGTH from
the standard 9 to around 20. 

xrefs
-----

In order that the web-site contains links to the human source gene for 
each projected entity, it is necessary to add this information as xrefs. 
This can be done by the following script, which should be run *after*
the main core xrefs have been loaded:

perl add_source_xrefs.pl -dbname $TARGET_CORE_DB -dbuser *** -dbpass *** -dbhost *** -srcdbname $SOURCE_CORE_DB -srcdbuser *** -srcdbpass *** -srcdbhost *** -srcdbport ***

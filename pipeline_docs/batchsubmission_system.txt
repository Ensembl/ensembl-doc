This document will describe how the ensembl pipeline system utilises which
even batch submission system is availible to it.

Code
----

This refers to code which is found in

ensembl-pipeline

but you will use it along side this code

ensembl
ensembl-analysis
bioperl-live (bioperl-release-1-2-3)


Batch Submission
----------------

A batch submission module is the module used by the pipeline system 
to create commandlines and actually run jobs. The module used in 
the one specified in the pipeline config file 
Bio::EnsEMBL::Pipeline::Config::BatchQueue by the variable 
QUEUE_MANAGER. This module is assumed to live under 
Bio::EnsEMBL::Pipeline::BatchSubmission

This module is used to construct the commandline which should be 
run, run the command and copy any output where appropriate

The base class Bio::EnsEMBL::Pipeline::BatchSubmission provides 
several standard methods which match the standard constructor used 
by the pipeline system but each child system needs to implement 
several methods themselves. Some of these methods are names for how 
we use them with LSF but could be hijacked for different 
functionality in different systems.

The base functionality provided by BatchSubmission includes this

container methods for

stdout_file, location for stdout file
stderr_file, location for stderr file
id submission, id (normally generated by the submission program)
parameters, any other commandline parameters for submission program
pre_exec, a pre exec check to ensure the code will run sucessfully
command,  the command line to be run
queue, the queue the job should be submitted to (LSF specific)
jobname, the name of the job (LSF specific)
nodes, the specific nodes to run the code on (LSF specific)
resource, arguments for the -R flag (LSF specific)

There are also 4 methods which simply throw if they don't exist

construct_command_line, this method should make your submission 
                        commandline
open_command_line, this should actually open the command, submitting
                   the job
copy_output, this should copy the output to the specified location
delete_output, this should remove the output from the  temporary 
               location

These methods must be implemented in any new BatchSubmission
module.

Currently there are two functional modules LSF and Local.

LSF creates and submits bsub lines. This is the system we use most 
here. 
Local will run jobs serially on one machine by simply running the 
runner script

Local
-----

As mentioned earlier this module will serially run any runner 
command it is given. It redirects the output into temporary files 
then copies them using the File::Copy copy  method to the specified 
locations.

If you wish to retain any output from your local jobs you need
to specify no in the cleanup option in the BatchQueue config file
otherwise the copy methods are not called

This module does also have a job_stats method which simply returns 
a hash with a single value. This is a hack to allow the system to 
work with the ensembl-pipeline/test_system but it shouldn't affect 
the normally running of the pipeline


LSF
---

This is the system our pipeline is most frequently run with and
the system our configuration is geared towards

This module has the 4 standard methods which do as described
above. If yes is specified for cleanup a small stdout file
will be seen as LSF copies this by default

If no is specified for cleanup both stdout and stderr files will
be seen.


The LSF module has to other optional methods which are used by the
pipeline system if present. The first is job_stats which returns a 
hash keyed on lsf id with a value of the job status. This allows
the pipeline to work out how many jobs of different status there
are and act accordningly. The second is check_existance. This takes
a hash of submission ids from the job table and checks if LSF is
aware of these ids. If it isn't the job id is added to a list and
returned to be marked at AWOL.



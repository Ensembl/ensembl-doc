=======================================================
  Contents
  A. Overview
  B. Requirements
  C. Schema
  D. Analyses/methods
  E. Input ids
  F. Configuration
  G. Instructions / Checklist
  H. Appendix
=======================================================



=======================================================
  A. Overview
=======================================================

This document describes the process of annotating the protein sequences in 
an Ensembl database (i.e. translations of Ensembl genes) with domains and
other regions of interest (e.g. low complexity).

========================================================
  B. Requirements
========================================================

Code
----

The following ensembl perl cvs trees are required:

ensembl
ensembl-pipeline
ensembl-analysis

In addition, you also need bioperl which again, is freely availible

bioperl-live (bioperl-release-1-2-3)

The document overview.txt should give you instructions on how to
download this code. 

Database
--------

The database should contain genes, with translations, and DNA. 

The dependencies for each individual analysis method (data and code)
are described below. 


================================================
  C. Schema
================================================

All the results from the protein annotation are stored in the protein_feature
table. This is defined in the core schema which can be found in 
ensembl/sql/table.sql

These are the columns :

protein_feature_id, Unique identifier of the protein feature
translation_id,     Identifier of the translation that the protein feature is
                    found on
seq_start           the alignment start point on the translation    
seq_end             the alignment end point on the translation   
hit_start           the alignment start point on the protein feature
hit_end             the alignment end point on the protein feature
hit_id              name / acession of the aligned feature
analysis_id         identifier of the analysis object
score               score (if provided)
evalue              E value (if provided)
perc_ident          percent identity (if provided)



=====================================================
  D. Analyses/methods
=====================================================

The ensembl-analysis cvs tree currently contains support for 12
distinct protein annotation analysis (databases/methods). There is
ostensibly a RunnableDB for each analysis, although some analyses are
performed by the same RunnableDB. In the descriptions below, the DATA 
sections denote the files that need to be updated when moving to a newer
release of the database (assuming that the scanning method requires no
change, which of course should be checked before updating). 

Seg
---

DESCRIPTION : Prediction of low-complexity regions
PROGRAM     : seg
MODULE      : RunnableDB/ProteinAnnotation/Seg.pm
DATA        : None


Signalp
-------

DESCRIPTION : Prediction of signal-peptide cleavage sites
PROGRAM     : signalp
MODULE      : RunnableDB/ProteinAnnotation/Signalp.pm
DATA        : None


Ncoils
------

DESCRIPTION : Prediction of coiled-coil regions
PROGRAM     : ncoils
MODULE      : RunnableDB/ProteinAnnotation/Coil.pm
DATA        : None


Tmhmm
-----

DESCRIPTION : Prediction of Trans-membrane domains
PROGRAM     : decodeanhmm
MODULE      : RunnableDB/ProteinAnnotation/Tmhmm.pm
DATA        :
 model file   (/usr/local/ensembl/lib/TMHMM2.0.model)
 options file (/usr/local/ensembl/lib/TMHMM2.0.options)


Prints
------

DESCRIPTION : hits to the Prints database of domain fingerprints
PROGRAM     : FingerPRINTScan
MODULE      : RunnableDB/ProteinAnnotation/Prints.pm
DATA        :
 Fingerprint file (/data/blastdb/Ensembl/interpro_scan/prints.pval)


PrositePatterns
---------------

DESCRIPTION : Hits to the Prosite Patterns database of domain regexps. 
PROGRAM     : None
MODULE      : RunnableDB/ProteinAnnotation/PrositePatterns.pm
DATA        :
 Pattern file (data/blastdb/Ensembl/interpro_scan/prosite.patterns)
 Confirmation file (/data/blastdb/Ensembl/confirm.patterns)


PrositeProfiles
---------------

DESCRIPTION : Hits to the Prosite Profiles database of protein domain profiles
PROGRAM     : pfscan 
MODULE      : RunnableDB/ProteinAnnotation/PrositeProfiles.pm
DATA        :
 Profile file (/data/blastdb/Ensembl/interpro_scan/prosite_prerelease.prf)


Pfam
----

DESCRIPTION : Hits to the Pfam database of protein domain HMMs
PROGRAM     : hmmpfam
MODULE      : RunnableDB/ProteinAnnotation/Pfam.pm
DATA        :
 Full-length hit HMM library file (/data/blastdb/Ensembl/Pfam_ls)
 Fragment-hit HMM library file (/data/blastdb/Ensembl/Pfam_fs) 


Tigrfam
-------

DESCRIPTION : Hits to the TIGRfam database of protein domain HMMs
PROGRAM     : hmmpfam
MODULE      : RunnableDB/ProteinAnnotation/Hmmpfam.pm
DATA        : 
 HMM libary file (/data/blastdb/Ensembl/interpro_scan/TIGRFAMs_HMM.LIB)


Superfamily
-----------

DESCRIPTION : Hits to the Superfamily database of protein domain HMMs
PROGRAM     : hmmpfam
MODULE      : RunnableDB/ProteinAnnotation/Superfamily.pm
DATA        :
 HMM library file (/data/blastdb/Ensembl/interpro_scan/superfamily.hmm)


Smart
-----

DESCRIPTION : Hits to the SMART database of protein domain HMMs
PROGRAM     : hmmpfam
MODULE      : RunnableDB/ProteinAnnotation/Hmmpfam.pm
DATA        :
 HMM library file (/data/blastdb/Ensembl/interpro_scan/smart.HMMs)


PIRSF
-----

DESCRIPTION : Hits to the PIR superfamily database of protein HMMs
PROGRAM     : hmmpfam
MODULE      : RunnableDB/ProteinAnnotation/PIRSF.pm
DATA        : 
 Superfamily HMM library file (/data/blastdb/Ensembl/interpro_scan/sf_hmm)
 Sub-family HMM library file (/data/blastdb/Ensembl/interpro_scan/sf_hmm_sub)
 Thresholds file (/data/blastdb/Ensembl/interpro_scan/pirsf.dat)



===============================================
  E. Input ids
===============================================

The RunnableDBs support two types of input ids: (1) translation
internal database identifiers (dbIDs); and (2) names of fasta files
containing pre-dumped translations.  

The reason for this is that the computational resources required
varies by analysis. For analyses that are not so computationally
demanding, it is most efficient to pre-dump the peptides in a set of
one-or-more multi-entry fasta files, each containing a chunk of
peptides, and to submit a job for each file (chunk). In these cases,
each input id is the name of one if these pre-dumped files.  

For the more computationally intensive analyses however, it is
most practical to submit a separate job for each individual
peptide. In these cases, each input_id is a translation internal
database identifier (and the system dumps the corresponding peptide on
the fly). 

In practice, we have found that most of the HMM-search based analyses
fit into the second category, everything else falling into the
first. For the file-based analyses, it is technically possible to have
a separate chunking granularity for each analysis; for example,
analysis A could be run on files of 50 entries each, wheras analysis B
could be run on fewer, larger files (say 200 entries each). Again in
practice though, we have found 2 levels of granularity to be
sufficient; firstly, a single file containing the whole proteome, and
secondly, a split of the proteome into ~100 entries per file. 

Instructions for how to generate the input ids for a pipeline run are
given later. 




===================================================
  F. Configuration
===================================================

The configutation is defined in the file:

Bio::EnsEMBL::Analyis::Config::ProteinAnnotation.pm

At present, the purpose of the configuration file is to define, for
those analyses with a file input id type, the base directory of
the files. Extracting this information into configuration has two
purposes: (1) it makes it unnecessary to have fully-qualified
path/file names as database input ids, which may be very long; and (2)
it provides a template/place-holder for future development of the
system (which may necessitate a more complex configuration). 

The base directory for an analysis is defined by setting BASE_DIR
variable in the hash reference with the corresponding logic_name as
the key. For example:

Prints => {
  BASE_DIR => '/dir/containing/peptide/chunk/files/';
}

In the RunnableDB, the input id is appended to this base directory to
ontain the fully qualified name of the file to be analysed. 

Since many analyses share the same base directory, it saves typing to
define this in the DEFAULT entry, and then to add extra
logic-name-keyed entries for which the value will be
different. For example

DEFAULT => {
  BASE_DIR => '/dir/containing/peptide/chunks/files/';
},

Seg => {
  BASE_DIR => '/dir/containing/whole/proteome/file/';
}


(Note: when an analyses is run with a translation database internal
identifier given as input id, the BASE_DIR is ignored; so it is not
necessary to define any entries in the config for these analyses). 




===========================================
  G. Instructions / Checklist
===========================================


(*) Dump the peptide sequences

This is done using a script called dump_translations.pl found in ensembl-analysis/scripts/protein

An example commandline is:

perl dump_translations.pl -dbname yourdatabase -dbhost yourhost 
-dbuser youruser  -db_id > /protein/data/path/all_peptides.fa

It will print to stderr warnings about transcripts which don't translate.
An important factor about this dumped file is the fasta header. The first 
element after the > must be the translation internal id as these is how the
RunnableDB make the correct associations when writing the information
back to the database. This is ensured by supplying the -db_id argument. 

(*) Chunk the peptide fasta file

This produces a new set of files in a separate directory, each file containing
the same number of fasta entries. This files each represent a feasible chunk of
the whole proteome to pass to an analysis at once. The script lies in 
ensembl-analysis/scripts and is called chunk_fasta_file.pl

mkdir /protein/data/path/chunks
perl chunk_fasta_file.pl /protein/data/path/all_peptides.fa /protein/data/path/chunks 100

The last argument is the number of proteins to place in each chunk. 


(*) Setup the analysis table

There is an example config file in the appendix which can be used with
analysis_setup.pl to populate the analysis and input_id_type_analysis
tables. The command line goes something like:

perl ensembl-pipeline/scripts/analysis_setup.pl  -dbhost yourhost 
-dbuser yourusername -dbpass yourpass -dbname yourdatabase -read 
-file /your/config/file

(*) Setup the rule tables

There is an example config file in the appendix which can be used with
rule_setup.pl to populate the rule tables. The command line goes
something like this:

perl ensembl-pipeline/scripts/rule_setup.pl  -dbhost yourhost 
-dbuser yourusername -dbpass yourpass -dbname yourdatabase -read 
-file /your/config/file

(*) Set up config

Explained earlier.

(*) Load the input ids

The input ids can be made by the ensembl-pipeline/scripts/make_input_ids 
script. This script is further explained in 
the_ensembl_pipeline_infrastructure.txt but here we will explain which sort of
input ids are generally used and give example command lines for how to
produce them.

As explained earlier, the more computationally intensive analyses
should be run on individual translations, and in these cases, the
input ids are translation internal database identifiers. To populate
the database with such input ids (for the corresponding dummy
analysis; see the_ensembl_pipeline_infrastructure.txt), do:

perl make_input_ids -dbhost yourhost -dbuser yourusername -dbpass yourpass
-dbname yourdatabase -dbport 3306 -translation_ids -logic_name SubmitTranslation

Some analyses are so fast computationally that they can be given the
whole proteome file and run in a reasonable time frame. These analyses
can therefore be run as a single job, with a single input id: the name
of the previously-dumped proteome file (the filename only, not the
fully qualified name; the directory in which this file is located is
specified in the config). To insert this input id for the dummy
analysis that these types of analyses depend on (see
the_ensembl_pipeline_infrastructure.txt) do:  

perl make_input_ids -dbhost yourhost -dbuser yourusername -dbpass yourpass
-dbname yourdatabase -dbport 3306 -single -single_name all_peptides.fa
-logic_name SubmitProteome

Finally, other analyses can be run in a sensible amount of time by
splitting the work into a number of jobs, each of which performs the
analyses on a chunk of peptides. These chunk files were pre-prepared
earlier. To insert the input ids for the dummy analysis that these
analyses depend on, do: 

perl make_input_ids -dbhost yourhost -dbuser yourusername -dbpass yourpass
-dbname yourdatabase -dbport 3306 -file -dir /path/to/chunkdir
-logic_name SubmitChunk

(*) Setup Pipeine::Config::BatchQueue.pm

This is explained in detail in
the_ensembl_pipeline_infrastructure.txt. The appendix contains entries
that have been used in the past and can be cut-and-pasted into the
module, but specific settings may need to be tweaked for best
performance.

(*) Run the pipeline

Like all other pipeline process the protein annotation pipeline can be 
controlled using the ensembl-pipeline/scripts/rulemanager.pl script and 
monitored with ensembl-pipeline/scripts/monitor both of which are explained in
the document the_ensembl_pipeline_infrastructure.txt. Following rulemanager.pls
standard useage pattern should work fine

perl ensembl-pipeline/scripts/rulemanager.pl -dbhost yourhost 
-dbuser yourusername -dbpass yourpass -dbname yourdatabase -dbport 3306 
>& /tmp/rule.txt&



==================================================
  H. Appendix
==================================================

Analysis configuration
----------------------

[Seg]
db=low_complexity
program=seg
program_file=seg
module=ProteinAnnotation/Seg
gff_source=Seg
gff_feature=annotation
input_id_type=PROTEOME

[Signalp]
db=signal_peptide
program=signalp
program_file=signalp
module=ProteinAnnotation/Signalp
gff_source=Signalp
gff_feature=annotation
input_id_type=CHUNK

[Ncoils]
db=coiled_coil
db_file=/usr/local/ensembl/data/coils
program=ncoils
program_file=ncoils
module=ProteinAnnotation/Coil
gff_source=ncoils
gff_feature=annotation
input_id_type=CHUNK

[Tmhmm]
db=transmembrane
program=decodeanhmm
program_file=decodeanhmm
parameters=-modelfile => /usr/local/ensembl/lib/TMHMM2.0.model, -optionsfile => /usr/local/ensembl/lib/TMHMM2.0.options
module=ProteinAnnotation/Tmhmm
gff_source=Tmhmm
gff_feature=annotation
input_id_type=CHUNK

[Prints]
db=prints
db_file=/data/blastdb/Ensembl/interpro_scan/prints.pval
program=FingerPRINTScan
program_file=FingerPRINTScan
module=ProteinAnnotation/Prints
gff_source=Prints
gff_feature=domain
input_id_type=CHUNK

[PrositePatterns]
db=prosite
db_file=/data/blastdb/Ensembl/interpro_scan/prosite.patterns
parameters=-confirm => /data/blastdb/Ensembl/confirm.patterns
module=ProteinAnnotation/PrositePattern
gff_source=Prosite_pattern
gff_feature=domain
input_id_type=CHUNK

[PrositeProfiles]
db=pfscan
db_file=/data/blastdb/Ensembl/interpro_scan/prosite_prerelease.prf
program=pfscan
program_file=pfscan
module=ProteinAnnotation/PrositeProfile
gff_source=Profile
gff_feature=Domain
input_id_type=CHUNK

[Pfam]
db=Pfam
db_file=/data/blastdb/Ensembl/Pfam_ls;/data/blastdb/Ensembl/Pfam_fs
program=hmmpfam
program_file=hmmpfam
parameters=--cut_ga
module=ProteinAnnotation/Pfam
gff_source=Pfam
gff_feature=domain
input_id_type=TRANSLATIONID

[Tigrfam]
db=TIGRfam
db_file=/data/blastdb/Ensembl/interpro_scan/TIGRFAMs_HMM.LIB
program=hmmpfam
program_file=hmmpfam
parameters=--cut_ga
module=ProteinAnnotation/Hmmpfam
gff_source=TIGRFAM
gff_feature=domain
input_id_type=TRANSLATIONID

[Superfamily]
db=Superfamily
db_file=/data/blastdb/Ensembl/interpro_scan/superfamily.hmm
program=hmmpfam
program_file=hmmpfam
parameters=-E 0.02
module=ProteinAnnotation/Hmmpfam
gff_source=Superfamily
gff_feature=domain
input_id_type=TRANSLATIONID

[Smart]
db=Smart
db_file=/data/blastdb/Ensembl/interpro_scan/smart.HMMs
program=hmmpfam
program_file=hmmpfam
parameters=-E 0.01
module=ProteinAnnotation/Hmmpfam
gff_source=Smart
gff_feature=domain
input_id_type=TRANSLATIONID

[PIRSF]
db=PIRSF
db_file=/data/blastdb/Ensembl/interpro_scan/sf_hmm;/data/blastdb/Ensembl/interpro_scan/sf_hmm_sub
program=hmmpfam
program_file=hmmpfam
parameters=-thresholds => /data/blastdb/Ensembl/interpro_scan/pirsf.dat, -options => -E 0.01
module=ProteinAnnotation/PIRSF
gff_source=PIRSF
gff_feature=domain
input_id_type=TRANSLATIONID

[SubmitTranslation]
input_id_type=TRANSLATIONID

[SubmitChunk]
input_id_type=CHUNK

[SubmitProteome]
input_id_type=PROTEOME


Rule configuration
------------------

[Seg]
condition=SubmitProteome

[Pfam]
condition=SubmitTranslation

[Tigrfam]
condition=SubmitTranslation

[Superfamily]
condition=SubmitTranslation

[Smart]
condition=SubmitTranslation

[PIRSF]
condition=SubmitTranslation

[Signalp]
condition=SubmitChunk

[Ncoils]
condition=SubmitChunk

[Tmhmm]
condition=SubmitChunk

[Prints]
condition=SubmitChunk

[PrositePatterns]
condition=SubmitChunk

[PrositeProfiles]
condition=SubmitChunk

[Prodom]
condition=SubmitChunk


BatchQueue.pm QUEUE_CONFIG entries
----------------------------------

    {
      logic_name => 'Seg',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      cleanup    => 'yes',
      output_dir => '/out/dir/seg',
    },

    {
      logic_name => 'Signalp',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      cleanup    => 'yes',
      output_dir => '/out/dir/signalp',
    },

    {
      logic_name => 'Ncoils',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/out/dir/ncoils',
    },

    {
      logic_name => 'Tmhmm',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      cleanup    => 'yes',
      output_dir => '/out/dir/tmhmm',
    },

    {
      logic_name => 'Prints',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      cleanup    => 'yes',
      output_dir => '/out/dir/prints',
    },

    {
      logic_name => 'PrositePatterns',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      queue      => 'normal',
      cleanup    => 'yes',
      output_dir => '/out/dir/prositepatterns',
    },
    {
      logic_name => 'PrositeProfiles',
      batch_size => 1,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/outdir/prositeprofiles',
    },

    {
      logic_name => 'Pfam',
      batch_size => 20,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'long',
      output_dir => '/out/dir/pfam',
    },

    {
      logic_name => 'Tigrfam',
      batch_size => 20,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/out/dir/tigrfam',
    },

    {
      logic_name => 'Superfamily',
      batch_size => 20,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/out/dir/superfamily',
    },

    {
      logic_name => 'Smart',
      batch_size => 20,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/out/dir/smart',
    },

    {
      logic_name => 'PIRSF',
      batch_size => 20,
      resource   => 'linux',
      retries    => 3,
      sub_args   => '',
      runner     => '',
      cleanup    => 'yes',
      queue      => 'normal',
      output_dir => '/out/dir/pirsf',
    },






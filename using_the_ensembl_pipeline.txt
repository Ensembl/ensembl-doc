This document describes the basics of the ensembl pipeline, how the 
tables are used and how they are set up.

Specific tasks such as an ensembl genebuild or rawcompute are 
contained in other documents.

All pipeline documents assume that you already have an ensembl core 
database with sequence loaded (see loading_sequence_into_ensembl.txt). 


If you have any questions please contact ensembl-dev@ebi.ac.uk

Document last updated (10/11/2003)



What you need
=============

A mysql_instance with an instance of the ensembl core database

bioperl-072
ensembl core
ensembl pipeline

all availible from cvs and on the web.

The code you should use is current the main trunk. For the core api use 
the most recent stable branch (branch-ensembl17 13/10/03). There will 
soon be a stable branch of the pipeline which contains this code but 
there isn't currently

The pipeline will probably run with newer versions of bioperl but this 
is the only version we guarantee compatability.

The EnsEMBL pipeline schema
===========================

The ensembl pipeline uses an extra set of tables which need to be loaded
onto a core database using a script which is located in:
ensembl-pipeline/sql/table.sql.

The tables in this extension should be in the same database as the core
database, the main table which the pipeline tables have links to in the 
core schema is the analysis table.

job
---
This contains information about specific jobs, what input_id they are
using, what analysis they are running, where their stdout and stderr are
being redirected, how many times the job has been retried and the id from
whichever submission system has been used. It also now stores the
host the job is being executed on and some temp_dir info from the
submission system

job_status
----------
This contains the record of the various statuses a particular job has,
the current_status has a y flag in the is_current column and the
time records when the entry was made.

Jobs can have 8 possible different statuses. 
CREATED, the job has been created inside the rulemanager
SUBMITTED, the job has been submitted to the batchsubmission system
WAITING, the job has been run inside a batch and while the batch is running
         that particular job
READING, the fetch_input method of the RunnableDB is being called
RUNNING, the run method of the RunnableDB is being called
WRITING, the write_output method of the RunnableDB is being called
FAILED, the job has failed for some reason and stopped running
KILLED, the job has been running for too long and the RuleManager has
	killed it
VOID, A blast job is being run the the sequence has too many masked bases
      to effectively run blast over 

rule_goal
---------
This contains the analysis_id of the analysis that should be completed
when this rule is executed.

rule_condition
--------------
This contains the rule_id of the goal which should be executed together 
with the logic_name of an analysis which needs to be completed before 
this rule can be executed.

Each rule can have more than one condition and all conditions need to
have the same rule_id and must be complete before that rule will be
executed.

You should not have two different rules which have the same goal but
different conditions otherwise this may break the pipeline.

input_id_analysis
-----------------
This contains a record of input_ids together with the analyses which 
have been completed for those input_ids, what type the input id is and 
when that analysis was completed.

input_id_type_analysis
----------------------
This table specifies which type of input_id a particular analysis has.
Analysis can have whatever type you want, but it is good syntax to have
types all in uppercase eg: CONTIG. The only type which is fixed in it's 
spelling/case/syntax is ACCUMULATOR. Further explanations are to be found
later in this document.


Dependancies within the pipeline
================================

There are 4 different types of dependencies that the ensembl pipeline
can handle.

Firstly, the RuleManager script which runs the pipeline finds the input 
ids it should use for a particular job. Which input_ids it takes depend 
on the condition that rule has.

A. No dependencies, these are analyses which don't depend on anything
   being complete before they are run they just need the sequence and
   possibly the assembly to have been loaded. They do however, require 
   a Dummy Analysis which provides the input ids.

   For example, the RepeatMask analysis has no dependancies but in the
   rule_goal table it must have SubmitContig and an input_id type of 
   CONTIG.Before the pipeline is set the input_id_analysis table has 
   entries for each contig under the analysis SubmitContig.

B. Dependencies which use the same input_id: these are analyses which
   depend on another analysis being complete and use the same input id.

   An example of this is a blast against swall, most of our blasts
   run on contigs and all of our blasts depend on RepeatMasker being
   complete so a Swall blast job would depend on RepeatMask being
   complete for a particular contig and that contig having an entry in
   the input_id_analysis table so when the rule was checked it could
   be run.

   Rules can be dependant on more than one condition hence two entries 
   in the rule_condition table under the same rule_id and each condition 
   would need to be complete for the same input_id for it to be able to 
   start the goal analysis.

C. Accumulator dependencies are a type of analysis which require all of 
   the previous analyses to have been run before it can complete.
   The Accumulator analysis runs a runnableDB called Accumulator which
   doesn't do anything except to allow the transition between different 
   input_id types ie: from CONTIG names to CHROMOSOMES for best-in-
   genome type analyses.

D. Dependencies which do not use the same input id type. This is a new
   feature of the pipeline for analyses which do not use the same 
   input_id type. This is done by introducing a Wait for all stages
   between the two analyses which have different input ids.

   To distinguish between different input_ids use Accumulators, 
   described in type C.

   An example would be the Pmatch run for the targetted genewise where
   the Pmatches are run in two stages. First each CHROMOSOME is pmatched
   against a set of peptides, then the Pmatch results across the entire
   GENOME are taken and filtered to get the best in genome hits and those
   within 2%.

   The rules are structured so that the first pmatch stage has no 
   dependancies and only requires a SubmitChromosome dummy analysis 
   which already has entries representing each CHROMOSOME in the 
   input_id_analysis table.

   Both the SubmitChromosome and pmatch analyses have the type 
   CHROMOSOME. There is then an accumulator called Best_Wait with type 
   ACCUMULATOR which depends all the pmatches being complete, when they 
   are all complete the accumulator runs.

   BestPmatch (the best in genome pmatch analysis) then depends on two
   things, the Best_Wait accumulator and a SubmitGenome dummy analysis
   which already has an entry in the input_id_analysis table, given
   that the SubmitGenome entry should already by in the input_id_analysis
   table when the pipeline starts the BestPmatch then just has to wait
   for the Best_Wait accumulator to complete which will happen once
   all the Pmatch analyses have run.


This diagram illustrates an example of flow rules


 SubmitContig      SubmitChromosome   SubmitGenome     	SubmitSlice
   CONTIG           CHROMOSOME         GENOME            SLICE
      ^                 ^                 ^                 ^
  A   |             A   |              A  |              A  |
  RepeatMask          Pmatch              |                /         
    CONTIG          CHROMOSOME            |              /    
      ^                 ^                 |            /      
  B   |             C   |                /           /        
    Swall            Best_Wait         /           /         
   CONTIG           ACCUMULATOR      /           /            
      ^                 ^          /           /              
  C   |             D   |        /           /               
  Blast_Wait         BestPmatch/           /                
 ACCUMULATOR           GENOME            /                 
      ^                 ^              /                 
      |                 |            /               
      |           TargettedGenewise/                  
      |               SLICE
      |                 ^
    D |             B   |
      |___________SimilarityGenewise
                      SLICE     


The flow between rules are arrows labeled with either A, B or C  
(explained in A, B and C type dependencies above) all other connections 
are covered by type D.

Rules can have more than one condition. In this situation there are
multiple conditions for a single rule where one of the two conditions is
an ACCUMULATOR. If there are multiple conditions which are not
accumulators all the conditions should provide the same type of input_id.


Setting up the database
=======================

You should already have an ensembl core database with sequence in it. 
Now a description of what you need to setup/run the pipeline system 
follows, and how setup those analyses in the rule flow described above.


1. First add the pipeline tables to your ensembl core database:

  mysql -hecs1a -u***REMOVED*** -p*** ensembl_db 
  < ensembl-pipeline/sql/table.sql

2. Then fill in the analysis table which is part of the core schema
   

There are two ways to fill in an analysis table. 

The script:

ensembl-pipeline/scripts/add_analysis

This will allow you to add individual lines to an analysis table. This
script depends on the database already having the pipeline tables included

The second method will fill the analysis table and input_id_type_analysis 
table all at once. This method takes a conf file which has this format

[RepeatMask]
db=repbase
db_version=020713
db_file=repbase
program=RepeatMasker
program_version=1
program_file=RepeatMasker
parameters=-low, -lib, /acari/work5a/lec/briggsae.lib 
module=RepeatMasker
module_version=1
gff_source=RepeatMasker
gff_feature=Repeat
type=CONTIG

each entry in the analysis table should have one of these in the file. The
header is the logic_name and the other key value pairs are for the other 
columns in the analysis table and one pair for the input_id_type_analysis
table

This script can also be pointed at a complete database to produce a config
file

this script can also be told not to expect pipeline tables

Both these script have a -help option which should print out the 
documentation from the script

The fields in the analysis table
--------------------------------

logic name 	is the name for your analysis, keep it informative,
db 		is the name for your database,
db_version 	the version of your database,
db_file 	is a path to a database or matrix file required for 
		blast, the path can be taken from the environment 
		variable BLASTDB (eg: /data/blastdb). If you want to 
		use the a different database 
		eg: /data/blastdb/Ensembl/swall then 
		type Ensembl/swall in the db_file column,
program 	this is the name for the program,
program_version version of program,
program_file 	this is the file for the program, if the program is
		located in the directory specified in BIN_DIR in 
		General.pm (see later in docs) you only need give the 
		program name otherwise it is best to specify the full 
		path,
parameters 	any commandline parameters your program might need,
		the format should be a comma delimited list, if the 
		option is just a commandline flag e.g -gi or -low that 
		is what should appear but if it is a flag and a value 
		the entry should look like this: 
		'-lib => /path/to/library_file'. For example the 
		parameters string '-low, -lib 
		=> /ecs2/work1/lec/briggsae.lib' would be parsed to 
		produce a command line like this:
		/usr/local/ensembl/bin/RepeatMasker  -low -lib,
		/ecs2/work1/lec/briggsae.lib AC091216.24470.2980374.seq
module 		this should either be the name of the module which is 
		located in Bio::EnsEMBL::Pipeline::RunnableDB or if 
		located elsewhere, the full name of the module 
		and module_version are required,
gff_source 	the source or program which produces these results,
gff_feature 	the type of feature e.g repeat, homology or gene,

The input_id_type_analysis table is also filled in by the 
add_Analysis script. This table links the analysis_id to an 
input_id_type.

The input_id types can in theory be anything. The only fixed
input id type is ACCUMULATOR, analyses which are to act as
ACCUMULATOR's should have the type ACCUMULATOR and use the module
Accumulator. Other than that the only guideline is that any analysis
which use the same set of input_ids should share an input_id type.
Examples of this are, if an analysis uses contig names as input_ids they
all should be of type CONTIG but if they use slice names ie:
chr_name.start-end the size of the slices should all be the same 
and therefore, a combination of say 5MB and 1MB slices would
produce many repeat results.


3. Now you need to fill in the rule tables
------------------------------------------

There are two rule tables:

rule_goal 
---------
Contains a rule_id and an analysis_id (labelled goal). The analysis_id 
points to the analysis which should be run when the rule is executed.

rule_condition 
--------------
Contains rule_id and a logic_name (labelled condition) which is the 
logic_name of an analysis that that rule depends on; a single
rule can have multiple conditions.

Below is an example of the rule_goal table and the rule_condition table
and a subset of the data from the analysis and input_id_type_analysis
tables.


analysis_id/logic_name/input_id_type
+-------------+---------------------+---------------+
| analysis_id | logic_name          | input_id_type |
+-------------+---------------------+---------------+
|           2 | TGE_gw              | SLICE         |
|           3 | Pmatch              | CHROMOSOME    |
|           4 | Pmatch_Wait         | ACCUMULATOR   |
|           5 | similarity_genewise | SLICE         |
|          11 | Swall               | CONTIG        |
|          13 | BestPmatch          | GENOME        |
|          15 | RepeatMask          | CONTIG        |
|          25 | SubmitContig        | CONTIG        |
|          26 | SubmitSlice         | SLICE         |
|          27 | SubmitChromosome    | CHROMOSOME    |
|          28 | Best_Wait           | ACCUMULATOR   |
|          29 | SubmitAll           | GENOME        |
|          30 | Blast_Wait          | ACCUMULATOR   |
+-------------+---------------------+---------------+


rule_goal
+---------+------+
| rule_id | goal |	logic_name  type
+---------+------+
|       3 | 15   |	RepeatMasker CONTIG
|      14 | 11   |	Swall	     CONTIG
|      17 | 30   |	Blast_Wait   ACCUMULATOR
|      19 | 3    |	Pmatch       CHROMOSOME
|      20 | 28   |	Best_Wait    ACCUMULATOR
|      23 | 13   |	BestPmatch   GENOME
|      24 | 4    |	Pmatch_Wait  ACCUMULATOR
|      26 | 2    |	TGE_gw       SLICE
|      27 | 5    |	similarity_genewise	SLICE
+---------+------+

rule_conditions
+---------+------------------+
| rule_id | condition        |
+---------+------------------+
|       3 | SubmitContig     |
|      14 | RepeatMask       |
|      17 | Swall            |
|      19 | Raw_Wait         |
|      19 | SubmitChromosome |
|      20 | Pmatch           |
|      23 | SubmitAll        |
|      23 | Best_Wait        |
|      24 | BestPmatch       |
|      26 | Pmatch_Wait      |
|      26 | SubmitSlice      |
|      27 | TGE_gw           |
|      27 | Blast_Wait       |
+---------+------------------+


These rules should have a rule flow as shown in the diagram for the
dependancies.

One chain from that diagram will be used to represent the rules, 
this is the first chain which starts with SubmitContig.

The RepeatMask analysis has no dependancies but requires a Dummy Analysis 
called SubmitContig which already has entries in the input_id_analysis 
table in order for the RuleManager script to run.

Once a RepeatMask job has run sucessfully entries should be written to
the input_id_analysis table. After each sucessful RepeatMask job has
finished a Swall job can then run.

Blast_Wait an ACCUMULATOR type job is dependant on Swall, logic within
the RuleManager script and the modules it uses, recognise that Blast_Wait
is an ACCUMULATOR and will only start this job once all Swall jobs are 
complete.

Similarity_Genewise is dependant on both TGE_gw jobs being finished
and the Blast_Wait job being run. The jobs get their input_ids from the
TGE_gw analysis, the Blast_Wait marker is just an entry in the 
input_id_analysis table to mark the accumulator being finished.

You can add rules to the rule table using the script:
 ensembl-pipeline/scripts/RuleHandler.pl

An example commandline is given:

 ./RuleHandler.pl -dbhost ecs1a -dbname pipeline_db -dbuser ***REMOVED***
 -pass ensro -insert -goal BestPmatch -condition SubmitAll
 -condition Best_Wait

More information about this script is available with the -help flag.
This script can also be used to display the rules and analysis and
also to delete rules. The fact that it can display the rules can
sometimes be very useful as it allows you to see the logic_names of 
both the rule goals and the rule conditions

this sql will also give you a summary

select rule_goal.rule_id, analysis.logic_name as goal, 
rule_conditions.condition from rule_goal, rule_conditions, analysis 
where analysis.analysis_id = rule_goal.goal and 
rule_goal.rule_id = rule_conditions.rule_id order by rule_id;


4. Filling in the input_id_analysis table
-----------------------------------------

To start the pipeline a series of entries are required in the 
input_id_analysis table. These entries fulfil two functions. First
they fulfil conditions which exist in the rule_condition table but
which don't actually match an analysis which needs to be run. Secondly
They provide a list of input ids each associated with a type which 
the RuleManager can check against the rules to see what to run.
The 'fake' conditions in the rule table as matched with 'dummy'
analyses in the analysis table. The dummy analyses generally have names
like SubmitContig or SubmitSlice but they can be called anything. These
analyses act as conditions for analysis which don't depend on anything
like Repeatmasker or Pmatch. Each different type of input_id
needs a different dummy analysis object in order to start its chain
of dependancy. The entries for these dummy analyses can be filled
in to the input_id_analysis table using the make_input_ids script
which lives n  ensembl-pipeline/scripts/

In my rule flow example the dummy entries which require
entries in the analysis table before the pipeline was started are
SubmitContig, SubmitChromosome, SubmitAll and SubmitSlice.


This script currently lets you generate 5 different types of input_ids

contig- this uses contig names.

slice- creates slice names in the format chr_name.start-end.

file- takes a directory and uses the files in that directory as
      input_ids, a regex can also be specified so only a subset of 
      the files are used.

single- this enters a single input_id which can be used for whole 
	genome analyses.

translation_ids- this is for the protein annotation analysis which run
		 on single proteins

An example commandline could look like:

 ./make_input_ids -dbhost ecs1a -pass *** -user ***REMOVED*** 
 -dbname pipeline_db -slice -slice_size 1000000

Producing a list of input_ids such as:

 1.1-1000000
 1.1000001-2000000
 2.1-1000000
 2.1000001-1543529

If you want your slice ids to cover whole chromosomes you need to make
your slice length longer than the longest chromosome as the code will not
make slices longer than the length of the current chromosome.


Configuration
=============

Once the database is setup the final task is to complete the 
configuration. The config files are all in: 
ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config

The files in the intial checkout will be appended with a .example 
which needs removing   e.g cp General.pm.example General.pm

There are 3 files which need editing for the pipeline to work: 
 General.pm, BatchQueue.pm and Blast.pm.


General.pm
----------
General.pm contains general settings for the pipeline plus other 
options, discussed later.

BIN_DIR, DATA_DIR, LIB_DIR are the directories for most of the 
binaries/data files/lib files. These variables are used to locate 
programs/data at runtime.

PIPELINE_WORK_DIR is the directory where analyses will be run,
this is set up in RunnableI and as default will be /tmp/.

PIPELINE_RUNNER_SCRIPT is the location of the script to run the pipeline 
and is usually: ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/runner.pl.

MAX_JOB_TIME this is the maximum length of time the RuleManager will
let a job have the status running for. For this method to work the
BatchSubmission module being used has to of implemented the get_job_time
method. This is implemented in the LSF module. If this method isn't 
implemented this functionalilty will never be used.

KILLED_INPUT_IDS, this is the path to a file where the input_id, logic_name
and module of a killed job is written. The status of a job which is killed
is also set to KILLED

Both MAX_JOB_TIME and KILLED_INPUT_IDS can be overided in RuleManager
on the commandline


BatchQueue.pm
-------------
BatchQueue.pm 	contains settings for the job object and the batch submission 
		system.

QUEUE_MANAGER 	is the module used for submitting jobs to the batch 
		submission system. These modules are located in: 
		Bio::EnsEMBL::Pipeline::BatchSubmission. 

		There are currently two modules which will submit jobs, 
		one to an LSF system and the other to a GridEngine 
		(from Sun).

		If a different submission system is required then use a 
		module which inherits from 
		Bio::EnsEMBL::Pipeline::BatchSubmission and implements 
		two methods, construct_command_line and open_command_line
		, to submit the analysis.

MAX_PENDING_JOBS is the maximum number of pending jobs allowed before 
		 the RuleManager script will stop submitting more jobs 
		 to the farm.

AUTO_JOB_UPDATE is true ie: set to 1 it automatically updates when a 
		job's status is changed. This will not occur if set to 0.
		Usually, it is left set to 1.
		
DEFAULT_BATCH_SIZE  -
DEFAULT_RETRIES      | series of default values, which will be used if
DEFAULT_BATCH_QUEUE  | there is no entry for an analysis logic name
DEFAULT_OUTPUT_DIR   |
DEFAULT_CLEANUP     -	

QUEUE_CONFIG    an array of hashes, each element contains information 
		about a particular analysis type (keyed in Job.pm 
		on logicname).
		{		
		logic_name => 'CpG', logic name of a particular 
		analysis, eg:CpG.
		
		batch_size => 1, number of jobs to give to the runner 
		script at once. Long jobs set to 1 but for short jobs 
		this is generally set this to 10.
		
		resource   => resource requirements which need to be 
		passed to a batch if running with LSF, if you are using
		another batch submission system you may which to use this
		slot to store other information, for resource 
		requirements an example would beSanger has a mixed 
		OS architecture farm so it is possible to specify either
		alpha or linux to improve performance of certain 
		analysises.
		
		retries    => 3, number of times the RuleManager should 
		retry a job
		
		sub_args   => any other commandline args job submission 
		system may need
		
		runner     => specify a different runner script for a 
		particular analysis rather than that in General.pm.
		
		queue      => 'acarilong', queue in the batchsubmission 
		system if a system does not use queues this can be left 
		blank.
		
		output_dir => '/acari/work1/lec/out/', paths for stderr 
		and stdout.
		
		cleanup    => 'yes', if yes, the job to delete stdout 
		and stderr when it finishes successfully, if no the 
		output will be kept	
   }		   

Resource and sub_args are used to pass information into the 
batchsubmission object when it is created. In theory, they could be used
to pass any information, provided the construct_command_line method knows
how to handle it. Their function relates to how they are used within the
LSF module, but they are both optional.


Blast.pm
--------
This contains an array of hashes which contain BLAST specific settings 
for each blast database you would like to run against.


  name  - is the entry in the db_file column of the analysis table.

  type -  determines if the BLAST search is against DNA or protein and 
  ensures that the correct type of AlignFeature is created.

  header - the regex to be used to parse the correct id out of the fasta 
  header for a particular hit.

  flavour - either NCBI or WU as these both require a slightly different
  command line construction.

  ungapped - if defined you will get the ungapped pieces of each hit as 
  feature pairs. If the data is to be stored in a standard ensembl
  database this should be set to 0. The blast features are stored as
  align features with cigar strings which describe how the coordinates
  can be split to produce the individual features which make up the
  alignment.

  refilter - if set, then the an internal filtering method will be
  used rather than the FeatureFilter runnable.

  min_unmasked - is the minimum number of consectutive unmasked bases 
  which must be present before a blast will run, as a default we use 10
  for blastn and 15 for blastx.

Once all of the above is this set then it is possible to run the 
pipeline, the following documents give further information on how to run 
a specific analysis:

	running_the_rawcomputes.txt
	running_the_pmatches.txt
	running_the_genebuild.txt
	running_the_protein_annotation.txt

Every pipeline though, is run using one script, RuleManager3.pl and 
monitored using another called, monitor.


RuleManager3.pl
---------------
Script location: ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/
RuleManager3.pl

It has several command line options:

 -dbhost     host where the pipeline database is
 -dbport     port
 -dbuser     user to connect as
 -dbpass     password to use
 -dbname     database name

dbhost, dbuser and dbname are obligatory options which you must provide
if your dbuser needs a password you will also have to provide that and if
your dbconnection needs to be set up on a port which isn't 3306 that will
also need to be included.

 -local           run the pipeline locally without the batch submission 
                  system
 -idlist_file     path to a file containing a list of input ids to be 
		  used
 -runner          path to a runner script (if you want to overide the 
		  setting in BatchQueue.pm)
 -output_dir      path to a output dir (if you want to overide the 
				       setting in BatchQueue.pm)
 -once            run through the RuleManager loop once only.
 -shuffle         before running though the loop shuffle the order of the
                  input ids
 -analysis        only run with these analyses objects, can be 
		              logic_names or analysis ids, this option can appear in
		              the commandline multiple times
 -skip_analysis  don't run with these analyses objects, like -analysis can
                 be logic_names or dbIDs and can appear in the commandline
                 multiple times                 
 -v               verbose mode
 -dbsanity        peform some db sanity checks, can be switched of with 
		  -nodbsanity
 -input_id_types  which input id types to run the RuleManager,
                  this option can also appear in the commandline 
		  multiple times
 -start_from      this is which analysis to use to gather the input ids
                  which will be checked by the RuleManagers central loop
 -accumulators    this flag switches the accumulators on, they are on  
                  by default but can be switched off with: 
		  -noaccumulators 
 -max_job_time    overides the $MAX_JOB_TIME value in General.pm
 -killed_file     overides the path to the $KILLED_INPUT_IDS file in 
		  General.pm
 -queue_manager   overides the $QUEUE_MANAGER in General.pm
 
 -rename_on_retry, this means any output already produced will be 
		   deleted before a job is retried, this defaults to off 
		   currently, it was put in as LSF appends output and error 
		   files and sometimes you don't want this
 -h or -help      print out the help 

Examples

  A standard run of the pipeline would look like this:
    perl RuleManager3.pl -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ****
      -dbname pipeline_db -shuffle

  To specify specific analysis: 
    perl RuleManager3.pl -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ****
     -dbname pipeline_db -analysis RepeatMask -analysis Swall

  To only run the RepeatMask and Swall analyses:
    perl RuleManager3.pl -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ****
     -dbname pipeline_db -analysis RepeatMask -local

  To run RepeatMask analysis locally:
   perl RuleManager3.pl -dbhost ecs2b -dbuser ***REMOVED*** -dbpass ensembl 
    -dbname rat_Jun03_mk2 -shuffle -idlist_file id_list


as a note RuleManager is relatively light on the CPU and is can also be
quite verbose even without the verbose flag so it is generally best to
run it in the background and redirect the output into a file. Also
a RuleManager script will not actually stop itself running unless
it is passed the -once flag. If it isn't it will continue running
ad infinitum untill it is stop manually. If the script recieves a sigterm
though in the form of ^C or a kill command it will stop cleanly but
you do need to remember Your RuleManager will continue running if you
don't stop it



monitor
-------
Monitor is found in ensembl-pipeline/scripts and displays information 
about a pipeline.

It takes standard db connection options of -dbhost, -dbport, -dbuser, -
dbpass and -dbname.

It has a number of commandline options to specify which pipeline 
information to display.

 -current               jobs currently in the database, the data is 
			grouped by both analyses and status
 -current_summary       jobs in the database grouped by status
 -finished              finished jobs in the database grouped by analysis
 -analysis              shows the analysis table
 -rules                 shows the rule_goal table
 -conditions            shows the rule_conditions table
 -status_analysis S:LN  shows the jobs with Status:Logic_Name
 -unfinished            unfinished input_ids & analyses in the database
 -nohits FT:ANAID       shows clones with no hits in 
			Feature_Table:Analysis_ID or 
			Feature_Table:Logic_name
 -assembly TYPE         unfinished input_ids & analyses in the assembly 
			of TYPE
 -input_id INPUT_ID     shows unfinished analyses for INPUT_ID
 -lock                  prints information about the pipeline.lock
 -help                  prints out the perl docs for the script

This script is generally used to watch the progress of the pipeline jobs.
The three most commonly used options are -current, -current_summary and 
-finished.

Example commandlines

  To display the current summary of the pipeline:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -current_summary
 
  To display ALL the input_id analysis combinations:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -unfinished
 
  To limit this to an assembly:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -assembly ChrX

  To limit it to an input_id:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -input_id AL321765.1.1.100009

  To get the list of contigs B WITHOUT hits in the dna_align_feature 
  table for the  analysis with ID 20 
   
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -nohits dna_align_feature:20


The pipeline needs access to a BatchSubmission system to run
in anything other than -local mode

The are modules written for two batch submission systems current
LSF, which we use internally at the sanger and GridEngine which is
freely availible from Sun

Any implemented BatchSubmission modules should inherit from
Bio::EnsEMBL::Pipeline::BatchSubmission and should implement these 4 
methods


construct_commandline, this is the method called in Job which
builds the commandline which needs to be issued to the system

open_commandline, this is a method which is also called in Job
which opens the above commandline submitting the job to the system
and reporting some ID which the Submission system as assigned to the
job

copy_output, this method is called in the runner to copy the output from 
the temporary location the submission system keeps it to where you want
the output saved if you want it saved 

delete_output, this method deletes the output from the temporary place
the submission system was storing it
-----------------------------------------------------------------------

As previouly mentioned these are generic intructions about running the
pipeline. There are 4 types sets of analyses which we use the pipeline
to run here at Ensembl these are

The raw computes, this is an intial set of analyses the contigs in the
database are run through before any other analyses are carried out. These
include blasts against various databases, repeatmasking, ab initio gene
prediction and identification of simple features like cpg island and 
transcription start sites, more information about this process can be 
found in running_the_raw_computes.txt


The genebuild, this is the standard processes we use to predict gene
structures. The core of our analysis is based on gene structures 
predicted by genewise on the basis of protein alignments. The
complete process takes alignments of both proteins and cdnas to the
genome and collapses them down to produce a non redundant set of 
transcripts which can have utrs (added by cdnas), a description of this
process can be found in  running_the_genebuild.txt

The est genebuild, this is where ests are aligned to the genome and are
used to produce a non redundant set of transcripts. These analyses
have exonerate and an algorithm called ClusterMerge at their core. A 
description of the process can be found in running_the_est_genebuild.txt

The protein annotation, these is where the peptide sequences produced
by the genebuild are analysed using algorithms and data such as pfam
prints, tmhmm and signalp. A description of this process can be found
in running_the_protein_annotation.txt 

This document describes the basics of the ensembl pipeline, how the 
tables are used and how they are set up.

Specific tasks such as an ensembl genebuild or rawcompute are 
contained in other documents.

All pipeline documents assume that you already have an ensembl core 
database with sequence loaded (see loading_sequence_into_ensembl.txt). 


If you have any questions please contact ensembl-dev@ebi.ac.uk

Document last updated (19/05/2004)



What you need
=============

A mysql_instance with an instance of the ensembl core database

bioperl-072
ensembl core
ensembl pipeline

all availible from cvs and on the web.

The code you should use is current the main trunk. For the core api use 
the most recent stable branch (branch-ensembl17 13/10/03). There will 
soon be a stable branch of the pipeline which contains this code but 
there isn't currently

The pipeline will probably run with newer versions of bioperl but this 
is the only version we guarantee compatability.

The EnsEMBL pipeline schema
===========================

The ensembl pipeline uses an extra set of tables which need to be loaded
onto a core database using a script which is located in:
ensembl-pipeline/sql/table.sql.

The tables in this extension should be in the same database as the core
database, the main table which the pipeline tables have links to in the 
core schema is the analysis table.

job
---
This contains information about specific jobs, what input_id they are
using, what analysis they are running, where their stdout and stderr are
being redirected, how many times the job has been retried and the id from
whichever submission system has been used. It also now stores the
host the job is being executed on and some temp_dir info from the
submission system

job_status
----------
This contains the record of the various statuses a particular job has,
the current_status has a y flag in the is_current column and the
time records when the entry was made.

Jobs can have 8 possible different statuses. 
CREATED, the job has been created inside the rulemanager
SUBMITTED, the job has been submitted to the batchsubmission system
WAITING, the job has been run inside a batch and while the batch is running
         that particular job
READING, the fetch_input method of the RunnableDB is being called
RUNNING, the run method of the RunnableDB is being called
WRITING, the write_output method of the RunnableDB is being called
FAILED, the job has failed for some reason and stopped running
KILLED, the job has been running for too long and the RuleManager has
	killed it
VOID, A blast job is being run the the sequence has too many masked bases
      to effectively run blast over 

rule_goal
---------
This contains the analysis_id of the analysis that should be completed
when this rule is executed.

rule_condition
--------------
This contains the rule_id of the goal which should be executed together 
with the logic_name of an analysis which needs to be completed before 
this rule can be executed.

Each rule can have more than one condition and all conditions need to
have the same rule_id and must be complete before that rule will be
executed.

You should not have two different rules which have the same goal but
different conditions otherwise this may break the pipeline.

input_id_analysis
-----------------
This contains a record of input_ids together with the analyses which 
have been completed for those input_ids, what type the input id is and 
when that analysis was completed.

input_id_type_analysis
----------------------
This table specifies which type of input_id a particular analysis has.
Analysis can have whatever type you want, but it is good syntax to have
types all in uppercase eg: CONTIG. The only type which is fixed in it's 
spelling/case/syntax is ACCUMULATOR. Further explanations are to be found
later in this document.


Dependancies within the pipeline
================================

There are 4 different types of dependencies that the ensembl pipeline
can handle.

Firstly, the RuleManager script which runs the pipeline finds the input 
ids it should use for a particular job. Which input_ids it takes depend 
on the condition that rule has.

A. No dependencies, these are analyses which don't depend on anything
   being complete before they are run they just need the sequence and
   possibly the assembly to have been loaded. They do however, require 
   a Dummy Analysis which provides the input ids.

   For example, the RepeatMask analysis has no dependancies but in the
   rule_goal table it must have SubmitContig and an input_id type of 
   CONTIG.Before the pipeline is set the input_id_analysis table has 
   entries for each contig under the analysis SubmitContig.

B. Dependencies which use the same input_id: these are analyses which
   depend on another analysis being complete and use the same input id.

   An example of this is a blast against swall, most of our blasts
   run on contigs and all of our blasts depend on RepeatMasker being
   complete so a Swall blast job would depend on RepeatMask being
   complete for a particular contig and that contig having an entry in
   the input_id_analysis table so when the rule was checked it could
   be run.

   Rules can be dependant on more than one condition hence two entries 
   in the rule_condition table under the same rule_id and each condition 
   would need to be complete for the same input_id for it to be able to 
   start the goal analysis.

C. Accumulator dependencies are a type of analysis which require all of 
   the previous analyses to have been run before it can complete.
   The Accumulator analysis runs a runnableDB called Accumulator which
   doesn't do anything except to allow the transition between different 
   input_id types ie: from CONTIG names to CHROMOSOMES for best-in-
   genome type analyses.

D. Dependencies which do not use the same input id type. This is a new
   feature of the pipeline for analyses which do not use the same 
   input_id type. This is done by introducing a Wait for all stages
   between the two analyses which have different input ids.

   To distinguish between different input_ids use Accumulators, 
   described in type C.

   An example would be the Pmatch run for the targetted genewise where
   the Pmatches are run in two stages. First each CHROMOSOME is pmatched
   against a set of peptides, then the Pmatch results across the entire
   GENOME are taken and filtered to get the best in genome hits and those
   within 2%.

   The rules are structured so that the first pmatch stage has no 
   dependancies and only requires a SubmitChromosome dummy analysis 
   which already has entries representing each CHROMOSOME in the 
   input_id_analysis table.

   Both the SubmitChromosome and pmatch analyses have the type 
   CHROMOSOME. There is then an accumulator called Best_Wait with type 
   ACCUMULATOR which depends all the pmatches being complete, when they 
   are all complete the accumulator runs.

   BestPmatch (the best in genome pmatch analysis) then depends on two
   things, the Best_Wait accumulator and a SubmitGenome dummy analysis
   which already has an entry in the input_id_analysis table, given
   that the SubmitGenome entry should already by in the input_id_analysis
   table when the pipeline starts the BestPmatch then just has to wait
   for the Best_Wait accumulator to complete which will happen once
   all the Pmatch analyses have run.


This diagram illustrates an example of flow rules


 SubmitContig      SubmitChromosome   SubmitGenome     	SubmitSlice
   CONTIG           CHROMOSOME         GENOME            SLICE
      ^                 ^                 ^                 ^
  A   |             A   |              A  |              A  |
  RepeatMask          Pmatch              |                /         
    CONTIG          CHROMOSOME            |              /    
      ^                 ^                 |            /      
  B   |             C   |                /           /        
    Swall            Best_Wait         /           /         
   CONTIG           ACCUMULATOR      /           /            
      ^                 ^          /           /              
  C   |             D   |        /           /               
  Blast_Wait         BestPmatch/           /                
 ACCUMULATOR           GENOME            /                 
      ^                 ^              /                 
      |                 |            /               
      |           TargettedGenewise/                  
      |               SLICE
      |                 ^
    D |             B   |
      |___________SimilarityGenewise
                      SLICE     


The flow between rules are arrows labeled with either A, B or C  
(explained in A, B and C type dependencies above) all other connections 
are covered by type D.

Rules can have more than one condition. In this situation there are
multiple conditions for a single rule where one of the two conditions is
an ACCUMULATOR. If there are multiple conditions which are not
accumulators all the conditions should provide the same type of input_id.


Setting up the database
=======================

You should already have an ensembl core database with sequence in it. 
Now a description of what you need to setup/run the pipeline system 
follows, and how setup those analyses in the rule flow described above.


1. First add the pipeline tables to your ensembl core database:

  mysql -hecs1a -u***REMOVED*** -p*** ensembl_db 
  < ensembl-pipeline/sql/table.sql

2. Then fill in the analysis table which is part of the core schema
   

There are two ways to fill in an analysis table. 

The script:

ensembl-pipeline/scripts/add_analysis

This will allow you to add individual lines to an analysis table. This
script depends on the database already having the pipeline tables included


The second method uses the ensembl-pipeline/scripts/analysis_setup.pl and
will fill the analysis table and input_id_type_analysis table all at once.
This method takes a conf file which has this format

[RepeatMask]
db=repbase
db_version=020713
db_file=repbase
program=RepeatMasker
program_version=1
program_file=RepeatMasker
parameters=-low, -lib, /acari/work5a/lec/briggsae.lib 
module=RepeatMasker
module_version=1
gff_source=RepeatMasker
gff_feature=Repeat
type=CONTIG

each entry in the analysis table should have one of these in the file. The
header is the logic_name and the other key value pairs are for the other 
columns in the analysis table and one pair for the input_id_type_analysis
table

This script can also be pointed at a complete database to produce a config
file

this script can also be told not to expect pipeline tables

Both these script have a -help option which should print out the 
documentation from the script

The fields in the analysis table
--------------------------------

logic name 	is the name for your analysis, keep it informative,
db 		is the name for your database,
db_version 	the version of your database,
db_file 	is a path to a database or matrix file required for 
		blast, the path can be taken from the environment 
		variable BLASTDB (eg: /data/blastdb). If you want to 
		use the a different database 
		eg: /data/blastdb/Ensembl/swall then 
		type Ensembl/swall in the db_file column,
program 	this is the name for the program,
program_version version of program,
program_file 	this is the file for the program, if the program is
		located in the directory specified in BIN_DIR in 
		General.pm (see later in docs) you only need give the 
		program name otherwise it is best to specify the full 
		path,
parameters 	any commandline parameters your program might need,
		the format should be a comma delimited list, if the 
		option is just a commandline flag e.g -gi or -low that 
		is what should appear but if it is a flag and a value 
		the entry should look like this: 
		'-lib => /path/to/library_file'. For example the 
		parameters string '-low, -lib 
		=> /ecs2/work1/lec/briggsae.lib' would be parsed to 
		produce a command line like this:
		/usr/local/ensembl/bin/RepeatMasker  -low -lib,
		/ecs2/work1/lec/briggsae.lib AC091216.24470.2980374.seq
module 		this should either be the name of the module which is 
		located in Bio::EnsEMBL::Pipeline::RunnableDB or if 
		located elsewhere, the full name of the module 
		and module_version are required,
gff_source 	the source or program which produces these results,
gff_feature 	the type of feature e.g repeat, homology or gene,

The input_id_type_analysis table is also filled in by the 
add_Analysis script and analysis setup. This table links the analysis_id 
to an input_id_type.

When writing the analysis table even if you using the analysis_setup
script with a core dbadaptor the code will throw if no input_id_type
is specified for the analysis

The input_id types can in theory be anything. The only fixed
input id type is ACCUMULATOR, analyses which are to act as
ACCUMULATOR's should have the type ACCUMULATOR and use the module
Accumulator. Other than that the only guideline is that any analysis
which use the same set of input_ids should share an input_id type.
Examples of this are, if an analysis uses contig names as input_ids they
all should be of type CONTIG but if they use slice names ie:
chr_name.start-end the size of the slices should all be the same 
and therefore, a combination of say 5MB and 1MB slices would
produce many repeat results.


Note if you have multiple databases all analysis tables must be identical 
down to the internal ids otherwise this can break the pipeline


3. Now you need to fill in the rule tables
------------------------------------------

There are two rule tables:

rule_goal 
---------
Contains a rule_id and an analysis_id (labelled goal). The analysis_id 
points to the analysis which should be run when the rule is executed.

rule_condition 
--------------
Contains rule_id and a logic_name (labelled condition) which is the 
logic_name of an analysis that that rule depends on; a single
rule can have multiple conditions.

Below is an example of the rule_goal table and the rule_condition table
and a subset of the data from the analysis and input_id_type_analysis
tables.


analysis_id/logic_name/input_id_type
+-------------+---------------------+---------------+
| analysis_id | logic_name          | input_id_type |
+-------------+---------------------+---------------+
|           2 | TGE_gw              | SLICE         |
|           3 | Pmatch              | CHROMOSOME    |
|           4 | Pmatch_Wait         | ACCUMULATOR   |
|           5 | similarity_genewise | SLICE         |
|          11 | Swall               | CONTIG        |
|          13 | BestPmatch          | GENOME        |
|          15 | RepeatMask          | CONTIG        |
|          25 | SubmitContig        | CONTIG        |
|          26 | SubmitSlice         | SLICE         |
|          27 | SubmitChromosome    | CHROMOSOME    |
|          28 | Best_Wait           | ACCUMULATOR   |
|          29 | SubmitAll           | GENOME        |
|          30 | Blast_Wait          | ACCUMULATOR   |
+-------------+---------------------+---------------+


rule_goal
+---------+------+
| rule_id | goal |	logic_name  type
+---------+------+
|       3 | 15   |	RepeatMasker CONTIG
|      14 | 11   |	Swall	     CONTIG
|      17 | 30   |	Blast_Wait   ACCUMULATOR
|      19 | 3    |	Pmatch       CHROMOSOME
|      20 | 28   |	Best_Wait    ACCUMULATOR
|      23 | 13   |	BestPmatch   GENOME
|      24 | 4    |	Pmatch_Wait  ACCUMULATOR
|      26 | 2    |	TGE_gw       SLICE
|      27 | 5    |	similarity_genewise	SLICE
+---------+------+

rule_conditions
+---------+------------------+
| rule_id | condition        |
+---------+------------------+
|       3 | SubmitContig     |
|      14 | RepeatMask       |
|      17 | Swall            |
|      19 | Raw_Wait         |
|      19 | SubmitChromosome |
|      20 | Pmatch           |
|      23 | SubmitAll        |
|      23 | Best_Wait        |
|      24 | BestPmatch       |
|      26 | Pmatch_Wait      |
|      26 | SubmitSlice      |
|      27 | TGE_gw           |
|      27 | Blast_Wait       |
+---------+------------------+


These rules should have a rule flow as shown in the diagram for the
dependancies.

One chain from that diagram will be used to represent the rules, 
this is the first chain which starts with SubmitContig.

The RepeatMask analysis has no dependancies but requires a Dummy Analysis 
called SubmitContig which already has entries in the input_id_analysis 
table in order for the RuleManager script to run.

Once a RepeatMask job has run sucessfully entries should be written to
the input_id_analysis table. After each sucessful RepeatMask job has
finished a Swall job can then run.

Blast_Wait an ACCUMULATOR type job is dependant on Swall, logic within
the RuleManager script and the modules it uses, recognise that Blast_Wait
is an ACCUMULATOR and will only start this job once all Swall jobs are 
complete.

Similarity_Genewise is dependant on both TGE_gw jobs being finished
and the Blast_Wait job being run. The jobs get their input_ids from the
TGE_gw analysis, the Blast_Wait marker is just an entry in the 
input_id_analysis table to mark the accumulator being finished.

There are two methods for filling in the rule table

You can add rules to the rule table using the script:
 ensembl-pipeline/scripts/RuleHandler.pl

An example commandline is given:

 ./RuleHandler.pl -dbhost ecs1a -dbname pipeline_db -dbuser ***REMOVED***
 -pass ensro -insert -goal BestPmatch -condition SubmitAll
 -condition Best_Wait

More information about this script is available with the -help flag.
This script can also be used to display the rules and analysis and
also to delete rules. The fact that it can display the rules can
sometimes be very useful as it allows you to see the logic_names of 
both the rule goals and the rule conditions

You can also fill in a complete set of rules in a similar manner to
a complete analysis by using the ensembl-pipeline/scripts/rule_setup.pl 
script. This expects a config file which looks like this

[RepeatMask]
condition=SubmitContig

[Pmatch]
condition=SubmitChromosome

[Pmatch_Wait]
condition=Pmatch

[BestPmatch]
condition=Pmatch_Wait
condition=SubmitGenome

the logic_name in the square brackets is the name of the goal analysis
those which follow a condition are the conditional analyses

unlike the analysis table config the same key can appear multiple times
although condition is the only key recognised

this sql will also give you a summary of the rules

select rule_goal.rule_id, analysis.logic_name as goal, 
rule_conditions.condition from rule_goal, rule_conditions, analysis 
where analysis.analysis_id = rule_goal.goal and 
rule_goal.rule_id = rule_conditions.rule_id order by rule_id;


4. Filling in the input_id_analysis table
-----------------------------------------

To start the pipeline a series of entries are required in the 
input_id_analysis table. These entries fulfil two functions. First
they fulfil conditions which exist in the rule_condition table but
which don't actually match an analysis which needs to be run. Secondly
They provide a list of input ids each associated with a type which 
the RuleManager can check against the rules to see what to run.
The 'fake' conditions in the rule table as matched with 'dummy'
analyses in the analysis table. The dummy analyses generally have names
like SubmitContig or SubmitSlice but they can be called anything. These
analyses act as conditions for analysis which don't depend on anything
like Repeatmasker or Pmatch. Each different type of input_id
needs a different dummy analysis object in order to start its chain
of dependancy. The entries for these dummy analyses can be filled
in to the input_id_analysis table using the make_input_ids script
which lives n  ensembl-pipeline/scripts/

In my rule flow example the dummy entries which require
entries in the analysis table before the pipeline was started are
SubmitContig, SubmitChromosome, SubmitAll and SubmitSlice.


This script currently lets you generate 4 different types of input_ids


slice- creates slice names in the format 
  coord_system_name:coord_system_version:seq_region_name:start:end:strand.
  As specified by Bio::EnsEMBL::Slice->name
  this also expects coord system name and slice size. If no size is 
  specified complete seq regions are used

file- takes a directory and uses the files in that directory as
      input_ids, a regex can also be specified so only a subset of 
      the files are used.

single- this enters a single input_id which can be used for whole 
	genome analyses.

translation_ids- this is for the protein annotation analysis which run
		 on single proteins

An example commandline could look like:

./make_input_ids -dbhost host -dbuser user -dbpass *** -dbport 3306 
-dbname my_database -contig


Configuration
=============

Once the database is setup the final task is to complete the 
configuration. The config files are all in: 
ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config

The files in the intial checkout will be appended with a .example 
which needs removing   e.g cp General.pm.example General.pm

There are 3 files which need editing for the pipeline to work: 
 General.pm, BatchQueue.pm and Blast.pm.


General.pm
----------
General.pm contains general settings for the pipeline plus other 
options, discussed later.

BIN_DIR, DATA_DIR, LIB_DIR are the directories for most of the 
binaries/data files/lib files. These variables are used to locate 
programs/data at runtime.

PIPELINE_WORK_DIR is the directory where analyses will be run,
this is set up in RunnableI and as default will be /tmp/.

PIPELINE_RUNNER_SCRIPT is the location of the script to run the pipeline 
and is usually: ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/runner.pl.

RENAME_ON_RETRY this is a option to move the stderr/stdout files from
there existing names to ones appended with .retry*retry_count* as otherwise
LSF at least appends the new output to the end of the old output which
can be confusing when searching for error messages


BatchQueue.pm
-------------
BatchQueue.pm 	contains settings for the job object and the batch submission 
		system.

QUEUE_MANAGER 	is the module used for submitting jobs to the batch 
		submission system. These modules are located in: 
		Bio::EnsEMBL::Pipeline::BatchSubmission. 

		There are currently two modules which will submit jobs, 
		one to an LSF system and the other to a GridEngine 
		(from Sun).

		If a different submission system is required then use a 
		module which inherits from 
		Bio::EnsEMBL::Pipeline::BatchSubmission and implements 
		two methods, construct_command_line and open_command_line
		, to submit the analysis.

MAX_PENDING_JOBS is the maximum number of pending jobs allowed before 
		 the RuleManager script will stop submitting more jobs 
		 to the farm.

AUTO_JOB_UPDATE is true ie: set to 1 it automatically updates when a 
		job's status is changed. This will not occur if set to 0.
		Usually, it is left set to 1.
		
JOB_LIMIT the maximum number of jobs of defined statuses allow to
to exist in the system. If this limit is reached the number of jobs
over the limit is used to calculate how long to sleep for

JOB_STATUSES_TO_COUNT an array of strings each of which is a status
reported by your submission system for jobs in the system. With LSF
jobs can have several different statuses PEND means they are waiting
for a node to run on. RUN means they are running. SSUSP means the job
has been suspended for some reason. EXIT means the job exiting with
a non zero exit code and DONE means the job finished successfully

MAX_JOB_SLEEP the maximum amount of time to sleep for when the job limit
is reached

MIN_JOB_SLEEP the minimum time to sleep for when the job limit is reached

SLEEP_PER_JOB the amount of time to sleep per job over the job limit

DEFAULT_BATCH_SIZE      -
DEFAULT_RETRIES          | series of default values, which will be used if
DEFAULT_BATCH_QUEUE      | there is no entry for an analysis logic name
DEFAULT_OUTPUT_DIR       |  
DEFAULT_CLEANUP     	   |
DEFAULT_RUNNABLEDB_PATH -

QUEUE_CONFIG    an array of hashes, each element contains information 
		about a particular analysis type (keyed in Job.pm 
		on logicname). You need an entry in this array for each analysis you
    intend to run 
		{		
		logic_name => 'CpG', logic name of a particular 
		analysis, eg:CpG.
		
		batch_size => 1, number of jobs to give to the runner 
		script at once. Long jobs set to 1 but for short jobs 
		this is generally set this to 10.
		
		resource   => resource requirements which need to be 
		passed to a batch if running with LSF, if you are using
		another batch submission system you may which to use this
		slot to store other information, for resource 
		requirements an example would beSanger has a mixed 
		OS architecture farm so it is possible to specify either
		alpha or linux to improve performance of certain 
		analysises.
		
		retries    => 3, number of times the RuleManager should 
		retry a job
		
		sub_args   => any other commandline args job submission 
		system may need
		
		runner     => specify a different runner script for a 
		particular analysis rather than that in General.pm.
		
		queue      => 'acarilong', queue in the batchsubmission 
		system if a system does not use queues this can be left 
		blank.
		
		output_dir => '/acari/work1/lec/out/', paths for stderr 
		and stdout.
		
		cleanup    => 'yes', if yes, the job to delete stdout 
		and stderr when it finishes successfully, if no the 
		output will be kept	
    runnabledb_path => 'Bio/EnsEMBL/Pipeline/RunnableDB' the location
    of the perl module specified in the module column of the analysis table

   }		   

Resource and sub_args are used to pass information into the 
batchsubmission object when it is created. In theory, they could be used
to pass any information, provided the construct_command_line method knows
how to handle it. Their function relates to how they are used within the
LSF module, but they are both optional.


Blast.pm
--------
This contains an array of hashes called DB_CONFIG which contain BLAST 
specific settings for each blast database you would like to run against.


  name  - is the entry in the db_file column of the analysis table.

  type -  determines if the BLAST search is against DNA or protein and 
  ensures that the correct type of AlignFeature is created.

  header - the regex to be used to parse the correct id out of the fasta 
  header for a particular hit.

  flavour - either NCBI or WU as these both require a slightly different
  command line construction.

  ungapped - if defined you will get the ungapped pieces of each hit as 
  feature pairs. If the data is to be stored in a standard ensembl
  database this should be set to 0. The blast features are stored as
  align features with cigar strings which describe how the coordinates
  can be split to produce the individual features which make up the
  alignment.

  refilter - if set, then the an internal filtering method will be
  used rather than the FeatureFilter runnable.

  min_unmasked - is the minimum number of consectutive unmasked bases 
  which must be present before a blast will run, as a default we use 10
  for blastn and 15 for blastx.

UNKNOWN_ERROR_STRING is the string you would like passed back to the 
runnabledb is an error unrecognised by the blast stderr parsing appears
We use FAILED as standard so those jobs can be retried but if you want
to see  an unknown error before it retried then you should alter this 
string

Running the pipeline
=====================

Once all of the above is this set then it is possible to run the 
pipeline, the following documents give further information on how to run 
a specific analysis:

	running_the_rawcomputes.txt
	running_the_pmatches.txt
	running_the_genebuild.txt
	running_the_protein_annotation.txt

There are a couple of scripts which can be used to run jobs in the pipeline
and these are all found in ensembl-pipeline


rulemanager.pl
--------------

This script looks at the commandline options and rule tables and
cycles though its given input_ids decide what analyses it can submit to you
compute farm. As standard it considers all rules in the rule table and
checks all unique input_ids for each input_id type against the rules.

Here are it's commandline options

It takes standard db connection options of -dbhost, -dbport, -dbuser, -
dbpass and -dbname.

The following arguments affect the scripts behaviour

   -once this flag means the script will only run through its loop once
    this is useful for testing 
   -reread_rules, this flag will force the rulemanager to reread the rule
    table every loop
   -reread_input_ids, this will force the rulemanager to reread the input
    ids each and every loop
   -accumulators, this is a flag to indicate if the accumulators are 
    running. By default it is on but it is turned off if you specify any
    flag which affects the rules or input_ids sets. You can switch
    it off by specifying -noaccumulators 
   -force_accumulators this forces accumulators on even if other conditions
    would switch it on
   -shuffle this reorders the array of input_ids each time it goes through
    through the loop. The purpose of this is to ensure the ids aren't 
    always checked in the same order'
   -utils_verbosity, this affects the amount of chatter you recieve from
    the core module Bio::EnsEMBL::Utils::Exception. By default this is set
    to WARNING which means you see the prints from warnings and throws but
    not from deprecate and info calls. See the modules itself for more 
    information
   -verbose, toggles whether some print statements are printed in this
    script and in the RuleManager
   -rerun_sleep, this is the amount of time the script will sleep for
    if no jobs have been submitted in the previous loop

Options which by default are taken from config files are 

BatchQueue.pm

  -queue_manager this specifies which 
   Bio::EnsEMBL::Pipeline::BatchSubmission module is used by 
   Bio::EnsEMBL::Pipeline::RuleManager
  -job_limit the maximun number of jobs of specified status allowed in
   system
  -max_job_time the maximum time the rulemanager will sleep for when
   job limit is reached
  -min_job_time the minimum time the rulemanager will sleep for when
   job limit is reached
  -sleep_per_job the amount of time per job the rulemanager will sleep
   for if the job limit is reached
  -output_dir the path to an output directory when the jobs stderr and
   stdout will be redirected. This alwaysoverrides the values specified in
   BatchQueue
  -mark_awol toggle to specify whether to mark jobs as awol if lost from
   the submission system this can apply strain to the job table. It is on
   by default and can be switched off using the -nomark_awol flag

General.pm

   -runner path to a default runner script. This will override what is
    set in General.pm but will be overidden by any analyses specific
    settings found in BatchQueue.pm
   -rename_on_retry a toggle to specify whether to rename stderr/stdout 
    file when a job is retried as otherwise the submission system just
    cats them all together

The following options alter which analyses or input_ids are considered
for the run

  -starts_from this should be a logic_name of an analysis which has been
  run. The input_ids used for the central loop are retrived on the basis of
  these logic_names. This option can appear in the commandline multiple 
  times
  -analysis a logic_name of an analysis you want run. If you specify
  this option you will only run this analysis or analyses as the option can
  appear on the command line multiple times
  -skip_analysis a logic_name of an analysis you don't want to run. If
  this option is specified these are the only analyses which won't be run
  note this option and -analysis are mutually exclusive
  -input_id_type a type of input_id you want to run with. If these is 
  specified no other type of input will be considered. Again this can 
  appear multiple times on the commandline
  -skip_input_id_type a type of input to no be considered. If this appears
  only these input_ids will not be run. Again this can appear multiple 
  times on the commandline and is multially exclusive with -input_id_type
  -input_id_file path to a file of input_ids which should be in the format
  input_id input_id_type
  -skip_input_id_file path to a file of input_ids to not run. The file
  should be in the same format as for -input_id_file

see the perldocs in the script for command line examples

This script should be used if you want your pipeline you run continuously
and submit jobs as and when they can be run. If you just want to run one
analysis at a time there are two other scripts you may want to use

job_submission.pl 
-----------------

This script is for submitting single analyses using the job system. 
As standard it will check the rules to see if a job should be run and check 
with the running jobs to see if a failed job needs resubmitting or if the 
job is already running but the script can be forced to ignore all this and 
just create and submit jobs. It can also create input_ids on the fly rather 
than having to read them from either a file or the input_id_analysis table

Here are its commandline options

It takes standard db connection options of -dbhost, -dbport, -dbuser, -
dbpass and -dbname.

The details of what analysis to run are specified by these options

  -logic_name the logic_name of the analysis you want to run. You 
  must have this analysis already in the analysis table and it
  must has an input_id_type and module specified. This logic
  name is also used to fetch the rule. This analysis should be
  the goal of the rule you want executed
  -force this forces the script to ignore the rule and just create
  and submit the jobs with no regard to whether they should be run
  or are already running. If this option is specifed jobs already 
  in the pipeline running this analysis are ignored and failed
  jobs of this type are not retried

Details on how the rules and jobs are considered

 -utils_verbosity, this affects the amount of chatter you recieve from
  the core module Bio::EnsEMBL::Utils::Exception. By default this is set
  to WARNING which means you see the prints from warnings and throws but
  not from deprecate and info calls. See the modules itself for more 
  information
  -verbose, toggles whether some print statements are printed in this
  script and in the RuleManager
  -config_sanity this is a test to check certain values are defined in
  your General.pm and BatchQueue.pm config files

  Some of the follow options can intially be set in either the General.pm
  of BatchQueue.pm config files see docs of  rulemanager.pl for more 
  details
  
  -queue_manager this specifies which 
  Bio::EnsEMBL::Pipeline::BatchSubmission module is used by 
  Bio::EnsEMBL::Pipeline::RuleManager
  -output_dir the path to an output directory when the jobs stderr and
  stdout will be redirected. This alwaysoverrides the values specified in
  BatchQueue
  -mark_awol toggle to specify whether to mark jobs as awol if lost from
  the submission system this can apply strain to the job table. It is on
  by default and can be switched off using the -nomark_awol flag
  -runner path to a default runner script. This will override what is
  set in General.pm but will be overidden by any analyses specific
  settings found in BatchQueue.pm
  -rename_on_retry a toggle to specify whether to rename stderr/stdout 
  file when a job is retried as otherwise the submission system just
  cats them all together

Details on the input_id generation

 If you specify no options to do with input_ids it will just take 
  all the input_ids from the input_id_analysis table with an appropriate
  input_id_type as specified by the analysis object

  -input_id_file this is a text file in the format input_id input_id_type
  if used these are the only input_ids which are considered
  -make_input_ids this indicates you want to use the InputIDFactory
   to make the input_ids for you. If you specify this option it
   makes you use force too so the rules are ignored as if the
   analysis needs its input ids created it won't pass a rule check'
 
  These are options needed for the manufacture of input ids

  -slice  signals to insert slice type input ids using
  the format 
  coord_system:coord_system_version:seq_region_name:start:end:strand
  -coord_system the coordinate system you want slices in
  -coord_system_version the version of the coord system you want
  -slice_size the size to make the slice ids
  -slice_overlap the slice overlap (non-overlapping by default)
  -file     if the input_ids are to be a list of filenames from a directory
  -dir      the directory to read the filenames from
  -file_regex a regex to impose on the filenames before using them
  -single if you just want a single dummy input_id ie for genome wide 
  analyses
  -single_name , by default this is genome but you can specify something
  different here, for example for protein annotation jobs which use the 
  whole proteome this must be proteome
  -translation_ids if you want your input ids to be translation ids
  -verbose if you want more information about what the script is doing
  -input_id_type if you want to specific an input_id_type not already
  used by the analysis object
  -insert_analysis if you want to insert an analysis object if it doesn't
     already exist in the database'
  -seq_level if you want the ids for the seq_level seq_regions, can
  work with slice_size but the -slice options isn't required'
  -top_level this will fetch all the non_redundant pieces in
  the database this may produce ids which are a mixture of different
  coordinate systems, if -coord_system_version is specified it will
  be ignored

example command lines can be found in the perl docs of the
script


lsf_submission.pl
-----------------

This script creates submission lines using which ever BatchSubmission
module is specified and runs them. As standard it creates commandlines
for the test_RunnableDB script but it can take a custom commandline and use
that instead


Here are its commandline options

It takes standard db connection options of -dbhost, -dbport, -dbuser, -
dbpass and -dbname.

Details of the analysis you wish run

  -logic_name the logic_name of the analysis you want to run. You 
   must have this analysis already in the analysis table and it
   must has an input_id_type and module specified. This logic
   name is also used to fetch the rule. This analysis should be
   the goal of the rule you want executed
  -insert_analysis this tells the script to insert the analysis object
   and requires both the follow options are also used
  -input_id_type this specified what type of input_id
  -module this is the name of the RunnableDB to be used

Details about the submission system

  -queue_manager this specifies which 
  Bio::EnsEMBL::Pipeline::BatchSubmission module is used
  -output_dir which directory to put the stderr and stdout files in
  This is the base directory the script creates 10 directorys from 0-9
  below that where the files are actually placed to ensure that no
  directory gets too many files in it
  -queue the submission system queue the jobs will be submitted to
  -pre_exec toggle if a pre exec command is to be used in the submission
  command
  -resource_requirements any resource requirements for the commandline
  in the LSF system this string will placed after the -R flag
  -sub_args any other arguments you want to pass to your submission system

  All of these options are discretionary and if not specified the
  settings will be taken from the BatchQueue config file

  -open this tells the script to actually open the commandlines to the
  submission system otherwise it will just print the commandlines
  -submission_interval the length of time to sleep for between opening
  submissions by default it is 5 seconds
  
Details about the test_RunnableDB commandline to create

  -script_dir the directory where you copy of test_RunnableDB lives
  if not specified the current working directory is used
  -runnabledb_path the perl path for the runnabledb to be used. Again
  this is discretionary and can be taken from BatchQueue if needed
  -write whether test_RunnableDB should write its output to the database
  -update_input_id_analysis whether test_RunnableDB should update the 
  input_id_analysis table on sucessful completion of a job
  -script_verbosity whether test_RunnableDB should have its verbose flag 
  set

details about custom command options
 
  -command this option allows you to specify an alternative script to 
  test_RunnableDB but you must specify the full alternatve commandline
  on in quotes after this placing the word INPUT_ID in the position in
  the commandline you wish the input_id to appear
  -pre_exec_command if you wish a pre_exec_command to be used with a
  custom commandline you must also specify the pre_exec on the 
  commandline using this options

Details about the input_ids to use

  If you specify no options to do with input_ids it will just take 
  all the input_ids from the input_id_analysis table with an appropriate
  input_id_type as specified by the analysis object

  -input_id_file this is a text file in the format input_id input_id_type
  if used these are the only input_ids which are considered
  -make_input_ids this indicates you want to use the InputIDFactory
   to make the input_ids for you. If you specify this option it
   makes you use force too so the rules are ignored as if the
   analysis needs its input ids created it won't pass a rule check'
 
  These are options needed for the manufacture of input ids

  -slice  signals to insert slice type input ids using
  the format 
  coord_system:coord_system_version:seq_region_name:start:end:strand
  -coord_system the coordinate system you want slices in
  -coord_system_version the version of the coord system you want
  -slice_size the size to make the slice ids
  -slice_overlap the slice overlap (non-overlapping by default)
  -file     if the input_ids are to be a list of filenames from a directory
  -dir      the directory to read the filenames from
  -file_regex a regex to impose on the filenames before using them
  -single if you just want a single dummy input_id ie for genome wide 
  analyses
  -single_name , by default this is genome but you can specify something
  different here, for example for protein annotation jobs which use the 
  whole proteome this must be proteome
  -translation_ids if you want your input ids to be translation ids
  -verbose if you want more information about what the script is doing
  -input_id_type if you want to specific an input_id_type not already
  used by the analysis object
  -insert_analysis if you want to insert an analysis object if it doesn't
     already exist in the database'
  -seq_level if you want the ids for the seq_level seq_regions, can
  work with slice_size but the -slice options isn't required'
  -top_level this will fetch all the non_redundant pieces in
  the database this may produce ids which are a mixture of different
  coordinate systems, if -coord_system_version is specified it will
  be ignored

example command lines can be found in the perl docs of the
script


all these scripts have -help and -perldoc options which will print out
more information about the script

monitor
-------
Monitor is found in ensembl-pipeline/scripts and displays information 
about a pipeline.

It takes standard db connection options of -dbhost, -dbport, -dbuser, -
dbpass and -dbname.

It has a number of commandline options to specify which pipeline 
information to display.

 -current               jobs currently in the database, the data is 
			grouped by both analyses and status
 -current_summary       jobs in the database grouped by status
 -finished              finished jobs in the database grouped by analysis
 -analysis              shows the analysis table
 -rules                 shows the rule_goal table
 -conditions            shows the rule_conditions table
 -status_analysis S:LN  shows the jobs with Status:Logic_Name
 -unfinished            unfinished input_ids & analyses in the database
 -nohits FT:ANAID       shows clones with no hits in 
			Feature_Table:Analysis_ID or 
			Feature_Table:Logic_name
 -assembly TYPE         unfinished input_ids & analyses in the assembly 
			of TYPE
 -input_id INPUT_ID     shows unfinished analyses for INPUT_ID
 -lock                  prints information about the pipeline.lock
 -help                  prints out the perl docs for the script

This script is generally used to watch the progress of the pipeline jobs.
The three most commonly used options are -current, -current_summary and 
-finished.

Example commandlines

  To display the current summary of the pipeline:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -current_summary
 
  To display ALL the input_id analysis combinations:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -unfinished
 
  To limit this to an assembly:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -assembly ChrX

  To limit it to an input_id:
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -input_id AL321765.1.1.100009

  To get the list of contigs B WITHOUT hits in the dna_align_feature 
  table for the  analysis with ID 20 
   
   ./monitor -dbhost ecs2b -dbport 3306 -dbuser ensro -dbname 
   -nohits dna_align_feature:20


The pipeline needs access to a BatchSubmission system to run
in anything other than -local mode

The are modules written for two batch submission systems current
LSF, which we use internally at the sanger and GridEngine which is
freely availible from Sun

Any implemented BatchSubmission modules should inherit from
Bio::EnsEMBL::Pipeline::BatchSubmission and should implement these 4 
methods


construct_commandline, this is the method called in Job which
builds the commandline which needs to be issued to the system

open_commandline, this is a method which is also called in Job
which opens the above commandline submitting the job to the system
and reporting some ID which the Submission system as assigned to the
job

copy_output, this method is called in the runner to copy the output from 
the temporary location the submission system keeps it to where you want
the output saved if you want it saved 

delete_output, this method deletes the output from the temporary place
the submission system was storing it


Pipeline Sanity
--------------

Before running any pipeline you may want to run this script which runs
a series of sanity checks on the config files and pipeline tables before 
you run

This script lives in ensembl-pipeline/scripts

pipeline_sanity.pl

It needs the standard database options -dbhost -dbuser -dbpass -dbname 
-dbport. As standard it will run the sanity checks and report any problems
but if you pass in the -info flag it will also print out some other summary
information about your setup

-----------------------------------------------------------------------

As previouly mentioned these are generic intructions about running the
pipeline. There are 4 types sets of analyses which we use the pipeline
to run here at Ensembl these are

The raw computes, this is an intial set of analyses the contigs in the
database are run through before any other analyses are carried out. These
include blasts against various databases, repeatmasking, ab initio gene
prediction and identification of simple features like cpg island and 
transcription start sites, more information about this process can be 
found in running_the_raw_computes.txt


The genebuild, this is the standard processes we use to predict gene
structures. The core of our analysis is based on gene structures 
predicted by genewise on the basis of protein alignments. The
complete process takes alignments of both proteins and cdnas to the
genome and collapses them down to produce a non redundant set of 
transcripts which can have utrs (added by cdnas), a description of this
process can be found in  running_the_genebuild.txt

The est genebuild, this is where ests are aligned to the genome and are
used to produce a non redundant set of transcripts. These analyses
have exonerate and an algorithm called ClusterMerge at their core. A 
description of the process can be found in running_the_est_genebuild.txt

The protein annotation, these is where the peptide sequences produced
by the genebuild are analysed using algorithms and data such as pfam
prints, tmhmm and signalp. A description of this process can be found
in running_the_protein_annotation.txt 

General checkpoints

1 do you have entries in BatchQueue.pm, General.pm and Blast.pm
2 do you have entries in the analysis, input_id_type_analysis, rule_goal
  rule_conditions and input_id_analysis tables
3 if your pipeline is running against multiple databases do they all have
  identical analysis tables down to the database ids
4 will the pipeline_sanity script run without any problems

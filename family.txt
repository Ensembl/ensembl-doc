 -*- mode: text -*-;

(For maintainer description, see below)

User description (should this go elsewhere?):

  The family database is generated by running Enright and van Dongen's Tribe
  sequence clustering algorithm on a set of peptides consisting of the
  EnsEMBL predictions, together with all vertebrate sequences from SWISS-PROT
  and SPTREMBL. When the clusters have converged, a 'consensus description'
  for the clusters is automatically generated from the SWISS-PROT/SPTREMBL
  description lines of all the SWISS-PROT/SPTREMBL members of each
  cluster. The annotation score is the percentage of SWISS-PROT/SPTREMBL
  family members that have this annotation, or part of it, in
  SWISS-PROT/SPTREMBL. If the description covers less than 40% of the
  SWISS-PROT/SPTREMBL members, the family description is assigned 'UNKNOWN'.

  NOTE: the family identifiers, members and descriptions used in the
  current release (1.10) are different from those used in previous
  releases. They will be stable as of this release.


------------------------------------------------------------------------
####
# admin description:
####

# this describes the way to go from ensembl.pep to the family database.
# 
# Still incomplete; not yet tested myself, so possibly wrong/inaccurate in
# places.  The blasting/clustering code is not (yet?) part of EnsEMBL; it
# currently lives in Anton's acari home, ~ae1/{srs,bin,blast} etc.

# First thing to do is to get the code

    cd work  # or whereever you keep your working copies

    cvs -d somewhere checkout ensembl-external # or cvs update, of course

# set/adjust some environment variables (FAMPATH and PATH).
# Look at ensembl-external/family/setenv.{shl,csh}.example; copy it and change
# to needs, then source it

    source setenv.sh

# If you're doing the source inside ensembl-external/family, you won't need 
# any adjustment (because it uses the value of `pwd`). It might be better to
# hardcode this though (so you can source it from elsewhere).

# Next, compile things

    cd src

    make

# that should be it

# ------------------------------------------------------------------------

# Getting the data.
# get swiss-prot/sptrembl data from SRS:

    cd  srs

    fetch.pl swissprot sptrembl


# gets all vertebrate seqs into vertebrate.pep and seq_types (latter says
# wether a seq camefrom swissprot or trembl; needed later during annotation
# phase).

# ------------------------------------------------------------------------
# get the EnsEMBL data
#  simply get it from the ensembl-ftp server or so. You may in fact need to
#  get several ensembl sets, e.g. mouse + human. 

# ------------------------------------------------------------------------
# concatenate:

    thedate=`date`

    cat ensembl.pep vertebrate.pep > $thedate.pep

# or: cat ensembl.pep mouse.pep vertebrate.pep > $thedate.pep (etc.)

    rm ensembl.pep vertebrate.pep # to save space. But keep seq_types

# ------------------------------------------------------------------------
# set the location of a scratch area, and create a directory for it:
# (I'm using sh syntax; use  ``set foo = bar'' when using csh syntax):


    release=fam112  # or whatever

    work=/work1/$USER/$release # or whaterver

    mkdir $work

    cd $work

    mkdir $work/blast

    mkdir $work/blast/seq

    mkdir $work/blast/results

    cd $work/blast

    formatdb_2.0.11 -p T  -i $work/srs/$release.pep


# results in $work/srs/$release.{pep,phr,pin,pesq}: the blast db to be run
# against

# ------------------------------------------------------------------------
# distribute the databases to all the cluster nodes:

    cd /data/sync

    mkdir  $release

    cd $release

    cp $work/blast/$release.* ./

    sync-data

# this will copy everything locally to the farm machines; takes an hour
# or so. 

# ------------------------------------------------------------------------
# split the data to be blasted into chunks:

    cd $work/blast

chopper $work/srs/$release.pep -n 400  # see also -h
# splits data into $work/blast/seq/

# ------------------------------------------------------------------------
# submit to blast: 

    cd $work/blast

    run-blasts.pl  /data/sync/$release.pep  # see also -h !

# This will show what would be done, but not do it
# If all seems well, do:

    run-blasts.pl  -g /data/sync/$release.pep

# ------------------------------------------------------------------------
# when blast finished (10 hours or so)

# check results:

# you can already check results during the running:

    run-blasts.pl -C /data/sync/$release.pep

# and already re-submit the failed ones (still during running):

    run-blasts.pl -C -g  /data/sync/$release.pep

# If you're sure that missing results are not due to jobs that haven't
# completed (e.g., if no lsf jobs are outstanding), then resubmit all those
# whose counts don't add up:

    run-blasts.pl -C -f -g /data/sync/$release.pep

# For e.g. testing, you can restrict the jobs to be checked/submitted using
# the -n option

# ------------------------------------------------------------------------

# concatenate results and compress things

    concat-blasts.sh results/*.out.gz > $release.parsed

# this will tell you how many peptides it found (on stderr), and this should
# match the number of peptides in $release.pep

# ------------------------------------------------------------------------
# make and go to new directory for preparing the clustering.

  mkdir ../clustering
  cd    ../clustering
  ln -s ../blast/$release.parsed ./  # for convenience

# convert blast hits to a markov matrix:

  nice markov $release.parsed -ind $release.index -out $release.mci

# (you may need to play with the -chunk option; see markov -h)

# This may take 30-300 min, and 1.5 Gbyte of memory, so run on a big machine,
# like ecs1h.
# 
# ------------------------------------------------------------------------
# 
# run clustering algorithm on the matrix:

  nice mcl $release.mci -I 3.0 -t 8 -P 1000 -progress 100 -o $release.mcl

# takes an hour or so (longer if -P is higher) 15 - 1500 min or so, depending
# on the P value (and of course set size).
#  -P basically determines the precision; the default value (1000) is
#  probably good enough. If you're not sure about this, run 
#
#     clmdist $one.mcl $other.mcl # in ensembl-external-family/src/mcl/bin
# 
# on clustering files from mcl run with different P's (e.g., 1000 and 4000);
# its output is two numbers that when added and divided by the number of
# peptides, give a measure of the difference between the clusterings. If this
# difference is smaller than say 1%, you might as well stick with the faster,
# lower P-number
# 
# Run it on ecs1h, which has enough memory (16Gbyte); the higher P, the more
# it will need (1.1 Gbyte for P=8000)

# -I is the inflation parameter: the higher, the more (and smaller) families
# will be.

## Further optimization stragegies, in case it's too slow:
#
#    -P 1000 -M 1000 -pct 90
#    -P 2000 -M 1500 -pct 90
#    -P 2000 -M 1500 --adapt -pct 90
#    -P 3000 -M 2000 --adapt -pct 90

# ------------------------------------------------------------------------
# put the numbers and strings back together again:

    parse_mcl.pl $release.mcl $release.index > $release.clusters

# This is a file of clusters-number, protein member (sorted by number of
# members, so the biggest clusters is at the end; first ~ third of the file is
# singletons)

# ------------------------------------------------------------------------

#  add the sp/trembl annotation lines (to each sp/trembl seq):

    add-annotations.pl $work/srs/$release.pep $work/srs/seq_types \
       $release.clusters > $release.annotated

# ------------------------------------------------------------------------

# find consensus descriptions:

    consensifier.pl -d SWISSPROT  $release.annotated > $release.SWISSPROT-consensus
    consensifier.pl -d SPTREMBL   $release.annotated > $release.SPTREMBL-consensus

# By now, there are many non-hits that have dropped out (sequences that
# did't even match themselves during the blast hits, due to low-complexity
# filtering. These are typically short repetive sequence). 

# ------------------------------------------------------------------------

# finally, put everything together using

    assemble-consensus.pl $release.annotated \
       $release.SWISSPROT-consensus $release.SPTREMBL-consensus  \
       >  $release.families 2> $release.discarded

# This not only puts things together, but also drastically cleans the
# annotations up. Typically well over half of the annotations are useless,
# and are therefore assigned 'UNKNOWN'.
# 
# Have a look (grep -v UNK) through the result to see if there's not too much
# junk, as well as through $release.discarded (grep -v UNK) to see if not too
# much valuable has been thrown away. The end of stderr has some statistics
# 
# If needed, have a look at assemble-consensus.pl; towards the top, there 
# are three tunable lists of regexps that may need adjusting  (but prolly
# not).

# ------------------------------------------------------------------------
# now import stuff into an ensembl-family database:

# If the ENSP's are not yet final (e.g., if they look like COBP or PGBP or
# so), then remap the ID's, using

    pep-id-remap.pl mapping.dat  < final.family > families.out

# load the families into the database:

    tablesql=$work/ensembl-external/sql/family.sql

    human='database=homo_sapiens_core_110;host=ensrv3;user=ensro'
    mouse='database=mouse_gb_aug01;host=ecs1d;user=ensro'
    ensdbs="$human,$mouse"

    famdbconnect='database=family110;host=ensrv3;user=***REMOVED***;pass=secret'

    perl $scripts/family-input.pl -r 3 -C $tablesql \

    -E $ensdbs -F $famdbconnect final.families  > famload.out 2> famload.err

# This may fail if the database or tables already exist, or if you have
# access problems. If so, just leave out -C $tablesql (which creates the
# database and tables). Creating the new family database + tables (when not
# available) may be easier with a mysqladmin -u ***REMOVED*** -pSECRET create
# DATABASENAME; mysql -u ***REMOVED*** -pSECRET DATABASENAME < $tablesql

# Very occasionally, you get one protein ending up in two families. This is
# rare, and harmless. If this happens, this is notified by family-input.pl,
# and the first one is taken, the rest ignored. 
# 
# The family-input.pl script tries to be careful with going from peptides
# (which the clusters contain) to genes (which are 'added' to the
# clusters). Occasionally, two peptides/transcripts from one gene end up in
# two different clusters. In that case, the family (and therefore
# description) having the best description is given  precedence. This is all
# written about to stderr, which is analyzed in the next step.
# 
# ------------------------------------------------------------------------

# Lastly, on the logfile from the loading (famload.err), run the statistics
# script:

     $scripts/fam-stats.sh $host $dbname $dbuser famload.err  > stat.out 2> stat.err

# The output gives a rough summary of the way things have  been going (in
# particular, the gene->family assignents. They should be roughly equal
# numbers (although I have seen +- 50% ... not sure what to make of that, it
# looked OK).
# 
# If the gene-descriptions (see gene-descriptions.txt) have not been done
# yet, do them, then run the script 

    ensembl/scripts/desc-from-fam.sh coreDB MUSG -h host -u user famDB

# and replace the existing gene-description table with the new one (which is
# created in famDB.

# ------------------------------------------------------------------------

# Alignments:

  mkdir $work/alignments
  cd $work/alignments

MORE HERE

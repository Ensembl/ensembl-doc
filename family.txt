 -*- mode: text -*-;

(For maintainer description, see below)

User description (should this go elsewhere?):

  The family database is generated by running Enright and van Dongen's Tribe
  sequence clustering algorithm on a set of peptides consisting of the
  EnsEMBL predictions (currently for mouse and human; in the future possibly
  more), together with all vertebrate sequences from SWISS-PROT and SPTREMBL.
  On this set of peptides, an all-against-all BLASTP is run to to establish
  similarities. Using these similarities, clusters can be established. This
  clustering is done using the MCL algorithm.

  For each cluster thus obtained, a 'consensus description' is automatically
  generated from the SWISS-PROT/SPTREMBL description lines of all the
  SWISS-PROT/SPTREMBL members of each cluster, using a longest common
  substring approach.

  The annotation score is the percentage of SWISS-PROT/SPTREMBL family
  members that have this annotation, or part of it, in
  SWISS-PROT/SPTREMBL. If the description covers less than 40% of the
  SWISS-PROT/SPTREMBL members, the family description is assigned
  'AMBIGUOUS'.

  For predicted genes that don't have a close match with an
  SWISS-PROT/SPTREMBL sequence, the gene's description line (see geneview) is
  actually taken from the family. This results in some kind of description
  for around 30% of the 'novel' genes. 

------------------------------------------------------------------------
#####################
# admin description #
#####################

# this describes how to create family database from an ensembl.pep file.

# First thing to do is to get the code

    cd work  # or whereever you keep your working copies

    cvs -d somewhere checkout ensembl-external # or cvs update, of course

# set/adjust some environment variables (FAMPATH and PATH).
# Look at ensembl-external/family/setenv.{sh,csh}.example; copy it and change
# to needs, then source it

  source setenv.sh
# or
  source setenv.csh

# (If you're doing the source inside ensembl-external/family, you won't need 
# any adjustment (because it uses the value of `pwd`). It might be better to
# hardcode this though (so you can source it from elsewhere)).

# Next, compile markov (which generates from blast results the matrix input for mcl)

  cd src/markov
  make
  make install
  make clean

# mcl programs package is no longer in the ensembl-external cvs repository. 
# If not already install in your machine, or if you want to install a
# newer version, get the mcl source code from http://micans.org/mcl/src/
#
# The actual version we use on our Alpha machines is 02-63, so you should get
# http://micans.org/mcl/src/mcl-02-063.tar.gz 
# It compiles well with the native DEC Alpha cc, just specifie 

  cd /usr/local/ensembl/src
  gunzip -c mcl-02-063.tar.gz | tar xvf -
  cd mcl-02-063
  CC=cc ./configure --prefix=/usr/local
  make

# In case of installing a new version test it before doing it the
# default one. If it works correctly,

  cd /usr/local/ensembl/src
  rm -f mcl
  ln -s mcl-02-063 mcl

# then copy all executables to the bin path (/usr/local/bin)
# and documentation in the man/share path (/usr/local/man and /usr/local/share)
# main path defined by --prefix option in configure step
# --prefix should be the path where bin/, man/ and share/ directories exists
#
# Next install the executables. Root access rights could be needed for
# this step.

  make install
  make clean

# For any futher information about mcl package go to the official site.
# Here is the URL http://micans.org/mcl/
#
# ------------------------------------------------------------------------

# Getting the data.
# get swiss-prot/sptrembl data from SRS:

    cd  srs

    srs-fetch.pl swissprot sptrembl

# gets all vertebrate seqs into vertebrate.pep and seq_types (latter says
# wether a seq camefrom swissprot or trembl; needed later during annotation
# phase).

# If you want to SRS from within Sanger, use plato:
# 
# Log into plato.  The nfs/infodata is not mounted on acari.  This is the
# second time I've been asked about it.  I'll get a working getz on acari,
# it will fit into next week's todo list.

# Alternatively, get the srs file from someone else (like Paul Kersey
# <pkersey@ebi.ac.uk> :-)

# ------------------------------------------------------------------------
# get the EnsEMBL data
#  simply get it from the ensembl-ftp server or so. You may in fact need to
#  get several ensembl sets, e.g. mouse + human. 

# It may well be that the ensembl peptides still have their preliminary,
# internal_id's (which are just numbers). The peptide id's will be mapped to
# stable_id's closer to the release. The family script still assume the old
# system, in which the preliminary id's looked like 'COBP0000' etc. 
# In order not to have to change things too much, I therefore faked this old
# style, but prepending 'COBP' to each ensembl internal_id. In the future,
# something cleverer will be needed, and some hard-coding (particularly in
# family-input.pl, see later) will have to be removed. Singapore is already
# working on this. 
# 
# This thing will become a bit more complicated if there are several
# new releases of different species involved, as the internal_id's of
# different species may conflict ! Currently, this should not be a problem,
# since at  most one of the peptide sets contains internal_id's; the rest
# should already have had  stable_id's assigned.

# ------------------------------------------------------------------------
# concatenate:

    thedate=`date`

    cat ensembl.pep vertebrate.pep > $thedate.pep

# or: cat ensembl.pep mouse.pep vertebrate.pep > $thedate.pep (etc.)

#    rm ensembl.pep vertebrate.pep # to save space. But keep seq_types

# ------------------------------------------------------------------------
# set the location of a scratch area, and create a directory for it:
# (I'm using sh syntax; use  ``set foo = bar'' when using csh syntax):


    release=fam112  # or whatever

    work=/work1/$USER/$release # or whaterver

    mkdir $work

    cd $work

    mkdir $work/blast

    mkdir $work/blast/seq

    mkdir $work/blast/results

    cd $work/blast

    formatdb_2.0.11 -p T  -i $work/srs/$thedate.pep


# results in $work/srs/$release.{pep,phr,pin,pesq}: the blast db to be run
# against

# ------------------------------------------------------------------------
# distribute the databases to all the cluster nodes:

    cd /data/sync

    mkdir  $release

    cd $release

    cp $work/srs/$thedate.* ./

    sync-data

# this will copy everything locally to the farm machines; takes an hour
# or so. 

# ------------------------------------------------------------------------
# split the data to be blasted into chunks:

    cd $work/blast

    chopper  -n 400  $work/srs/$thedate.pep  # see also -h

# splits data into $work/blast/seq/

# ------------------------------------------------------------------------
# submit to blast: 

    cd $work/blast

    run-blasts.pl  /data/sync/$release/$thedate.pep  # see also -h !

# This will show what would be done, but not do it
# If all seems well, do:

    run-blasts.pl  -g /data/sync/$release/$thedate.pep

# ------------------------------------------------------------------------
# when blast finished (1 - 12 hours, depending on how loaded farm is )

# check results:

# you can already check results during the running:

    run-blasts.pl -C /data/sync/$release/$thedate.pep

# and already re-submit the failed ones (still during running):

    run-blasts.pl -C -g  /data/sync/$release/$thedate.pep

# The *.joberr frequently contains stuff like
#   [blastall_2.0.11] WARNING:  [000.000]  SetUpBlastSearch failed.
#   [blastall_2.0.11] ERROR:  [000.000]  BLASTSetUpSearch: Unable to calculate Karlin-Altschul params, check query sequence
# This is due to very short sequences. These messages can be ignored. 

# If you're sure that missing results are not due to jobs that haven't
# completed (e.g., if no lsf jobs are outstanding), then resubmit all those
# whose counts don't add up:

    run-blasts.pl -C -f -g /data/sync/$release/$thedate.pep

# For e.g. testing, you can restrict the jobs to be checked/submitted using
# the -n option

# ------------------------------------------------------------------------

# concatenate results and compress things

    concat-blasts.sh results/*.out.gz > $release.parsed

# this will tell you how many peptides it found (on stderr), and this should
# match the number of peptides in $release.pep

# ------------------------------------------------------------------------
# make and go to new directory for preparing the clustering.

  mkdir ../clustering
  cd    ../clustering
  ln -s ../blast/$release.parsed ./  # for convenience

# convert blast hits to a markov matrix:

  nice markov $release.parsed -ind $release.index -out $release.mci

# (you may need to play with the -chunk option; see markov -h)

# This may take 10-60 min, and 1.5 Gbyte of memory, so run on a big machine,
# like ecs1h.
# 
# ------------------------------------------------------------------------
# 
# run clustering algorithm on the matrix:

  nice mcl $release.mci -I 3.0 -t 8 -P 1000 -progress 100 -o $release.mcl

# takes an hour or so (longer if -P is higher) 15 - 1500 min or so, depending
# on the P value (and of course set size).
#  -P basically determines the precision; the default value (1000) is
#  probably good enough. If you're not sure about this, run 
#
#     clmdist $one.mcl $other.mcl # in ensembl-external-family/src/mcl/bin
# 
# on clustering files from mcl run with different P's (e.g., 1000 and 4000);
# its output is two numbers that when added and divided by the number of
# peptides, give a measure of the difference between the clusterings. If this
# difference is smaller than say 1%, you might as well stick with the faster,
# lower P-number
# 
# Run it on ecs1h, which has enough memory (16Gbyte); the higher P, the more
# it will need (1.1 Gbyte for P=8000)

# -I is the inflation parameter: the higher, the more (and smaller) families
# will be. See note below, too.

## Further optimization stragegies, in case it's too slow:
#
#    -P 1000 -M 1000 -pct 90
#    -P 2000 -M 1500 -pct 90
#    -P 2000 -M 1500 --adapt -pct 90
#    -P 3000 -M 2000 --adapt -pct 90

# Note added after doing the (test) ID mapping from 110 to 120 : it turns out
# that there is some 'chaining' due to the addition, as of 120, of the mouse
# peptides. That is, the new families are much bigger, because some mouse
# peptides provide 'bridges of similarity' between families that were
# formerly separate. So the families are not bigger in over-all terms (as
# would be expected when clustering more peptides), but also when counted per
# contributing database. Some of this chaining is to be expected. Because of
# this, it is worth doing the clustering with higher inflation levels. The
# optimum is determined by the total %-tage of additional gene-descriptions
# that can be obtained this way (because that's one of the main reasons for
# doing the clustering in the first place ...)

# ------------------------------------------------------------------------
# put the numbers and strings back together again:

    parse_mcl.pl $release.mcl $release.index > $release.clusters

# This is a file of clusters-number, protein member (sorted by number of
# members, so the biggest clusters is at the end; first ~ third of the file is
# singletons)

# ------------------------------------------------------------------------

#  add the sp/trembl annotation lines (to each sp/trembl seq):

    add-annotations.pl $work/srs/$thedate.pep $work/srs/seq_types \
       $release.clusters > $release.annotated

# ------------------------------------------------------------------------

# find consensus descriptions:

    consensifier.pl -d SWISSPROT  $release.annotated > $release.SWISSPROT-consensus
    consensifier.pl -d SPTREMBL   $release.annotated > $release.SPTREMBL-consensus

# By now, there are many non-hits that have dropped out (sequences that
# did't even match themselves during the blast hits, due to low-complexity
# filtering. These are typically short repetive sequencs).

# Note: this needs stuff from Algorithm::Diff, which will be found under
# $FAMPATH/modules; see�e setenv.{c}sh  bit above. 

# ------------------------------------------------------------------------
# new directory for the families:

mkdir ../families

# and put everything together using

    assemble-consensus.pl $release.annotated \
       $release.SWISSPROT-consensus $release.SPTREMBL-consensus  \
       >  ../families/$release.families 2> $release.discarded

# This not only puts things together, but also drastically cleans the
# annotations up. Typically well over half of the annotations are useless,
# and are therefore assigned 'UNKNOWN'. The reason not to do this straight
# away in consensifier.pl is that the latter is fairly slow, so you can't
# really easily tweak scores and things to make the annotations ready for
# consumption. 
#
# 
# Have a look (grep -v UNK) through the result to see if there's not too much
# junk, as well as through $release.discarded (grep -v UNK) to see if not too
# much valuable has been thrown away. The end of stderr has some statistics
# 
# If needed, have a look at assemble-consensus.pl; towards the top, there 
# are three tunable lists of regexps that may need adjusting  (but prolly
# not).

# ------------------------------------------------------------------------
# now import stuff into an ensembl-family database:

# If the ENSP's are not yet final (e.g., if they look like COBP or PGBP or
# so), then remap the ID's, using

    universal-id-mapper.pl translations.map  < $release.families > $release.fam

# (universal-id-mapper.pl is in ensembl/scripts/). The translations.map file
# contains internal_id -> stable_id mappings for peptides, and is done by
# Arne. If you used faked COBP-identifiers, then clear the translations.map
# will have to be converted first to include the COBP-prefixes. 

# If several new releases per species will have to be mapped, you may have to
# use several steps of id mappings, by giving not one but several
# translations.map files as arguments to universal-id-mapper.pl
#

# load the families into the database:

    tablesql=$work/ensembl-external/sql/family.sql

    human='database=homo_sapiens_core_110;host=ensrv3;user=ensro'
    mouse='database=mouse_gb_aug01;host=ecs1d;user=ensro'
    ensdbs="$human,$mouse"

    famdbconnect='database=family110;host=ensrv3;user=***REMOVED***;pass=secret'

    perl $scripts/family-input.pl -r 3 -C $tablesql \

    -E $ensdbs -F $famdbconnect $release.fam  > famload.out 2> famload.log

# This may fail if the database or tables already exist, or if you have
# access problems. If so, just leave out -C $tablesql (which creates the
# database and tables). Creating the new family database + tables (when not
# available) may be easier by hand, with 

  mysqladmin -u ***REMOVED*** -pSECRET create DATABASENAME; 
  mysql -u ***REMOVED*** -pSECRET DATABASENAME < $tablesql

#
# Very occasionally, you get one protein ending up in two families. This is
# rare, and harmless. If this happens, this is notified by family-input.pl,
# and the first one is taken, the rest ignored. 
# 
# The family-input.pl script tries to be careful with going from peptides
# (which the clusters contain) to genes (which are 'added' to the clusters by
# family-input.pl). Occasionally, two peptides/transcripts from one gene end
# up in two different clusters. In that case, the family (and therefore
# description) having the best description is given precedence. This is all
# written about to stderr, which is analyzed in the next step.
# 
#
# NOTE: Currently, the families contain SWISS-PROT's, SPTREMBL's, ENSG's,
#       ENSP's, and ENSMUSP's. In the future, the ENSMUSP's will have to be
#       mapped back to ENSMUSG's. sub ens_find in
#       ensembl-external/family/scripts/family-input.pl does this for
#       ENSP->ENSG, so this should not be too difficult.

# ------------------------------------------------------------------------

# Then, on the logfile from the loading (famload.log), run the statistics
# script:

     $scripts/fam-stats.sh famload.log -h host -u user databasename  > stat.out 2> stat.err

# The output gives a rough summary of the way things have  been going (in
# particular, the gene->family assignents. They should be roughly equal
# numbers (although I have seen +- 50% ... not sure what to make of that, it
# looked OK).
# 
# As far as the histogram is concerned, typical output is shown here:
#
# 0-0: 6558
# 1-1: 9171
# 2-10: 3800
# 11-100: 248
# 101-1000: 5
# 1001-10000: NULL
# Largest family has  784 ensembl members. Row is:
# internal_id     id      description     release annotation_confidence_score     num_ens_pepts
# 1       ENSF00000000001 ZINC FINGER PROTEIN     3       91      784
# 
# 
# Families with 0 members are OK: we're only looking at the ENSPEPT members
# in this figure. 
#
# If the gene-descriptions have not been done yet, do them first, then run
# the script

    ensembl/scripts/desc-from-fam.sh coreDB DBNAME -h host -u user famDB

# and replace the existing gene-description table with the new one (which is
# created in famDB.  See also ensembl-doc/gene-descriptions.txt

# ------------------------------------------------------------------------

# OK, the only thing left to do is ID mapping. This is done as follows:
# (see also NOTEs below)

  $scripts/family-id-map.pl \
     -old 'user=ensro;host=ensrv1;dbname=homo_sapiens_family_110' \
     -new 'user=ensro;host=ensrv1;dbname=homo_sapiens_family_120' \
   > family.map 2>log.map

# This will take long, something like 6 hours last time I did it. Do a 
#   `tail -f log.map' to see the progress.
# 
# The output is in TAB-delimited format.  Currently the mappings are from ENSF
# to ENSF; it may be better to use internal_id's at some point, but that is
# trivial to change. 
# 
# For the 'after the fact' mapping from homo_sapiens_family_110 to
# homo_sapiens_family_120 (which has been caculated but never applied), the run
# took 5 hours, and resulted in loss of 15% old families, and new id's for 41%
# new families. These results are not typical, because from 110 -> 120, mouse
# peptides were added. The mouse singletons resulting from those could of
# course not possibly have mapped. I therefore mapped 102 to 110 (which is
# human ENS only), and found
# 
#   No mapping for old ids: (4337/15786=27 %)
#   No mapping for new ids: (2362/13811=17 %)
#
# This time, a fair number of old families (prolly singletons, I didn't look)
# got lost. I think the upcoming mapping (120 ->130), both figures below 20%
# can be expected. 
#
# Note on the algorithm: nothing fancy really, the two lists of families are
# sorted by size, then starting with the largest of the old families, the
# best matching family is greedily picked from the new list, and so on down
# till we have run out of matches. (The sorting is only to speed things up,
# as the likelihood to find a good match between the big families is higher,
# so you'll more quickly reduce the search space).  The 'best matching'
# family is the one that has the largest MAX(overlap(SPTR), overlap(ENSPEP)),
# where overlap(X) is the percentage of matching cross-references to database
# X in the old and new family databases. The percentage used is
# 100*accessions-in-both-old-and-new/those-in-either. If this max overlap
# percentage is > $minperc (default: 10), the match is kept; otherwise, new
# id's will be used.
# 
# You may want to fiddle with the -minperc option; the default is 10.  (And for
# a twilight zone of $minperc + 10, the match is kept, but a warning is output;
# since the old and new family descriptions are printed to stderr, you can look
# through it to see if you trust it. Search for '%' in stderr, to see
# instances that may be dodgy, and raise $minperc if necessary. The large
# majority of mappings appears the be perfect, with identical description
# lines in old and new release.
# 
# NOTE: (1) To run the mapping, the gene/peptide id's that are read from the old
#           and new databases must already have been mapped !
# 
#       (2) The family-id-map.pl script only looks at SWISS-PROT's, SPTREMBL's
#           and ENSP's, because homo_sapiens_family_110 didn't have
#           ENSMUSp's. The next release should have both, in which case they
#           can be taken into consideration (prolly won't matter much; it might
#           make the mapping take longer, no idea). 
# 
#
# OK, after you have got the mapping, you have to apply it. One way of doing
# it is to map the family id's in final.families, and then run
# family-input.pl on the result. This, however, would be very wasteful, since 
# family-input.pl then completely redoes the ens-peptide -> ens-gene mapping,
# which doesn't need redoing as it is the same. 
#
# So instead, create an SQL update script from the family.map file, e.g as
# follows (or write a little script if you're not into one-liners):

  perl -nae 'print "UPDATE family SET id = \"$F[1]\" where id = \"$F[0]\"\n";' \
     < family.map > family-map.sql

# and then apply it to the database as
 
  mysql -h host -u ***REMOVED*** -pSECRET the-database-name \ 
       < family-map.sql >& family-map.log 
#
# 
# ------------------------------------------------------------------------
#
# Alignments: not fully documented yet. Family.pm has untested code for
# alignments, and there is no web code for it yet. 
# 
# To obtain the alignments, run 
# run-clustals.pl; to input them, run alignment-input.pl.
# 
# 
# ------------------------------------------------------------------------
# AltaVista indexing:
# 
# run fam-fulltext.sh
# 

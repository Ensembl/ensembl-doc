 -*- mode: text -*-;

(For maintainer description, see below)

User description (should this go elsewhere?):

  The family database is generated by running Enright and van Dongen's Tribe
  sequence clustering algorithm on a set of peptides consisting of the
  EnsEMBL predictions, together with all vertebrate sequences from SWISS-PROT
  and SPTREMBL. When the clusters have converged, a 'consensus description'
  for the clusters is automatically generated from the SWISS-PROT/SPTREMBL
  description lines of all the SWISS-PROT/SPTREMBL members of each
  cluster. The annotation score is the percentage of SWISS-PROT/SPTREMBL
  family members that have this annotation, or part of it, in
  SWISS-PROT/SPTREMBL. If the description covers less than 40% of the
  SWISS-PROT/SPTREMBL members, the family description is assigned 'UNKNOWN'.

  NOTE: the family identifiers, members and descriptions used in the
  current release (1.10) are different from those used in previous
  releases. They will be stable as of this release.


------------------------------------------------------------------------
####
# admin description:
####

# this describes how to create family database from an ensembl.pep file.
# 

# First thing to do is to get the code

    cd work  # or whereever you keep your working copies

    cvs -d somewhere checkout ensembl-external # or cvs update, of course

# set/adjust some environment variables (FAMPATH and PATH).
# Look at ensembl-external/family/setenv.{sh,csh}.example; copy it and change
# to needs, then source it

    source setenv.sh

# (If you're doing the source inside ensembl-external/family, you won't need 
# any adjustment (because it uses the value of `pwd`). It might be better to
# hardcode this though (so you can source it from elsewhere)).

# Next, compile things

    cd src

    make

# that should be it

# ------------------------------------------------------------------------

# Getting the data.
# get swiss-prot/sptrembl data from SRS:

    cd  srs

    srs-fetch.pl swissprot sptrembl

# gets all vertebrate seqs into vertebrate.pep and seq_types (latter says
# wether a seq camefrom swissprot or trembl; needed later during annotation
# phase).

# If you want to SRS from within Sanger, use plato:
# 
# Log into plato.  The nfs/infodata is not mounted on acari.  This is the
# second time I've been asked about it.  I'll get a working getz on acari,
# it will fit into next week's todo list.


# ------------------------------------------------------------------------
# get the EnsEMBL data
#  simply get it from the ensembl-ftp server or so. You may in fact need to
#  get several ensembl sets, e.g. mouse + human. 

# ------------------------------------------------------------------------
# concatenate:

    thedate=`date`

    cat ensembl.pep vertebrate.pep > $thedate.pep

# or: cat ensembl.pep mouse.pep vertebrate.pep > $thedate.pep (etc.)

#    rm ensembl.pep vertebrate.pep # to save space. But keep seq_types

# ------------------------------------------------------------------------
# set the location of a scratch area, and create a directory for it:
# (I'm using sh syntax; use  ``set foo = bar'' when using csh syntax):


    release=fam112  # or whatever

    work=/work1/$USER/$release # or whaterver

    mkdir $work

    cd $work

    mkdir $work/blast

    mkdir $work/blast/seq

    mkdir $work/blast/results

    cd $work/blast

    formatdb_2.0.11 -p T  -i $work/srs/$thedate.pep


# results in $work/srs/$release.{pep,phr,pin,pesq}: the blast db to be run
# against

# ------------------------------------------------------------------------
# distribute the databases to all the cluster nodes:

    cd /data/sync

    mkdir  $release

    cd $release

    cp $work/srs/$thedate.* ./

    sync-data

# this will copy everything locally to the farm machines; takes an hour
# or so. 

# ------------------------------------------------------------------------
# split the data to be blasted into chunks:

    cd $work/blast

chopper  -n 400  $work/srs/$thedate.pep  # see also -h
# splits data into $work/blast/seq/

# ------------------------------------------------------------------------
# submit to blast: 

    cd $work/blast

    run-blasts.pl  /data/sync/$release/$thedate.pep  # see also -h !

# This will show what would be done, but not do it
# If all seems well, do:

    run-blasts.pl  -g /data/sync/$release/$thedate.pep

# ------------------------------------------------------------------------
# when blast finished (1 - 12 hours, depending on how loaded farm is )

# check results:

# you can already check results during the running:

    run-blasts.pl -C /data/sync/$release/$thedate.pep

# and already re-submit the failed ones (still during running):

    run-blasts.pl -C -g  /data/sync/$release/$thedate.pep

# The *.joberr frequently contains stuff like
#   [blastall_2.0.11] WARNING:  [000.000]  SetUpBlastSearch failed.
#   [blastall_2.0.11] ERROR:  [000.000]  BLASTSetUpSearch: Unable to calculate Karlin-Altschul params, check query sequence
# This is due to very short sequences. These messages can be ignored. 

# If you're sure that missing results are not due to jobs that haven't
# completed (e.g., if no lsf jobs are outstanding), then resubmit all those
# whose counts don't add up:

    run-blasts.pl -C -f -g /data/sync/$release/$thedate.pep

# For e.g. testing, you can restrict the jobs to be checked/submitted using
# the -n option

# ------------------------------------------------------------------------

# concatenate results and compress things

    concat-blasts.sh results/*.out.gz > $release.parsed

# this will tell you how many peptides it found (on stderr), and this should
# match the number of peptides in $release.pep

# ------------------------------------------------------------------------
# make and go to new directory for preparing the clustering.

  mkdir ../clustering
  cd    ../clustering
  ln -s ../blast/$release.parsed ./  # for convenience

# convert blast hits to a markov matrix:

  nice markov $release.parsed -ind $release.index -out $release.mci

# (you may need to play with the -chunk option; see markov -h)

# This may take 10-60 min, and 1.5 Gbyte of memory, so run on a big machine,
# like ecs1h.
# 
# ------------------------------------------------------------------------
# 
# run clustering algorithm on the matrix:

  nice mcl $release.mci -I 3.0 -t 8 -P 1000 -progress 100 -o $release.mcl

# takes an hour or so (longer if -P is higher) 15 - 1500 min or so, depending
# on the P value (and of course set size).
#  -P basically determines the precision; the default value (1000) is
#  probably good enough. If you're not sure about this, run 
#
#     clmdist $one.mcl $other.mcl # in ensembl-external-family/src/mcl/bin
# 
# on clustering files from mcl run with different P's (e.g., 1000 and 4000);
# its output is two numbers that when added and divided by the number of
# peptides, give a measure of the difference between the clusterings. If this
# difference is smaller than say 1%, you might as well stick with the faster,
# lower P-number
# 
# Run it on ecs1h, which has enough memory (16Gbyte); the higher P, the more
# it will need (1.1 Gbyte for P=8000)

# -I is the inflation parameter: the higher, the more (and smaller) families
# will be.

## Further optimization stragegies, in case it's too slow:
#
#    -P 1000 -M 1000 -pct 90
#    -P 2000 -M 1500 -pct 90
#    -P 2000 -M 1500 --adapt -pct 90
#    -P 3000 -M 2000 --adapt -pct 90

# ------------------------------------------------------------------------
# put the numbers and strings back together again:

    parse_mcl.pl $release.mcl $release.index > $release.clusters

# This is a file of clusters-number, protein member (sorted by number of
# members, so the biggest clusters is at the end; first ~ third of the file is
# singletons)

# ------------------------------------------------------------------------

#  add the sp/trembl annotation lines (to each sp/trembl seq):

    add-annotations.pl $work/srs/$thedate.pep $work/srs/seq_types \
       $release.clusters > $release.annotated

# ------------------------------------------------------------------------

# find consensus descriptions:

    consensifier.pl -d SWISSPROT  $release.annotated > $release.SWISSPROT-consensus
    consensifier.pl -d SPTREMBL   $release.annotated > $release.SPTREMBL-consensus

# By now, there are many non-hits that have dropped out (sequences that
# did't even match themselves during the blast hits, due to low-complexity
# filtering. These are typically short repetive sequencs).

# ------------------------------------------------------------------------
# new directory for the families:

mkdir ../families

# and put everything together using

    assemble-consensus.pl $release.annotated \
       $release.SWISSPROT-consensus $release.SPTREMBL-consensus  \
       >  ../families/$release.families 2> $release.discarded

# This not only puts things together, but also drastically cleans the
# annotations up. Typically well over half of the annotations are useless,
# and are therefore assigned 'UNKNOWN'.
# 
# Have a look (grep -v UNK) through the result to see if there's not too much
# junk, as well as through $release.discarded (grep -v UNK) to see if not too
# much valuable has been thrown away. The end of stderr has some statistics
# 
# If needed, have a look at assemble-consensus.pl; towards the top, there 
# are three tunable lists of regexps that may need adjusting  (but prolly
# not).

# ------------------------------------------------------------------------
# now import stuff into an ensembl-family database:

# If the ENSP's are not yet final (e.g., if they look like COBP or PGBP or
# so), then remap the ID's, using

    universal-id-mapper.pl mapping.dat  < final.family > families.out

# (universal-id-mapper.pl is in ensembl/scripts/)

# load the families into the database:

    tablesql=$work/ensembl-external/sql/family.sql

    human='database=homo_sapiens_core_110;host=ensrv3;user=ensro'
    mouse='database=mouse_gb_aug01;host=ecs1d;user=ensro'
    ensdbs="$human,$mouse"

    famdbconnect='database=family110;host=ensrv3;user=***REMOVED***;pass=secret'

    perl $scripts/family-input.pl -r 3 -C $tablesql \

    -E $ensdbs -F $famdbconnect final.families  > famload.out 2> famload.log

# This may fail if the database or tables already exist, or if you have
# access problems. If so, just leave out -C $tablesql (which creates the
# database and tables). Creating the new family database + tables (when not
# available) may be easier with a mysqladmin -u ***REMOVED*** -pSECRET create
# DATABASENAME; mysql -u ***REMOVED*** -pSECRET DATABASENAME < $tablesql

# Very occasionally, you get one protein ending up in two families. This is
# rare, and harmless. If this happens, this is notified by family-input.pl,
# and the first one is taken, the rest ignored. 
# 
# The family-input.pl script tries to be careful with going from peptides
# (which the clusters contain) to genes (which are 'added' to the
# clusters). Occasionally, two peptides/transcripts from one gene end up in
# two different clusters. In that case, the family (and therefore
# description) having the best description is given  precedence. This is all
# written about to stderr, which is analyzed in the next step.
# 
# ------------------------------------------------------------------------

# Lastly, on the logfile from the loading (famload.log), run the statistics
# script:

     $scripts/fam-stats.sh famload.log -h host -u user databasename  > stat.out 2> stat.err

# The output gives a rough summary of the way things have  been going (in
# particular, the gene->family assignents. They should be roughly equal
# numbers (although I have seen +- 50% ... not sure what to make of that, it
# looked OK).
# 
# If the gene-descriptions (see gene-descriptions.txt) have not been done
# yet, do them, then run the script 

    ensembl/scripts/desc-from-fam.sh coreDB MUSG -h host -u user famDB

# and replace the existing gene-description table with the new one (which is
# created in famDB.

# ------------------------------------------------------------------------

OK, the only thing left to do is ID mapping. This is done as follows:

  $scripts/family-id-map.pl \
     -old 'user=ensro;host=ensrv1;dbname=homo_sapiens_family_110' \
     -new 'user=ensro;host=ensrv1;dbname=homo_sapiens_family_120' \
   > family.map 2>log.map

This may take a while (do a tail -f on the log file). Look at the log file
to get an idea. the larger families (>= 10) should be fine. 

The output is in TAB-delimited format.  Currently the mappings are from ENSF
to ENSF; it may be better to use internal_id's at some point, but that is
trivial to change. NOTE: to run the mapping, the gene/peptide id's that are
read from the old and new databases must already have been mapped!

For the 'after the fact' mapping from homo_sapiens_family_110 to
homo_sapiens_family_120 (which has been caculated but never applied), the run
took 5 hours, and resulted in loss of 15% old families, and new id's for 41%
new families.


Note on the algorithm: nothing fancy really, the two lists of families are
sorted by size, then starting with the largest of the old families, the best
matching family is greedily picked from the new list, and so on down till we
have run out of matches. The 'best matching' family is the one that has the
largest MAX(overlap(SPTR), overlap(ENSPEP)), where overlap(X) is the
percentage of matching cross-references to database X in the old and new
family databases. The percentage used is
100*accessions-in-both-old-or-new/those-in-either. If this max overlap
percentage is > $minperc, the match is kept; otherwise, new id's will be
used.  

You may want to fiddle with the -minperc option; the default is 10.  (And for
a twilight zone of $minperc + 10, the match is kept, but a warning is output;
since the old and new family descriptions are printed to stderr, you can look
through it to see if you trust it.

# ------------------------------------------------------------------------

alignments: to come.

This docs should give the basics of running the affy mapping


Before you start
----------------

To run the affy mapping you need three cvs modules

ensembl
ensembl-pipeline and
ensembl-analysis

You also need several data sets of probe sequences

These can be fetched from here

http://www.affymetrix.com/support/technical/byproduct.affx?cat=arrays

you need to pick your species then download all Genome array set fasta 
files


Setting up the data and database
--------------------------------

The fasta files need to be chunked into files all containing approximately
1000 entries. There are several tools about for doing this. Most efficient
are programs like fastasplit which will try and produce either a certain 
number of chunks or files of a certain size 

this program can be found here

~searle/progs/fastasplit/fastasplit
Usage: fastasplit [-size] <fastaFile> <nchunk> <outdir>

Alternatively there is a slower script which can be found in 
ensembl-analysis called chunk_fasta_file.pl this is just given a fasta
filename an outputdir and a number of chunks and just iterates through
the file putting the number specified in each file untill it reachs the end
this means the last file may have far fewer entries in it than all the
others

ensembl-analysis/scripts/chunk_fasta_file.pl
Usage: chunk_fasta_file.pl fasta_file output_dir chunk_size

You need two analysis objects

A dummy object to link to the input_ids which should be filenames and
an analysis object to represent the mapping below is an example of the
analysis conf file you would need to point 
ensembl-pipeline/scripts/analysis_setup.pl at to create this

[mapping]
program=exonerate
program_version=0.8.3
program_file=exonerate-0.8.3
module=Exonerate2Array
input_id_type=AFFY_FILE

[SubmitAffy]
input_id_type=AFFY_FILE


note the runnabledb you should be using is

Bio::EnsEMBL::Analysis::RunnableDB::Exonerate2Array


and yoru input ids will be file names

You will also need one rule. This is a simple rule which requires no
dependances and below is the rule.conf file you would need to point
ensembl-pipeline/scripts/analysis_setup.pl

[mapping]
condition=SubmitAffy


Note this may change to needing to depend on RepeatMask and Dust 
being finished in order to run and then depend on an accumulator

Remember you will also need an entry in the BatchQueue file for this 
analysis before you run


To create the input_ids you will need a  commandline which looks like this

perl make_input_ids -dbhost myhost -dbuser myuser -dbpass mypass 
-dbport 3306 -dbname mypipeline_db -logic_name SubmitAffys -file 
-dir path_to_data/human_affys/all_seq/

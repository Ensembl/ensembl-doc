This document describes how we run our BAC mapping pipeline against any genome

The code you need to run this is

The ensembl cvs modules

ensembl
ensembl-pipeline 
ensembl-analysis

and bioperl

bioperl-live branch bioperl-release-1-2-3


Data
----

BAC clone data can be obtained for many species from ftp.embl.org

ftp ftp.ensembl.org
user anonymous
pass email

#Get sequence data

ftp> cd /pub/traces/your_species/fasta
ftp> mget jcvijtc*

# Get clone specific data

ftp> cd /pub/traces/your_species/traceinfo
ftp> mget jcvijtc*

# Append all fasta files together
cat your_species*.fa > species.fa

# Append all trace info files together
cat your_species*.xml > species.xml

Create an index for the flat fasta file (needed for the pipeline module MapCLoneEnds)
---------------------------------------
indicate /directory/containing/fasta/files species.fa species_idx singleWordParser


Remove sequence contamination and/or low quality sequence
---------------------------------------------------------
Use the information in the trace info file to filter the BAC end sequences, 
so shorter high quality sequences are will be used for the mapping.

This is done with a script:

perl ~/cvs_checkout/ensembl-personal/jb16/scripts/general/cleanCloneEnds.pl \
      -xmlfile /path/to/trace/info/file/species.xml \
      -seqFetchFB /path/to/fasta/index \
      -outfile /where/you/want/to/store/clean/fasta/sequences.fa

You should provide as args : the xml file that contains the trace info for all the clones,
the index for the fasta file and a file where you want to store your high quality sequences. 

Now you should rerun indicate to create the index for the new fasta sequences

indicate /where/you/want/to/store/clean/fasta sequences.fa HQ_species_idx singleWordParser

Add input_ids to the database
-----------------------------
The input ids are automaticaly generate by a script:

perl ~/cvs_checkout/ensembl-personal/jb16/scripts/general/chunker.pl

as args you should provide
- xmlfile The path to the xml file with the trace info (used to put together all BAC
          ends that belong to the same BAC.

- analysis_id The analysis id for BAC end mapping

- insert_input_ids The path to the file where the input_ids will be stored

- outfile Path to a file which will store BAC id information that will be used when
          running the analysis.

Once the input ids are generated these might be loaded into your database:
 
mysql -u user -p pass -h host -P port -D your_db < /path/to/input_ids.sql

The input ids are in the format $line_number:$total_lines and it refers to the outfile 
generated by chunker.pl. Each line contains a number of BAC end ids grouped by Clone so
all the BAC ends that belong to the same Clone are within the same chunk (This is important
due to the filtering method used).

Running the analysis
--------------------
First you need to configure your dbs at ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Config/ExonerateCloneEnds.pm
You should also add the path to the chunk file (output file) from chunker.pl CHUNKSLIST  => /path/to/chunk/list
and also the path to the BAC end fasta sequences "QUERYSEQS    =>/where/you/want/to/store/clean/fasta/sequences.fa"
and to the index of the BAC end fasta sequences "SEQFETCHDB  => /path/to/HQ_species_idx"


The runnable Bio::EnsEMBL::Analysis::RunnableDB::MapClonesEnds should be used to
run the BAC end mapping. 

You need 4 analysis objects,  a dummy object to associate with your BAC end Chunk
input ids (you may already have this) and 2 objects EXONERATE_CLONE_ENDS and 
REFINE_CLONE_ENDS that point to module Bio::EnsEMBL::Analysis::RunnableDB::ExonerateCloneEnds
which is used to filter the BAC end mapping and the main object Map_Clone_Ends that
runs Bio::EnsEMBL::Analysis::RunnableDB::MapClonesEnds


You will also need one rule which tells the pipeline it can 
run Map_Clone_Ends when there are BAC end input_ids pointed at your dummy analysis
in the input_id_analysis table

Run your analysis like:
perl rulemanager.pl -dbhost host -dbport port -dbname dbname -dbuser user -dbpass pass -input_id_type FILE -analysis Map_Clone_Ends &
(if it doesn't run check that the input_ids in the input_id_analysis have the same analysis_id as SubmitChunks in the analysis table)


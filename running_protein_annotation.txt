This document should describe what you need to do in order to run the
protein annotation pipeline.

if you have any questions contact ensembl-dev@ebi.ac.uk

What you need
+++++++++++++

In order to run this you need

Bioperl.0.72
core ensembl
ensembl-pipeline


You also need a core database which contains a set of transcripts which
translate.


What to do
==========

It is a good idea to have read the document using_the_ensembl_pipeline
document before this one

First you need to fill out the appropriate config files


Bio::EnsEMBL::Pipeline::Config::Protein_Annotation::General

This contains general settings for the the protein_annotation

PA_PEPTIDE_FILE the location of the peptide file
PA_CHUNKS_DIR   the location of the directory the chunks of the peptide
                file will go
PA_CHUNKS_SIZE  the size of the chunks file, we tend to use 100
                peptides in a file
PA_IPRSCAN_DIR the directory where the architecture specifc directories for the interpro code lives
               for example /acari/analysis/iprscan/bin where there two directories dec_osf and linux
               and the runnabledb code knows how to decide which one to use.  
There are also three other files which must be filled out which live in
Bio::EnsEMBL::Pipeline::Config and are described in the document
using_the_ensembl_pipeline.txt

What to run
-----------

Once you have the config filled in you need to run a few things. All
the scripts mentioned here live in ensembl-pipeline/scripts/
protein_pipeline.

First you need to dump the peptides. The script is called 
dump_translations.pl

An example command line is:

./dump_translations.pl -dbname gene_db -dbhost localhost -dbuser ensro 
-db_id 1 > peptide.fa

This would print the peptides to stdout and they would be caught in the
file and any translation which contain internal stops would be skipped
and a message printed to stderr.

the fasta header of this file should be have a specific format

the first thing after the > should be the translation internal_id

>1

this script also puts other information into the header but this isn't 
required by the protein annotation pipeline but if the translation id 
isn't the  first thing after the pipeline the translation_ids won't be 
stored properly in the database


Once this is done you need to run

chunk_protein_file.pl

this would produce files in the directory specified in the
Protein_Annotation/General config each containing the specified number
of peptides till the file which contains all the peptides has been
completely split.

This doesn't damage the file it is pointed at 


You now need to make sure you pipeline database has analyses, rules to 
represent this process and input_ids associated with the appropiate
dummy analysis in the input_id_analysis table. There are scripts for
doing all this and they are described in using_the_ensembl_pipeline.txt

For this analysis you are likely to need 3 input_id types and as such
3 dummy analyses. The three input_id_types are translation_ids, filenames
and the proteome. You can name these types what you like but we generally
find it useful to name them something meaningful. Once you have your
three dummy analyses all the protein annotation you want to run will need
rules which depend on them. Generally we have the pfam analyses depend on
the translation ids. Signal Peptide detection, transmembrane domain
detection, prints and profile domain detection and ncoils analyses are
running with the protein chunk files and scanprosite and seg are run
with the whole proteome.

These three types of input_ids can all be generated with and stored in the
input_id_analysis table with the make_input_ids script described in 
using_the_ensembl_pipeline.txt. An important note is the input_id for the
jobs which run on the whole proteome must be 'proteome' this can be
achieved by using the -single_name option in the make_input_ids script



Now all these set up steps have been carried out it is time to run the
RuleManager script.

The current rulemanager script used is RuleManager3.pl and this can be
found in Bio/EnsEMBL/Pipeline/

The commandline should look something like:

perl RuleManager3.pl -dbname elegans_95 -dbhost ecs1b -dbuser ***REMOVED*** 
-dbpass **** -shuffle 


The db options are to tell the script which database to use.

You can also use enviroment variables

my $dbhost    = $ENV{'ENS_DBHOST'};
my $dbname    = $ENV{'ENS_DBNAME'};
my $dbuser    = $ENV{'ENS_DBUSER'};
my $dbpass    = $ENV{'ENS_DBPASS'};

The other options which you can use are explaining in 
using_the_ensembl_pipeline.txt or you can pass a -help option to the
RuleManager script


Loading the Interpro data 


A link is needed between the signatures ids  (eg: Pfam, Prints, Prosite)
and the interpro accessions. Two files are needed, interpro.txt goes 
into the interpro table and  interpro_desc.txt goes into the xref table.
A simple load data infile should be enough to load the data into a core 
database

An updated version of these 2 files should be found in the 
following directory: /acari/analysis/iprscan/interpro_update/

mysql to use: 

load data infile '/acari/analysis/iprscan/interpro_update/interpro.txt' 
into table interpro;
load data infile '/acari/analysis/iprscan/interpro_update/interpro_desc.txt' into table xref;


Checklist
=========

When running the protein annotation it is worth checking these things out
before you hit go

1. Do you have all the executables, and data files for the
analysis you want to run? are they push out across you compute 
farm where appropriate

2. Have you dumped your peptide file and chunked it up into pieces

3. Have you filled out all the config files in 
Bio::EnsEMBL::Pipeline::Config::Protein_Annotation

4. Does BatchQueue.pm contain entries for all the analyses you wish to run

5. Have you filled in the analysis table

6. Have you filled in the rule tables

7. Are the appropriate dummy entries in the input_id_analysis table

8. Have you tested jobs for your different analyses

If the answers to all these questions are yes you are probably read
to set the RuleManager going

This system generally has three dummy analyses and three types of input ids

First there is SubmitTranscript which represents the individual transcripts.
Generally we only make Pfam depend on SubmitTranscript as the pfam
analysis is the only one slow enough to warrent running each protein
individually. Even so we still batch together the indivual jobs within
the batch submission system to improve CPU useage, generally the batch size
is set to about 5

Second there is SubmitTranscriptChunk. Here the input_ids would be file 
names which should be located in the directory specified in PA_CHUNKS_DIR.
These are relatively quick programs which can run in a time frame
which makes it worthwhile running them across multiple proteins
at once but they still can't run across the whole proteome.


Lastly there is SubmitProteome, not the input_id for this must be
proteome otherwise the code won't work. These are used for the programs
which can run so quickly it is worth giving them the whole proteome at once
like seg. 

Here is an example of the conf file you may need to setup the analysis for
the genebuild using the analysis_setup.pl script 
(see using_the_ensembl_pipeline.txt)

In these analysis objects there are settings which are specific to the
data you are using so you may need to change some entries before using 
the data. You should note that all this objects do need a 
input_id_type as specified by the type variable and if at anypoint the
type of a goal analysis isn't the same as the type of the conditional
analysis you need an accumulator between the two and another condition
of the same type to provide the appropriate input_ids



[SubmitTranscript]
type=TRANSLATIONID

[Pfam]
db=Pfam
db_file=/data/blastdb/Ensembl/Pfam_ls;/data/blastdb/Ensembl/Pfam_fs
program=/usr/local/ensembl/bin/hmmpfam
program_file=/usr/local/ensembl/bin/hmmpfam
module=Protein/Hmmpfam
gff_source=Pfam
gff_feature=domain
type=TRANSLATIONID

[SubmitTranscriptChunk]
type=FILENAME


[Signalp]
db=signal_peptide
program=signalp
program_file=signalp
module=Protein/Signalp
gff_source=Signalp
gff_feature=annotation
type=FILENAME

[tmhmm]
db=transmembrane
program=/path/to/run_tmhmm
program_file=/path/to/run_tmhmm 
module=Protein/Tmhmm
gff_source=Tmhmm
gff_feature=annotation
type=FILENAME

#note run_tmhmm can be found in the cvs reposistory here
# ensemb-pipeline/scripts/protein_pipeline/run_tmhmm

[Prints]
db=prints
db_file=/path/o/prints.pval
program=FingerPRINTScan
program_file=FingerPRINTScan
module=Protein/Prints
gff_source=Prints
gff_feature=domain
type=FILENAME

[ncoils]
db=coiled_coil
program=/usr/local/ensembl/bin/ncoils
program_file=/usr/local/ensembl/bin/ncoils
module=Protein/Coil
gff_source=ncoils
gff_feature=annotation
type=FILENAME


[pfscan]
db=pfscan
db_file=/path/to/prosite_prerelease.prf
program=pfscan
program_file=pfscan
module=Protein/Profile
gff_source=Profile
gff_feature=Domain
type=FILENAME


[SubmitProteome]
type=PROTEOME

[scanprosite]
db=prosite
db_file=/path/to/prosite.patterns
program=/path/to/scanregexpf.pl
program_file=/path/to/scanregexpf.pl
parameters=-confirm /path/to/confirm.patterns
module=Protein/ScanProsite
gff_source=Prosite
gff_feature=domain
type=PROTEOME

[Seg]
db=low_complexity
db_file=low_complexity
program=seg
program_file=seg
module=Protein/Seg
gff_source=Seg
gff_feature=annotation
type=PROTEOME

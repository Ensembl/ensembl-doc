Running cDNA analysis in Ensembl
-------------------------------

Val Curwen 01.02.2002

The method for running cDNA analyses in EnsEMBL is virtually identical
to the method for running EST analysis, though there are a couple of
differences in the configuration options used. The same modules are
used for both, even though their names suggest they are EST specific
:-)

Overview
--------

Our aim is to use cDNA data initially to add UTRs to genewise
predicted genes, and ultimately for gene prediction int heir own right
using Genomewise

Like the genebuild, the cDNA build is controlled by scripts and a
config file, which should hopefully make it fairly painless. The scripts
files live in ensembl-pipeline/scripts/EST, and the config file lives in 
ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/ESTConf.pm

Modules
-------

The modules that control the cDNA run are:

1. ExonerateESTs - this runs exonerate between a file of cDNAs and a
genomic sequence, and writes the results out to file.

2. FilterESTs_and_E2G - this retrieves exonerate features from an
ensembl database, filtered by score and %id, blasts against the
genomic region, filters using the FeatureFilter (so we don't stack up
100s of identical hits), makes a minigenomic, runs est_genome and
writes out a gene prediction for each EST.

3. EST_GeneBuilder - clusters the est_genome predictions, generates
potential alternative transcripts and uses Genomewise to produce gene
structures that are written to the database. (still in testing)

4. Combine_Genewises_and_E2Gs - uses est2genome predictions from
FilterESTs_and_E2G to add UTRs onto genewise predicted genes from the
main gene build - running this is described in
ensembl-doc/assembly_based_genebuild.txt

Scripts
-------

The running of these 3 modules is currently coordinated by a range of scripts:
( ultimately need something groovy for polyA/polyT clipping/masking )
prepare_ests.pl   - splits the ESTs into small files
make_bsubs.pl     - generates bsub commands for running the various stages
submit.pl         - submits jobs (basically just a  system call!)

Step by step instructions
-------------------------

A. Prerequisites
----------------
1. bioperl-0.7
ensembl-pipeline
ensembl core
ensembl-external

PERL5LIB correctly set to access each of these!

2. Two databases:

main ensembl database - we call this "refdb" - holds dna sequences,
static_golden_path, pipeline results etc. We just use this as a source
for dna sequence information. This is fairly common in ensembl as it
saves space - not every database has to have all the genomic DNA in it.

est database - called "estdb" - we store the est results in a separate
database to keep table sizes under control, rather than adding
millions of extra features into the main ensembl database feature
table. This database needs to have the clone, contig and
static_golden_path tables filled in - dump the data from these tables
in refdb, and load it up into estdb.

Also, the estdb needs to have certain entries in the analysisprocess
table. These are in ensembl-pipeline/sql/EST_analysisprocess.sql. You
need to edit this file so that the string dbEST is replaced with the
EST_SOURCE string you've defined in ESTConf.pm (see 4 below). Then
load up the file using:

mysql -uuser -ppass -hhost estdbname < path/to/ensembl-pipeline/sql/EST_analysisprocess.sql

Both databases also need to have an entry in the meta table giving the
default assembly - it should look something like this:

| meta_id | meta_key         | meta_value                 |
+---------+------------------+----------------------------+
|       2 | assembly.default | NCBI_28                    |

If not, load one up like this:

mysql -uuser -ppassword -hhost -e "insert into meta values (\N, 'assembly.default', 'NCBI_28');" dbname

Obviously, replace NCBI_28 with the name of the appropriate assembly -
use the string from the 'type' column of the static_golden_path table.

3. Some sort of sequence fetcher. We use James Cuff's makeseqindex and
getseqs as they are the fastest and best we've found, but you can use
anything you like as long as the ensembl modules know how to use
it. Look at ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/SeqFetcher/*
for inspiration.

4. fill in the fields in ESTConf.pm; it should be self explanatory but
email me (vac@sanger.ac.uk) if it isn't.  If you're not going to run
the EST_GeneBuilder, you don't need to fill in the EST_GeneBuilder
options ...

5. fastasplit or something similar to divide the cDNA file into smaller chunks.

6. dust for low complexity masking of genomic sequence

B. Preparing cDNA sequences
---------------------------

0. get the mRNA sequences:

for refseq you have to go to the NCBI website:
www.ncbi.nlm.nih.gov, go to ftp site, select refseq, h.sapiens, 

If you have troubles getting there, try this mirror site:
ftp://bio-mirror.net/biomirror/refseq/cumulative
> ftp bio-mirror.net  (as usual, name: anonymous, password: email-address)
ftp> cd biomirror/refseq/cumulative
ftp> get rscu.gbff.Z
ftp> bye
unzip rscu.gbff.Z (this is a really huge multiple-entry genbank file)


for embl_vertRNAs you have to check first where the latest version is
(/data/blastdb/Ensembl/embl_vertrna-1  in plato or acari )
Use the script ensembl-pipeline/ESTs to get the mRNAs you want from 
this database.

There may be repeats and pseudogenes in embl_vertrna-1. If you're using mRNAs
only for adding UTRs to genewise genes, that should be fine. If you are going to use 
their CDSs as well (perhaps in EST_GeneBuilder), they should be rejected.


1. Make a file of the cDNAs/mRNAs you want to use. For human we use
the human entries from embl_vertrna, and will be using Refseq NM_*
sequences. The file should be a multiple entry fasta format file;
headers are best kept simple ie 
>Accession number 
ATCGGCGGTTAGCA...

2. Index the cDNA file so you can retrieve sequences later 

You can use James Cuff's makeseqindex to index
the file and getseqs to retrieve sequences but you can use any
sequence fetcher you like as long as you have implemented an
appropriate Bio::EnsEMBL::Pipeline::Seqfetcher module for it.

Also, a gotcha with makeseqindex - if you've made a beautiful simple
fasta file, avoid giving it a name that contains swall, humanest or
EST, or you'll end up using the wrong parsing method and be unable to
fetch any sequences.

You can also use Steve Searle's indicate:
	/nfs/acari/searle/progs/ensembl/ensembl-nci/src/bioglib/program/indicate

	which creates the OBDA indexes.
	( you may need some libraries:
	setenv LD_LIBRARY_PATH /nfs/acari/searle/progs/local2/lib:${LD_LIBRARY_PATH} )

	Example usage

	1. Make a directory for the index
	mkdir dbSTS

	2. Run:

	/nfs/acari/searle/progs/ensembl/ensembl-nci/src/bioglib/program/indicate
	--data_dir /data/blastdb/Ensembl --file_prefix dbSTS- --index dbSTS
	--parser dbParser

	Where:
	--data_dir is directory contain fasta files to index
	--file_prefix is prefix of files to index (all files beginning with
	  prefix and ending in a number will be indexed). If you want to index a single
	  fasta file you can write here just the name of that file
	--index directory created in step 1
	--parser parser function to use (dbParser for db* databases,
	  swallParser for swall, rikenParser for riken,... )
	   ( if you only have 1-word display_ids in a multiple-entry fasta file
	  the parser to use is 'singleWordParser')

	The seqfetcher to use with this index is Bio::EnsEMBL::Pipeline::SeeFetcher::OBDAIndexSeqFetcher.pm
	make sure that it is correctly defined in your Runnables

3. distribute cDNA seqfile and indexfile somewhere where
they can be seen from all run nodes 
eg at Hinxton they need to be in /data/blastdb/Ensembl and rdisted over
the farm.

options to be set in ESTConf.pm:

EST_INDEX			location of the cDNA seqfile which can be seen from al run nodes


4. run prepare_ests.pl
All options are set in ESTConf.pm:

EST_FILE			location of fasta file containing all cDNAs
EST_CHUNKDIR			path to directory where cDNA chunks are to be placed
EST_CHUNKNUMBER			number of chunks to split cDNA file into
EST_FILESPLITTER		path to fastasplit

This script splits the cDNA file into smaller chunks to be run with
exonerate. The size of each chunk is CRITICAL - too large and your
exonerate jobs will run out of memory, too small and they'll just
waste time.

Our farm nodes have 1G of memory. We have found that for human
sequence (dumped as RawContigs) ~350 seqs per chunk is optimal, while
for mouse (very small RCs, dumpaed as 500K virtual contigs) we had to
drop this to <100 seqs per chunk. You'll always get some jobs failing
and have to resplit and rerun the troublesome chunks.

Set ESTConf::EST_CHUNKNUMBER appropriately - this is the total number
of chunks, not the number of sequences in each chunk!!!


The spliting can be done by hand:

    /acari/work2/gs2/gs2/bin/fastasplit <fastapath> <outputdir> <chunknum>
 
 <fastapath> = the myltiple-entry fastA file with the ests
 <outputdir> = dir where we're going to put the chunks
 <chunknum>  = total number of chunks


C. Preparing genomic sequence for exonerate
-------------------------------------------

Exonerate runs a cDNA sequence file against the entire genome. 

1. prepare a multiple entry fasta file of the contigs in the golden
path, repeatmasked and dusted (or a file of whatever sequence you are
going to run against)

a. cd to somewhere which has plenty of space! You'll be dumping the whole
genome effectively 3 times though you can delete interim files as you
go.

b. For human sequence we dump RawContigs as these are of reasonable size (10s of kb)
You can use ensembl-pipeline/scripts/EST/dump_golden_contigs.pl and set the following options in ESTConf.pm:

EST_REFDBNAME		name of reference database
EST_REFDBUSER		user of reference database - read only is a good idea!
EST_REFDBHOST		host of reference database
EST_GOLDEN_PATH		name of assembly - copy the 'type' column of your static_golden_path table

Run the script like this:

dump_golden_contigs.pl -outfile masked_contigs.fa

c. Or, for mouse, where the RawContigs were tiny, we dump 500K masked virtual contigs using ensembl-pipeline/scripts/GeneBuild/dump_vc_seq.pl:

generate the chrname, start, end you need for each of the contigs (mysql + perl -e), then for each:

dump_vc_seq.pl -dbname dbname -dbhost dbhost -dbuser dbuser -path golden_path -start start -end end -chrname chrname -masked -outfile masked_contigs.fa

2. dust the masked sequence:
a. mkdir contig_chunks

b. mkdir dust_results

c. split contig file into smaller chunks (for human we use 4000
chunks) using eg fastasplit and put these in the directory
contig_chunks

d. edit ensembl-pipeline/scripts/EST/dust/config.txt to reflect the
path to your dust executable and the path to the contig_chunks
directory

At Hinxton I tend to restrict dust jobs to just the ecsnodes to avoid
having to distribute sequence files across the farm.

e. cp ensembl-pipeline/scripts/EST/dust/* dust_results

f. cd contig_chunks
   ls *chunk* > ../dust_results/job_ids.txt

g. ensembl-pipeline/scripts/EST/dust/fam.sh /path/to/dust_results init
   sets up directories for output in the dust_results directory you made above

h. ensembl-pipeline/scripts/EST/dust/fam.sh /path/to/dust_results submit
   makes a script called to_submit.sh

i. run to_submit.sh to submit jobs (as usual I generally run a couple
by hand to make sure all the configuration has been correctly set)

j. ensembl-pipeline/scripts/EST/dust/fam.sh /path/to/dust_results collate > dusted_masked_contigs.fa
   collates results into a multiple entry fasta file

3. Put dusted_masked_contigs.fa into the place you specified in
ESTConf::EST_GENOMIC and distribute across all nodes where you intend
to run - this is CRITICAL otherwise nfs overhead is horrific - at
least, it is here.

D. ExonerateEST
---------------

1. ensembl-pipeline/scripts/EST/make_bsubs.pl

The ExonerateEST jobs will go to the file you specified in ESTConf::EST_EXONERATE_BSUBS

There are a number of ESTConf options that need to be set for this to work properly:

EST_TMPDIR		      path to a scratch area
EST_REFDBHOST		      host of reference database
EST_REFDBUSER		      user of reference database - read only is good!
EST_REFDBNAME		      name of reference database
EST_EXONERATE_BSUBS	      location to write jobs file for ExonerateEST jobs
EST_FILTER_BSUBS	      location to write jobs file for FilterEsts_and_E2G jobs
EST_QUEUE		      queue (plus any host restricting options) where jobs are to be run
EST_SCRIPTDIR		      full path to ensembl-pipeline/script/EST
EST_CHUNKNUMBER		      number of chunks the cDNA file has been split into
EST_FILTERCHUNKSIZE	      size of genomic chunk for running est2genome jobs; we tend to use 1000000 bases
EST_GENEBUILDER_BSUBS	      location to write jobs file for EST_GeneBuilder jobs
EST_GENEBUILDER_CHUNKSIZE     size of genomic chunk for running EST_GeneBuilder jobs
EST_FILE		      location of the original unsplit cDNA file
EST_EXONERATE                 path to exonerate executable

check that the bsub commands look something like:

bsub -q acari -C0 
-o /scratch2/ensembl/eae/est_NCBI_28//exonerate_est/bsub/stdout/human.cdna_chunk_0000056 
-e /scratch2/ensembl/eae/est_NCBI_28//exonerate_est/bsub/stderr/human.cdnas_chunk_0000056 
-E "/nfs/acari/eae/ensembl-branch-121/ensembl-pipeline/scripts/EST//check_node.pl human.cdnas_chunk_0000056" 
/nfs/acari/eae/ensembl-branch-121/ensembl-pipeline/scripts/EST//exonerate_est.pl -chunkname human.cdnas_chunk_0000056

We are currently using exonerate-0.3d with arguments -w 14 -t 65 -H 100 -D 15 -m 500

2. submit the jobs

I ALWAYS check a few jobs by hand to make sure the output directories
are set up right, and there are going to be no nasty bouncing jobs due
to failing pre-exec commands etc. Then I send the rest of the jobs
off, but be careful not to overload LSF - at Hinxton this means try
not to have >5000 odd jobs in the queues at any time.

Then 
 submit.pl exonerate_ests.jobs.dat

3. Once they are run, you'll need to check for memory errors. 

     grep -il glib *.stderr 

     in the .../exonerate_est/bsub/stderr/ (and .../results/stderr/ ) directories.
     Since they are surely very big, use a foreach loop

     ecs1g[vac] % foreach i ( exest* )
     foreach? grep -il 'glib' $i
     foreach? end

If you identify chunks that have run out of memory, you need to split
them up smaller (using fastasplit again) and rerun them until you have
successfully run all the cDNA sequences.

4. Load the results into the database

cd to the place where you have specified output files to go (ESTConf::EST_TMPDIR)

Dump out information about the golden contigs into a file with form:
     id	 internal_id
You can do this by:

edit ensembl-pipeline
(/work2/vac/ensembl-scripts/)exonerate2mysqltab.pl -> converts o/p files into tab delimited files for loading


need to dump out golden contig ids first

ecs1g[vac] % foreach i ( exest* )
foreach? gunzip $i
foreach? exonerate2mysqltab.pl $i:r >> estfeatures
foreach? gzip $i:r
foreach? end

strips out features with id <90:

5. make est table in est database - 

mysql estdb < ensembl-pipeline/sql/est.sql
(/work2/vac/ensembl-scripts/)get_estlengths > estlengths
load data infile 'estlengths' into table est

check for duplicates before loading!

6.  make appropriate entries in the analysisprocss table by loading up

mysql -uuser -ppass -hhost dbname < ensembl-pipeline/sql/EST_analysisprocess.sql

Now you need to make sure the db column is the same as the EST_SOURCE
you've defined in ESTConf - or no features will be retrieved during
later stages eg

mysql> update analysisprocess set db='wibble' where db='dbEST';


F. FilterESTs_and_E2G
---------------------

1. submit the jobs that are listed in ESTConf::EST_FILTER_BSUBS

check that the bsub commands look something like:

bsub -q acari -C0 
-o /scratch2/ensembl/eae/est_NCBI_28//filter_and_e2g/1/1.49000001-50000000.out 
-e /scratch2/ensembl/eae/est_NCBI_28//filter_and_e2g/1/1.49000001-50000000.err 
-E "/nfs/acari/eae/ensembl-branch-121/ensembl-pipeline/scripts/run_EST_RunnableDB -check -runnable Bio::EnsEMBL::Pipeline::RunnableDB::FilterESTs_and_E2G" 
/nfs/acari/eae/ensembl-branch-121/ensembl-pipeline/scripts/EST//filter_and_e2g.pl -input_id 1.49000001-50000000

As ever, I start off slowly and make sure to test the pre-exec command before loading up the farm.

2. these jobs will read and write in the same databases, so check status of the database with the commands:
   mysql -u user -h host -e "show processlist";
   mysql -u user -h host -e "show variables"  ;

3. check for errors. One thing that can go wrong is that the est2genome runs out of memory. The option
   -space in the command in Est2Genome.pm may have to be changed. If you plan to re-run this region, make
   sure it does not trouble later stages, or otherwise you'll have to delete the genes written in the
   corresponding job.

once the jobs have finished, we're ready for the next step:


G. EST_GeneBuilder
------------------

If you're doing this stage, it is precisely the same as outlined in
EST_analysis.txt so look there for the most up to date instructions.

H. Combine_Genewises_and_E2Gs
-----------------------------

This module combines the est2genome predictions from cDNAs with
genewise protein-based predictions from the Targetted and Similarity

Genewise stages of the gene build. It is formally part of the main
gene build and is described in assembly_based_genebuild.txt rather than here.

I. Data delivery
----------------

cDNA features/genomewise predictions are delivered as described in EST_analysis.txt

Genes created from genewises with UTRs from cDNAs are now dealt with in the main GeneBuild - so from here go back to the assembly_based_genebuild docs.




###############
RefSeq Database
###############


We have to build an extra database with all the exons from exonerate_e2g genes that have been built from
RefSeq sequences, written as features (in the same way we prepare the EST database with features from the
exons written by Filter_Exonerate_and_E2G...


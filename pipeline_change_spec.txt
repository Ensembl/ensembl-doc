ENSEMBL-PIPELINE API Change Specification
=========================================


CONTENTS
--------


Introduction
Goals
Code Reorganisation
General Objectives
 Documentation
 Test system
 Core code conventions
 Database connection and contention issues
 Use strict and warnings
 Infrastructure API Changes
   RuleManager code
   Configuration
   Pipeline dna database
   VOID status
 Analysis API Changes
  Runnable/RunnableDB's
   Blast
   Genewise modules
   Genebuilder
   Ab Initio modules
   Protein Annotation modules 
   ESTTranscriptFilter
   EST genebuilding code
  Configuration
  SeqFetchers
  Error string passing
Bioperl


Introduction
------------

This document describes the changes that are being made to the EnsEMBL
pipeline code.

Goals
-----
A cleaner less redundant api.
A greater level of flexibility in using the pipeline code both for job 
submission and for running different analyses.

Code Reorganisation
-------------------

The pipeline cvs module will be split into two. The pipeline infrastructure
code which deals creating and submitting jobs, handling dependancies etc
will remain in ensembl-pipeline but the Runnable/RunnableDB code and
the modules they use to run the various different analysis will move into
a new cvs module called ensembl-run like bioperl.



General Objectives
------------------

 Documentation
 #############

 The perl docs in the modules need to be improved or even added to some 
 modules to make it clearer how particular modules behave. We also need
 more documentation of the whole process and of challenges particular
 species introduce.   

 Test System
 ###########

 The existing test system needs to be revamped to make is both usable and
 maintainable and people need to be encouraged to both run the tests and
 update the tests when new functionality is added to the pipeline.

 Core Code conventions
 #####################

 Move the database adaptors to using the core code conventions such as 
 returning listrefs etc. We also need to move over to using the 
 Bio::EnsEMBL::Utils::Exceptions code for throws, warning etc and try
 and use methods like info to introduce configurable levels of chattiness.


 Database connection and contention issues
 #########################################
  
 Steve has recently written some code which allows more sensible handling
 of inactive database connections. This code is relatively easy to use
 in the runner script and analyses which only utilize one database 
 connection but this gets more complicated when multiple connections to
 different databases are used as in the genebuild code. We need to find
 a sensible way to use this new functionality.

 
 Use strict and warnings
 #######################

 While most of the pipeline code does use strict not all of it seems
 to use warnings. We need to make sure this is switched on everywhere
 as this has let subtle bugs through in the past.
  

 Infrastructure API changes
 ##########################

   RuleManager code
   ****************

   The RuleManager script is quite monolithic and more than 1000 lines of
   code. Most of the methods in RuleManager should be moved into a module
   to allow easier reuse and improve maintainability. While doing this
   we should also look to optimise the code as the central loop of the
   RuleManager can take a long time to execute. The RuleManager and runner 
   should also probably be moved into the scripts directory as they are 
   scripts rather than modules. The batchsubmission checks for awol and 
   pending job numbers can probably also be merged

   Configuration
   ************* 
 
   The Pipeline infrastructure configuration does work quite well but 
   it would be a a good idea to make sure most generic options are 
   overridable using the command line.
 
   Dna database
   ************

   Give the runner script the possiblity of using a dna database

   VOID status
   ***********

   most Jobs which get marked as Void will probably just eventually get 
   added to the input_id_analysis table and the pipeline will continue
   It may be a good idea to make the string used when setting jobs to VOID
   as configurable somewhere so you can just continue on with the piepline
   without delay

  Analysis API changes
  ####################


   Runnable/RunnableDB's
   *********************

   The Runnable/RunnableDBs are now getting quite unmaintainable due to 
   vast amounts of code duplication and poor documentation. Much of the 
   code also needs to be optimised to run more efficiently with the vast 
   amount of data we can now provide it. Most of the runnables can have 
   methods reduced down to base classes to make them more maintainable but 
   below specific things which we can do to certain sets of modules is 
   described.

     Blast
     ----- 
     
     The Blast runnable is now huge, over a 1000 lines of code and it is
     making it quite difficult to used flexibly. To improve this we can do
     several things. The first is make all options set by configuration
     also settable in the constructor which will allow people to either not
     fill out the configuration (as long as it exists) but also run 
     different blasts against the same database. 
  
     Another element we can change is the way parse results works. 
     Currently the blast runnable is tied to BPlite. It be better if all 
     the parsing code and feature generation code is moved out to another 
     module which lives with BPlite in tools which takes the file or file 
     handle and produces the desired feature type which should be either 
     feature pairs or align features depending on what you want. The 
     default parser to use would then be the wrapper which sits on top of 
     BPlite but it would make it  easier to move parsers if desired or 
     just offer more alternatives.
      

     Genewise and related modules
     ----------------------------

     These modules contain a lot of redundant code like sequence fetching
     and gene creation which can be factored out into base classes. 

     The code currently also generates SeqFeatures and SubSeqFeatures in 
     the Runnables which are then converted into genes once in the 
     RunnableDB's we will change the code to create Exons etc directly.

     
     Genebuilder and Prediction Genebuilder
     --------------------------------------
     
     These are currently two modules which don't follow the 
     Runnable/RunnableDB module but instead simply live in 
     Bio::EnsEMBL::Pipeline.

     Ab initio modules
     -----------------
     
     Most of the modules which run ab initio gene predictors follow the 
     same basic structure and only have few different modules, generally
     run_analysis and parse_results. These will be easy to boil down to
     a base class with children to perform the program specific 
     functionality and this will make them much easier to maintain.

     Protein Annotation Runnables
     ----------------------------

     This code is now most non redundant but this can still be improved
     and it can be moved in to Runnable/RunnableDB from Protein as there
     is no need for a separate directory for it.


     ESTTrancriptFilter
     ------------------

     This code currently uses Bio::EnsEMBL::SeqFeatures, we need to move
     it to using something else   

     EST Genebuilding code
     ---------------------

     The config for this needs reworking to make is easy to run with 
     multiple different sets of data without needing multiple different
     sets of configuration. WE will probably move to a system like 
     BatchQueue.pm with hashes containg values keyed on logic_name or
     database name


     Prediction Genebuilder
     ----------------------
       
     Make prediction genebuilder run as a standalone module rather than
     being tied in with Genebuilder
     

   Configuration
   *************
     
   The configuration is getting very crufty so some variables which. In
   general the configuration needs to be rationalised and redundancies 
   removed.


   Seqfetchers
   ***********

   The genebuild process in particular uses seqfetchers a lot to 
   get protein and cdna sequences for its analysis. The seqfetcher we
   use most is Steves OBDA indicate indexing and the code we use currently
   resides in bioperl. We propose moving this code back inside EnsEMBL 
   to make it easier for us to maintain.


   Error string passing
   ********************

    Roy recently introduced some code into the Blast runnable which 
    parsed the blast processes STDERR output and returns a more sensible
    error message when certain errors are throw like VOID when no valid
    sequence context is found or OUT OF MEMORY when we run out of memory.
    This makes it easier to see some errors and take appropriate action
    and I think if possibly it would be sensible to have similar code for
    other analysis which are prone to similar errors like Exonerate, Pmatch
    and Genewise. We also want to introduce a controlled vocabulary for
    error messages to make it easier to assess what has gone wrong and if 
    the errors are significant enough to need to stop the pipeline


Bioperl
------

We should move to using the most recent stable version of bioperl
or at least the version of bioperl supported by the webcode   






===================================================================
Specific areas of the pipeline that could be improved (dta 070504):
===================================================================


Error messages.  Runnable chattiness.
-------------------------------------

* A controlled set of error messages would allow a set of decisions to
be made by the runner as to whether to proceed, given some kind of
error severity cut-off.

* By default Runnables and RunnableDBs should be silent on STDERR and
STDOUT and only make noise when something has gone awry.

* The stderr and stdout of the runnable modules needs to be of
controlled syntax to allow parsing of the success/failure/outcome of a
job.  A reporting module could be constructed to simplify this.

* If eval is used, every time an error is caught the error string MUST
be printed - or you'll get the chair.


Config Files.
-------------

* The cDNA database details currently have to be entered into the
config files twice.  Once in the Config/GeneBuild/databases.pm file
and once in the Config/cDNA_ESTs/EST_GeneBuilder.pm file.  This is a
redundancy that could be removed.

* It is impossible to run the EST and cDNA builds simultaneously,
purely due to problems with the config files being the same.  This
could do with a clever solution.


RuleManager and Job Control
---------------------------

* RuleManager is very hard to run 'just quickly'.  Getting input ids
for each cycle of the RuleManager run is glacially slow and makes this
script a blunt tool for getting analyses run.

* The time that RuleManager spends sleeping should be easily set
through config files - quick jobs certainly don't want the RuleManager
to sleep for 90 minutes if they exceed their pending limit.

* The number of jobs running needs to be controlled somehow.
Controlling the number of pending jobs is only indirectly controlling
this.  Maybe the control could be shifted to the total number of
submitted jobs (RUN + PEND + SUSP).

* Where runnable execution is eval'd, there are many cases
(e.g. SimilarityGeneWise) where one run out of many will fail.  It is
impossible to simply rerun these as duplicate entries in the database
will result.  It is important that an option exist that allows the
user to specify what should be done when a runnable fails (ie. throw
or warn).  In many cases a warning is not serious enough and leads to
patchy data.  In these cases a throw before writing data would be all
that is needed.

* Jobs that are known to consume large amounts of memory need to be
flagged as such in the documentation.  An internal check by the
runnable of minimum memory available should also be performed.


Pipeline tables.
----------------

* It is hard to fill in the input_id_type_analysis table.  Currently
needs manual (error prone) SQL entry.  The table filling-in scripts
could be tweaked to include this table too.

* The need to insert dummy analyses is not ideal - especially since
there are eight to ten of them which don't really have set names.
Some lateral new way of doing this, or some fancy coding, or a
documented standard set of dummy analysis, could remove
this minor annoyance.


Documentation.
--------------

* Thorough documentation of all that is expected from a core database
before handover.  A sequence of steps, annotated with anecodtal
warnings, heads-up for gotchas and even very general comments on
conservative amounts of time needed for each stage (maybe ranked as
QUICK, SLOW, VERY SLOW).  The purpose of this document would be to
avoid stomach lurches because a genebuilder didn't know that protein
annotation was required, or that some EST_Genebuilder jobs can run for
days.  I'd like to see one loooooong flowchart that starts at loading
sequence and ends at post-genebuild healthchecks and includes all
optional excursions,the data sources used/needed and the various
databases where everything is written.  This is a document that will
benefit from lots of input and experiences of others.
  --- I've begun work of this documentation and flow diagram (using an
automated way of updating the flow diagram with 'dot').


Writing to database.
--------------------

* Internal safeguards should be inherant within the pipeline that
prevent the same job from writing twice.  This would be in the form of
a record of rows inserted rather than a status in the
job_status/input_id_analysis table.  This might not be necessary for
each stage of the build, to reduce overhead, but when writing genes
and features in the final stage of the build an anti-duplication
function would avoid problems being written to release databases.

ALTERNATIVELY,

* RunnableDBs should be required to report the object types and db
insertion ids of any objects that it stores in the database.  An ideal
outcome of this would be that a tool for parsing this information
could collate this information in order to identify features that have
been stored either not at all or multiple times.


Miscellaneous.
--------------

* Fetching a repeatmasked sequence seems to require the return of
several Mb of text to stderr.  This should not be the default behaviour.

* Everything to do with logic names should be case insensitive.


* The LSF output directories divided into 0,1,2,3,4,5, etc are still
not fine-grained enough.  When using 1Mbp slices, if jobs fail a
number of times it is possible to have nearly 2000 files in a single directory.

* The OBDA index seqfetcher is flawed due to a BioPerl bug.  This is
quite a serious bug and stops 5-10% of proteins being fetched from swall.

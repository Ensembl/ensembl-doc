ENSEMBL-PIPELINE API Change Specification
=========================================


CONTENTS
--------


Introduction
Goals
Code Reorganisation
General Objectives
 Documentation
 Test system
 Core code conventions
 Database connection and contention issues
 Use strict and warnings
 Infrastructure API Changes
   RuleManager code
   Configuration
   Pipeline dna database
   VOID status
 Analysis API Changes
  Runnable/RunnableDB's
   Blast
   Genewise modules
   Genebuilder
   Ab Initio modules
   Protein Annotation modules 
   ESTTranscriptFilter
   EST genebuilding code
  Configuration
  SeqFetchers
Bioperl
Private cvs for specifc genomes config files
Logging Module

Introduction
------------

This document describes the changes that are being made to the EnsEMBL
pipeline code.

Goals
-----
A cleaner less redundant api.
A greater level of flexibility in using the pipeline code both for job 
submission and for running different analyses.

Code Reorganisation
-------------------

The pipeline cvs module will be split into two. The pipeline infrastructure
code which deals creating and submitting jobs, handling dependancies etc
will remain in ensembl-pipeline but the Runnable/RunnableDB code and
the modules they use to run the various different analysis will move into
a new cvs module called ensembl-analysis. The convention will be that for
ensembl-pipeline to work you should also have ensembl-analysis but ensembl
analysis should have no dependancies back on ensembl-pipeline



General Objectives
------------------

 Documentation
 #############

 The perl docs in the modules need to be improved or even added to some 
 modules to make it clearer how particular modules behave. We also need
 more documentation of the whole process and of challenges particular
 species introduce.   

 Test System
 ###########

 The existing test system needs to be revamped to make is both usable and
 maintainable and people need to be encouraged to both run the tests and
 update the tests when new functionality is added to the pipeline.

 The idea for the test system to to have a database setup which is easy
 to produce so you know you can test any step individually or as a whole
 and a system where the config and the reference set of analysis results
 are easy to regenerate

 Core Code conventions
 #####################

 Move the database adaptors to using the core code conventions such as 
 returning listrefs etc. We also need to move over to using the 
 Bio::EnsEMBL::Utils::Exceptions code for throws, warning etc and try
 and use methods like info to introduce configurable levels of chattiness.

 This is a list of core code conventions I was given

 * Functions that return lists (especially potentially large ones) should
   use pass by reference.

 * Adaptor methods that return list references start with fetch_all

 * Other functions that return list references start with get_all

 * Adaptor methods that create objects begin with fetch, and adaptors
   should only return objects of one type (e.g. the GeneAdaptor only 
   creates Gene objects).

 * The internal identifier of Objects is denoted by the mixed case 'dbID', 
   even though the dbID is not an object. E.g. Gene::dbID, 
   GeneAdaptor::fetch_by_dbID

 * Adaptor methods that return lists of scalar values (not objects) start 
   with 'list'.  E.g. GeneAdaptor::list_dbIDs

 * Class names are always mixed case, with each word beginning with a 
   capital letter. E.g. GeneAdaptor, ChainedAssemblyMapper

 * Functions that return objects use mixed case for the object names, with 
   the exception that functions never start with a capital letter.
   E.g. Transcript::get_all_Exons, Transcript::translation

 * SQL is only performed by ObjectAdaptors, never anywhere else in the 
   code.

 What follows are more conventions we should stick too

 * If eval is used, every time an error is caught the error string MUST
   be printed.

 Database connection and contention issues
 #########################################
  
 Steve has recently written some code which allows more sensible handling
 of inactive database connections. This code is relatively easy to use
 in the runner script and analyses which only utilize one database 
 connection but this gets more complicated when multiple connections to
 different databases are used as in the genebuild code. We need to find
 a sensible way to use this new functionality.

 This is code which has already been implemented and is already used in
 places but we need to make descisions about some of the genebuild modules
 which have more database access than most to see where it is sensible
 to have the code switched on and off
 
 Use strict and warnings
 #######################

 While most of the pipeline code does use strict not all of it seems
 to use warnings. We need to make sure this is switched on everywhere
 as this has let subtle bugs through in the past.
  

 Infrastructure API changes
 ##########################

   RuleManager code
   ****************

   The RuleManager script is quite monolithic and more than 1000 lines of
   code. Most of the methods in RuleManager should be moved into a module
   to allow easier reuse and improve maintainability. While doing this
   we should also look to optimise the code as the central loop of the
   RuleManager can take a long time to execute. The RuleManager and runner 
   should also probably be moved into the scripts directory as they are 
   scripts rather than modules. The batchsubmission checks for awol and 
   pending job numbers can probably also be merged

   This is already done for the most part but the scripts still need to
   be optimized and tested in anger


   Configuration
   ************* 
 
   The Pipeline infrastructure configuration does work quite well but 
   it would be a a good idea to make sure most generic options are 
   overridable using the command line. 

   The pipeline config currently consists of two modules BatchQueue.pm
   and General.pm. I may alter this to Analysis.pm and RuleManager.pm
   Analysis will contain most of what BatchQueue does now but some
   of the rulemanager and batch submission system specific options
   will move into RuleManager.pm. Any options which are to be defined
   here.
 
   Dna database
   ************

   Give the runner script the possiblity of using a dna database
   This would probably be configuration options which lies in Analysis.pm
   If the dbargs for a dna database were defined two connections would
   be established the defined dna database being attached to the standard
   pipeline database. There would probably be some checking to make sure 
   they aren't the same database

   VOID status
   ***********

   most Jobs which get marked as Void will probably just eventually get 
   added to the input_id_analysis table and the pipeline will continue
   It may be a good idea to make the string used when setting jobs to VOID
   as configurable somewhere so you can just continue on with the pipeline
   without delay.

   After deciding this in't the best solution any sequence which is found
   to have no value context by the check in RunnableDB will be marked as
   SUCCESSFUL but those which find context validty errors in the actual
   blast run will still be marked as VOID 

  Analysis API changes
  ####################


   Runnable/RunnableDB's
   *********************

   The Runnable/RunnableDBs are now getting quite unmaintainable due to 
   vast amounts of code duplication and poor documentation. Much of the 
   code also needs to be optimised to run more efficiently with the vast 
   amount of data we can now provide it. Most of the runnables can have 
   methods reduced down to base classes to make them more maintainable but 
   below specific things which we can do to certain sets of modules is 
   described.

   The base classes have been spec'd out in RunnableDB.spec and 
   Runnable.spec which should be found in this directory

     Blast
     ----- 
     
     The Blast runnable is now huge, over a 1000 lines of code and it is
     making it quite difficult to used flexibly. To improve this we can do
     several things. The first is make all options set by configuration
     also settable in the constructor which will allow people to either not
     fill out the configuration (as long as it exists) but also run 
     different blasts against the same database. 
  
     Another element we can change is the way parse results works. 
     Currently the blast runnable is tied to BPlite. It be better if all 
     the parsing code and feature generation code is moved out to another 
     module which lives with BPlite in tools which takes the file or file 
     handle and produces the desired feature type which should be either 
     feature pairs or align features depending on what you want. The 
     default parser to use would then be the wrapper which sits on top of 
     BPlite but it would make it  easier to move parsers if desired or 
     just offer more alternatives.
      

     Genewise and related modules
     ----------------------------

     These modules contain a lot of redundant code like sequence fetching
     and gene creation which can be factored out into base classes. 

     The code currently also generates SeqFeatures and SubSeqFeatures in 
     the Runnables which are then converted into genes once in the 
     RunnableDB's we will change the code to create Exons etc directly.

     
     Genebuilder and Prediction Genebuilder
     --------------------------------------
     
     These are currently two modules which don't follow the 
     Runnable/RunnableDB module but instead simply live in 
     Bio::EnsEMBL::Pipeline.

     Ab initio modules
     -----------------
     
     Most of the modules which run ab initio gene predictors follow the 
     same basic structure and only have few different modules, generally
     run_analysis and parse_results. These will be easy to boil down to
     a base class with children to perform the program specific 
     functionality and this will make them much easier to maintain.

     Protein Annotation Runnables
     ----------------------------

     This code is now most non redundant but this can still be improved
     and it can be moved in to Runnable/RunnableDB from Protein as there
     is no need for a separate directory for it.


     ESTTrancriptFilter
     ------------------

     This code currently uses Bio::EnsEMBL::SeqFeatures, we need to move
     it to using something else   

     EST Genebuilding code
     ---------------------

     The config for this needs reworking to make is easy to run with 
     multiple different sets of data without needing multiple different
     sets of configuration. WE will probably move to a system like 
     BatchQueue.pm with hashes containg values keyed on logic_name or
     database name


     Prediction Genebuilder
     ----------------------
       
     Make prediction genebuilder run as a standalone module rather than
     being tied in with Genebuilder
     

   Configuration
   *************
     
   The configuration is getting very crufty so some variables which. In
   general the configuration needs to be rationalised and redundancies 
   removed.

   The principle is that any one RunnableDB/Runnable should only have to
   include 3 config files. General.pm for general settings, Database.pm
   for database settings and then an analysis specific file if required 
   for any other analysis specific settings. If one analysis needs to know
   config options for another analysis its config file such fetch these not
   the RunnableDB itself    

   For the cDNA/EST analysis we need a method of running multiple analysis
   across different data sets at the same times. The two current proposals
   include using a BatchQueue style module and array of hashes to contain
   the data specific settings for the run keyed on say logic_name. The
   other is to have high level runnabledbs which wrap the exonerate and 
   estgenebuilder modules and the cdna/est specific config files but this
   doesn't allow you to align multiple est or multipl cdna data sets

   Seqfetchers
   ***********

   The genebuild process in particular uses seqfetchers a lot to 
   get protein and cdna sequences for its analysis. The seqfetcher we
   use most is Steves OBDA indicate indexing and the code we use currently
   resides in bioperl. We propose moving this code back inside EnsEMBL 
   to make it easier for us to maintain.

   The OBDA module doesn't need fixing as it fails to fetch sequences which
   are present in the index a significant proportion of the time and we
   need to figure out why. This problem is particularly notable when
   two indexes are being queried at the same time



Bioperl
------

We should move to using the most recent stable version of bioperl
or at least the version of bioperl supported by the webcode   



Private CVS
-----------

It would be a good idea to have a private cvs repository to keep
copies of the site specific config files for each build so we have a record
of what people used for future reference

this would keep versions of all files found in 
Bio::EnsEMBL::Pipeline::Config
and copies of the analysis.conf and rules.conf which could be generated
from the analysis and rules tables using the analysis_setup and rules_setup
scripts. Plus a dump of the different input_ids used


Logging module
-------------

We need a module which is easy to use and easy to remember what the
different levels of verbosity are. After various discussions I think we
only need 3 levels of verbosity

0, this should be short terse messages which at least contain some
of the controlled vocab so it can be easily parsed e.g

COMMAND LINE: Repeatmasker sequence.fa
PROBLEM: OUT OF MEMORY ERROR

with level 0 stderr so be empty without any problems and stdout should
only have standard progress messages like the commandline etc

1 this would be useful information when debug but should still be
  relatively short

RepeatMasker produces 5 results
validate_genes has thrown out 3 genes

2 this can be as verbose as you like print out all the information
about genes right down to their supporting evidence etc

There was also a suggestion that the logging was divided into 3 chunks 
which could be configured indenpendantly the chunks would probably be
Runnable/RunnableDBs utils modules/parsers like TranscriptUtils
everything else

this would mean if you were debugging the Runnables and wanted them very
chatty you wouldn't have to see everything. It would also probably
pay to try and ensure the formatting of the prints is as easy to read
as possible to make the messages actually useable



Error checking and error strings
--------------------------------

The main error string should always be part of the controlled vocabulary
to allow for easy checking and potentially machine parsing

Whether errors should stop a process entirely is a more difficult
question. The default behavoir is that a throw should kill the running
job leaving it will a FAILED status or something else appropriate but
there are situations like in the Genewise runs where this isn't always
sensible. 

Data should be always validated before a write occurers to make sure 
the objects have all the data the write is expecting them to have
but a failed write shouldn't necessarily kill the whole process other
wise you potentially lose 20hrs of computes for the sake of 1 gene.

It should also be made clear which stages failed writes need investigating
and cleaning up and which stages are allowed to have duplicate data





LSF Documentation and useage
----------------------------

There should be clear docs somewhere which indicate which LSF options
are appropriate to use in the context of the pipeline and which analysis
should be run using options. The options we need docs for are memory 
resources, specific archtectures and machine types, database limiting
options and time limits. As far as memory goes whether it is sensible to 
have the jobs themselves check memory usage aswell is another thing to 
consider


Input_id_analysis table
-----------------------

The pipeline infrastructure itself will not be developed in the next
couple of months at least but the need for dummy analysis have been
reduced if you use either the lsf_submission or the job_submission scripts
as these can manfacture input_ids on the fly so at least for the first
analysis in your chain you don't necessarily need dummies but it is still
a good idea to add them anyway. A document could describe what is a 
standard set of dummies but having a coded version I think removed some
of the flexibilty the current system offers and won't work in situations
like filenames really





dtas comments not yet addressed
-------------------------------


Documentation.
--------------

* Thorough documentation of all that is expected from a core database
before handover.  A sequence of steps, annotated with anecodtal
warnings, heads-up for gotchas and even very general comments on
conservative amounts of time needed for each stage (maybe ranked as
QUICK, SLOW, VERY SLOW).  The purpose of this document would be to
avoid stomach lurches because a genebuilder didn't know that protein
annotation was required, or that some EST_Genebuilder jobs can run for
days.  I'd like to see one loooooong flowchart that starts at loading
sequence and ends at post-genebuild healthchecks and includes all
optional excursions,the data sources used/needed and the various
databases where everything is written.  This is a document that will
benefit from lots of input and experiences of others.
  --- I've begun work of this documentation and flow diagram (using an
automated way of updating the flow diagram with 'dot').


Writing to database.
--------------------

* Internal safeguards should be inherant within the pipeline that
prevent the same job from writing twice.  This would be in the form of
a record of rows inserted rather than a status in the
job_status/input_id_analysis table.  This might not be necessary for
each stage of the build, to reduce overhead, but when writing genes
and features in the final stage of the build an anti-duplication
function would avoid problems being written to release databases.

ALTERNATIVELY,

* RunnableDBs should be required to report the object types and db
insertion ids of any objects that it stores in the database.  An ideal
outcome of this would be that a tool for parsing this information
could collate this information in order to identify features that have
been stored either not at all or multiple times.


Miscellaneous.
--------------

* Fetching a repeatmasked sequence seems to require the return of
several Mb of text to stderr.  This should not be the default behaviour.

* Everything to do with logic names should be case insensitive.


* The LSF output directories divided into 0,1,2,3,4,5, etc are still
not fine-grained enough.  When using 1Mbp slices, if jobs fail a
number of times it is possible to have nearly 2000 files in a single 
directory.

* The OBDA index seqfetcher is flawed due to a BioPerl bug.  This is
quite a serious bug and stops 5-10% of proteins being fetched from swall.



* move Bio::EnsEMBL::Analysis:: code into pipeline from core api as it 
doesn't really belong there



*utility to check what proteins already have genes run on them, dnagling
foreign keys etc before rerunning failed genewise jobs to try and reduce
the amount of work you have to rerun

